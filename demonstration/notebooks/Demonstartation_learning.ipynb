{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "id": "uIGnEvc0tXsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "LslcVcUsiswb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ALE\n",
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ],
      "metadata": {
        "id": "51x3TyFqmNxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "from collections import deque\n",
        "import pickle\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7GzB4go8h6f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Policy"
      ],
      "metadata": {
        "id": "x8EzP66GiwZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, buffer, env, device=torch.device('cpu')):\n",
        "        super(Policy, self).__init__()\n",
        "        self.device = device\n",
        "        self.env = env\n",
        "        learning_rate = 0.001\n",
        "        self.epsilon = 0.5\n",
        "        self.batch_size = 64\n",
        "        self.network = Q_network(self.env, learning_rate)\n",
        "        self.target_network = deepcopy(self.network)\n",
        "        self.buffer = buffer\n",
        "        self.window = 50\n",
        "        self.reward_threshold = 800\n",
        "        self.training_rewards = []\n",
        "        self.training_loss = []\n",
        "        self.update_loss = []\n",
        "        self.mean_training_rewards = []\n",
        "        self.sync_eps = []\n",
        "        self.rewards = 0\n",
        "        self.step_count = 0\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "    \n",
        "    def act(self, state):\n",
        "        action = self.network.greedy_action(torch.FloatTensor(state))\n",
        "        return action\n",
        "\n",
        "    def update(self):\n",
        "        self.network.optimizer.zero_grad()\n",
        "        batch,weights,tree_idxs = self.buffer.sample(batch_size=self.batch_size)\n",
        "        loss,td_error = self.calculate_loss(batch,weights=weights)\n",
        "        self.buffer.update_priorities(tree_idxs,td_error)\n",
        "        loss.backward()\n",
        "        self.network.optimizer.step()\n",
        "\n",
        "        self.update_loss.append(loss.item())\n",
        "\n",
        "    def take_step(self, mode='exploit'):\n",
        "        if mode == 'explore':\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            action = self.network.greedy_action(torch.FloatTensor(self.s_0))\n",
        "\n",
        "        s_1, r, done, _ = self.env.step(action)\n",
        "\n",
        "        self.buffer.add(self.s_0, action, r, done, s_1)\n",
        "        self.rewards += r\n",
        "        self.s_0 = s_1.copy()\n",
        "\n",
        "        self.step_count += 1\n",
        "        if done:\n",
        "            self.s_0 = self.env.reset()\n",
        "        \n",
        "        return done\n",
        "\n",
        "    def expert_train(self):\n",
        "        self.gamma = 0.99\n",
        "        network_sync_frequency = 2\n",
        "        batch_size_sum = 0\n",
        "        self.loss_function = nn.MSELoss()\n",
        "\n",
        "        ep = 0\n",
        "        \n",
        "        print(\"############ Pre-training with expert trace started!\")\n",
        "        while batch_size_sum <= self.buffer.get_memory_size():    \n",
        "            self.update()\n",
        "            \n",
        "            if ep % network_sync_frequency == 0:\n",
        "              self.target_network.load_state_dict(self.network.state_dict())\n",
        "              self.sync_eps.append(ep)\n",
        "\n",
        "           \n",
        "            if len(self.update_loss) == 0:\n",
        "              self.training_loss.append(0)\n",
        "            else:\n",
        "              self.training_loss.append(np.mean(self.update_loss))\n",
        "\n",
        "            self.update_loss = []\n",
        "            mean_loss = np.mean(self.training_loss[-self.window:])\n",
        "            print(\"\\rEpisode {:d} || mean loss = {:.2f}\\t\\t\".format(ep, mean_loss))\n",
        "            \n",
        "            if(ep%10 == 0):\n",
        "                print(\"Checkpoint!\")\n",
        "                self.save_check(\"pre\"+str(ep))\n",
        "            \n",
        "            batch_size_sum += self.batch_size\n",
        "            ep+=1\n",
        "\n",
        "        self.pretrain_save()\n",
        "\n",
        "    def train(self):\n",
        "        self.gamma = 0.99\n",
        "        max_episodes = 5\n",
        "        network_update_frequency = 10\n",
        "        network_sync_frequency = 200\n",
        "        self.loss_function = nn.MSELoss()\n",
        "        self.s_0 = self.env.reset()\n",
        "\n",
        "        for _ in range(self.batch_size):\n",
        "            self.take_step(mode='explore')\n",
        "        \n",
        "        ep = 0\n",
        "        training = True\n",
        "        self.populate = False\n",
        "        print(\"############ Training started\")\n",
        "        while training:\n",
        "            self.s_0 = self.env.reset()\n",
        "\n",
        "            self.rewards = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "\n",
        "                p = np.random.random()\n",
        "                if p < self.epsilon:\n",
        "                    done = self.take_step(mode='explore')\n",
        "                else:\n",
        "                    done = self.take_step(mode='exploit')\n",
        "                \n",
        "                if self.step_count % network_update_frequency == 0:\n",
        "                    self.update()\n",
        "                \n",
        "                if self.step_count % network_sync_frequency == 0:\n",
        "                    self.target_network.load_state_dict(\n",
        "                        self.network.state_dict())\n",
        "                    self.sync_eps.append(ep)\n",
        "\n",
        "                if done:\n",
        "                    ep += 1\n",
        "\n",
        "                    if self.epsilon >= 0.05:\n",
        "                        self.epsilon = self.epsilon * 0.7\n",
        "                \n",
        "                    self.training_rewards.append(self.rewards)\n",
        "\n",
        "                    if len(self.update_loss) == 0:\n",
        "                        self.training_loss.append(0)\n",
        "                    else:\n",
        "                        self.training_loss.append(np.mean(self.update_loss))\n",
        "\n",
        "                    self.update_loss = []\n",
        "                    mean_rewards = np.mean(self.training_rewards[-self.window:])\n",
        "                    mean_loss = np.mean(self.training_loss[-self.window:])\n",
        "                    self.mean_training_rewards.append(mean_rewards)\n",
        "                    print(\n",
        "                        \"\\rEpisode {:d} Mean Rewards {:.2f}  Episode reward = {:.2f}   mean loss = {:.2f}\\t\\t\".format(\n",
        "                            ep, mean_rewards, self.rewards, mean_loss))\n",
        "\n",
        "                    if ep >= max_episodes:\n",
        "                        training = False\n",
        "                        print('\\nEpisode limit reached.')\n",
        "                        break\n",
        "                    if mean_rewards >= self.reward_threshold:\n",
        "                        training = False\n",
        "                        print('\\nEnvironment solved in {} episodes!'.format(ep))\n",
        "                    \n",
        "                    if(ep%10 == 0):\n",
        "                        print(\"Checkpoint!\")\n",
        "                        self.save_check(ep)\n",
        "\n",
        "        self.save()\n",
        "        self.plot_training_rewards()\n",
        "\n",
        "    def plot_training_rewards(self):\n",
        "        plt.plot(self.mean_training_rewards)\n",
        "        plt.title('Mean training rewards')\n",
        "        plt.ylabel('Reward')\n",
        "        plt.xlabel('Episods')\n",
        "        plt.show()\n",
        "        plt.savefig('mean_training_rewards.png')\n",
        "        plt.clf()\n",
        "\n",
        "    def calculate_loss(self, batch,weights=None):\n",
        "        states, actions, rewards, next_states, dones = list(batch)\n",
        "        rewards = rewards.reshape(-1, 1)\n",
        "        actions = torch.from_numpy(np.array(actions[:,-1],dtype=\"int64\")).reshape(-1, 1)\n",
        "        dones = dones.reshape(-1, 1)\n",
        "        states = from_tuple_to_tensor(states)\n",
        "        next_states = from_tuple_to_tensor(next_states)\n",
        "        loss=0\n",
        "\n",
        "        for i,s in enumerate(states):\n",
        "            qvals = self.network.get_qvals(s)\n",
        "            qvals = torch.gather(qvals, 0, actions[i])\n",
        "            next_qvals= self.target_network.get_qvals(next_states[i])\n",
        "            next_qvals_max = torch.max(next_qvals, dim=-1)[0].reshape(-1, 1)\n",
        "            target_qvals = rewards[i] + (1 - dones[i])*self.gamma*next_qvals_max\n",
        "            loss+=torch.mean((qvals - target_qvals) ** 2 * weights)\n",
        "\n",
        "        loss=loss/len(states)\n",
        "\n",
        "        if weights is None:\n",
        "            weights = torch.ones_like(qvals)\n",
        "\n",
        "        td_error = torch.abs(qvals - target_qvals).detach()\n",
        "        rewards.detach()\n",
        "        actions.detach()\n",
        "        dones.detach()\n",
        "        states.detach()\n",
        "        next_states.detach()\n",
        "        return loss,td_error\n",
        "\n",
        "    def save_check(self,ep):\n",
        "        name = \"/content/drive/MyDrive/Colab Notebooks/RL/Demonstration/model_\" + str(ep) + \".pt\"\n",
        "        torch.save(self.state_dict(), name)\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.state_dict(), '/content/drive/MyDrive/Colab Notebooks/RL/Demonstration/model.pt')\n",
        "\n",
        "    def pretrain_save(self):\n",
        "        torch.save(self.state_dict(), '/content/drive/MyDrive/Colab Notebooks/RL/Demonstration/pre_model.pt')\n",
        "\n",
        "    def load(self):\n",
        "        self.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/RL/Demonstration/model.pt'))\n",
        "\n",
        "    def to(self, device):\n",
        "        ret = super().to(device)\n",
        "        ret.device = device\n",
        "        return ret"
      ],
      "metadata": {
        "id": "ucgFbdESij4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN"
      ],
      "metadata": {
        "id": "Bii9DTlmXWd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def from_tuple_to_tensor(tuple_of_np):\n",
        "    tensor = torch.zeros((len(tuple_of_np), tuple_of_np[0].shape[0],tuple_of_np[0].shape[1],tuple_of_np[0].shape[2]))\n",
        "    for i, x in enumerate(tuple_of_np):\n",
        "        tensor[i] = torch.FloatTensor(x)\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "QqfFxcJsXbMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, n_frames,n_actions, hidden_size=32, bias=True):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_frames = n_frames\n",
        "        self.conv1 = nn.Conv2d(n_frames, hidden_size, 7)\n",
        "        self.conv2 = nn.Conv2d(hidden_size, hidden_size, 5)\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.reshape(self.hidden_size, -1).max(axis=1).values\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=0)\n",
        "        return x\n",
        "\n",
        "class Q_network(nn.Module):\n",
        "    def __init__(self, env,  learning_rate=1e-4):\n",
        "        super(Q_network, self).__init__()\n",
        "\n",
        "        self.network = Net(1, env.action_space.n)\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(),lr=learning_rate)\n",
        "        self.gs = transforms.Grayscale()\n",
        "        self.rs = transforms.Resize((64,64))\n",
        "\n",
        "    def greedy_action(self, state):\n",
        "        qvals = self.get_qvals(state)\n",
        "        greedy_a = torch.max(qvals, dim=-1)[1].item()\n",
        "        return greedy_a\n",
        "\n",
        "    def get_qvals(self, state):\n",
        "        state=self.preproc_state(state)\n",
        "        out = self.network(state)\n",
        "        return out\n",
        "\n",
        "    def preproc_state(self, state):\n",
        "        state=state.numpy()\n",
        "        state = state[:83, :].transpose(2, 0, 1)\n",
        "        state = torch.from_numpy(state)\n",
        "        state = self.gs(state)\n",
        "        state = self.rs(state)\n",
        "        return state / 255"
      ],
      "metadata": {
        "id": "POBitfioXYJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SumTree:\n",
        "    def __init__(self, size):\n",
        "        self.nodes = [0] * (2 * size - 1)\n",
        "        self.data = [0] * size\n",
        "\n",
        "        self.size = size\n",
        "        self.count = 0\n",
        "        self.real_size = 0\n",
        "\n",
        "    @property\n",
        "    def total(self):\n",
        "        return self.nodes[0]\n",
        "\n",
        "    def update(self, data_idx, value):\n",
        "        idx = data_idx + self.size - 1\n",
        "        change = value - self.nodes[idx]\n",
        "\n",
        "        self.nodes[idx] = value\n",
        "\n",
        "        parent = (idx - 1) // 2\n",
        "        while parent >= 0:\n",
        "            self.nodes[parent] += change\n",
        "            parent = (parent - 1) // 2\n",
        "\n",
        "    def add(self, value, data):\n",
        "        self.data[self.count] = data\n",
        "\n",
        "        self.update(self.count, value)\n",
        "\n",
        "        self.count = (self.count + 1) % self.size\n",
        "        self.real_size = min(self.size, self.real_size + 1)\n",
        "\n",
        "    def get(self, cumsum):\n",
        "        assert cumsum <= self.total\n",
        "\n",
        "        idx = 0\n",
        "        while 2 * idx + 1 < len(self.nodes):\n",
        "            left, right = 2*idx + 1, 2*idx + 2\n",
        "\n",
        "            if cumsum <= self.nodes[left]:\n",
        "                idx = left\n",
        "            else:\n",
        "                idx = right\n",
        "                cumsum = cumsum - self.nodes[left]\n",
        "\n",
        "        data_idx = idx - self.size + 1\n",
        "\n",
        "        return data_idx, self.nodes[idx], self.data[data_idx]\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.real_size\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"SumTree(nodes={self.nodes.__repr__()}, data={self.data.__repr__()})\""
      ],
      "metadata": {
        "id": "AV85jTlogUSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Prioritized_experience_replay_buffer:\n",
        "    def __init__(self,env, device,memory_size=50000, burn_in=10000,eps=1e-2, alpha=0, beta=0.1):\n",
        "        self.env=env\n",
        "        self.tree = SumTree(size=memory_size)\n",
        "        self.device = device\n",
        "        self.memory_size = memory_size\n",
        "        self.burn_in = burn_in\n",
        "        self.count = 0\n",
        "        self.real_size = 0\n",
        "        self.replay_memory = deque(maxlen=memory_size)\n",
        "        self.eps = eps\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.max_priority = eps\n",
        "        self.state = torch.empty(memory_size, env.observation_space._shape[0],env.observation_space._shape[1],env.observation_space._shape[2], dtype=torch.float)\n",
        "        self.action = torch.empty(memory_size, env.action_space.n, dtype=torch.float)\n",
        "        self.reward = torch.empty(memory_size, dtype=torch.float)\n",
        "        self.next_state = torch.empty(memory_size, env.observation_space._shape[0],env.observation_space._shape[1],env.observation_space._shape[2], dtype=torch.float)\n",
        "        self.done = torch.empty(memory_size, dtype=torch.int)\n",
        "\n",
        "    def sample(self, batch_size=32):\n",
        "        assert self.real_size >= batch_size, \"buffer contains less samples than batch size\"\n",
        "        sample_idxs, tree_idxs = [], []\n",
        "        priorities = torch.empty(batch_size, 1, dtype=torch.float)\n",
        "        segment = self.tree.total / batch_size\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            a, b = segment * i, segment * (i + 1)\n",
        "\n",
        "            cumsum = random.uniform(a, b)\n",
        "            tree_idx, priority, sample_idx = self.tree.get(cumsum)\n",
        "            priorities[i] = torch.tensor(priority)\n",
        "            tree_idxs.append(tree_idx)\n",
        "            sample_idxs.append(sample_idx)\n",
        "\n",
        "        probs = priorities / self.tree.total\n",
        "        weights = ((1/self.real_size) * (1/probs)) ** self.beta\n",
        "        weights = weights / weights.max()\n",
        "\n",
        "        batch = (\n",
        "            self.state[sample_idxs],\n",
        "            self.action[sample_idxs],\n",
        "            self.reward[sample_idxs],\n",
        "            self.next_state[sample_idxs],\n",
        "            self.done[sample_idxs]\n",
        "        )\n",
        "        return batch, weights, tree_idxs\n",
        "\n",
        "    def update_priorities(self, data_idxs, priorities):\n",
        "        if isinstance(priorities, torch.Tensor):\n",
        "            priorities = priorities.detach().cpu().numpy()\n",
        "\n",
        "        for data_idx, priority in zip(data_idxs, priorities):\n",
        "            priority = (priority + self.eps) ** self.alpha\n",
        "            self.tree.update(data_idx, priority)\n",
        "            self.max_priority = max(self.max_priority, priority)\n",
        "\n",
        "    def burn_in_capacity(self):\n",
        "        return len(self.replay_memory) / self.burn_in\n",
        "\n",
        "    def capacity(self):\n",
        "        return len(self.replay_memory) / self.memory_size\n",
        "\n",
        "    def get_memory_size(self):\n",
        "        return self.real_size\n",
        "\n",
        "    def add(self, state, action, reward,done, next_state):\n",
        "        self.tree.add(self.max_priority, self.count)\n",
        "\n",
        "        self.state[self.count] = torch.as_tensor(state)\n",
        "        self.action[self.count] = torch.as_tensor(action)\n",
        "        self.reward[self.count] = torch.as_tensor(reward)\n",
        "        self.next_state[self.count] = torch.as_tensor(next_state)\n",
        "        self.done[self.count] = torch.as_tensor(done)\n",
        "\n",
        "        self.count = (self.count + 1) % self.memory_size\n",
        "        self.real_size = min(self.memory_size, self.real_size + 1)"
      ],
      "metadata": {
        "id": "x6bxDBhQXgLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "objects = []\n",
        "with (open(\"/content/drive/MyDrive/Colab Notebooks/RL/Demonstration/expert_tracefinal.pkl\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            objects.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break\n",
        "\n",
        "obs = objects[0][0]\n",
        "act = objects[0][1]\n",
        "rew = objects[0][2]\n",
        "next = objects[0][3]\n",
        "done = objects[0][4]"
      ],
      "metadata": {
        "id": "O2tmXYRvnbop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num states: \"+str(len(objects[0][0])))\n",
        "print(\"Num actions: \"+str(len(objects[0][1])))\n",
        "print(\"Num rewards: \"+str(len(objects[0][2])))\n",
        "print(\"Num next states: \"+str(len(objects[0][3])))\n",
        "print(\"Num dones: \"+str(len(objects[0][4])))"
      ],
      "metadata": {
        "id": "mhtL_cIBpbwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('MontezumaRevenge-v4', render_mode='rgb_array')\n",
        "buffer = Prioritized_experience_replay_buffer(env,device)"
      ],
      "metadata": {
        "id": "8muzu9P8aLCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(obs)):\n",
        "  buffer.add(obs[i], act[i], rew[i], done[i],next[i])"
      ],
      "metadata": {
        "id": "spEv7KDGv-ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Policy(buffer,env,device)"
      ],
      "metadata": {
        "id": "li1jlTeFwyqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.expert_train()"
      ],
      "metadata": {
        "id": "GK4FURJHw4Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.train()"
      ],
      "metadata": {
        "id": "yVVLBOP04OkX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}