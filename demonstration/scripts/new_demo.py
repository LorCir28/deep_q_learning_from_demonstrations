# -*- coding: utf-8 -*-
"""New_demo (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l-ZwRNpqj6M2GVN25OH6AIeo6dYNgFcq
"""


import torch
import torch.nn as nn
import torch.nn.functional as F
import random
from collections import namedtuple, deque
import numpy as np
import math
from numpy.random import choice
import gym 
import operator
from copy import deepcopy

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

class ReplayBuffer:
    """Fixed-size buffer to store experience tuples."""

    def __init__(self, action_size, buffer_size, batch_size, experiences_per_sampling, seed, compute_weights):
        """Initialize a ReplayBuffer object.

        Params
        ======
            action_size (int): dimension of each action
            buffer_size (int): maximum size of buffer
            experiences_per_sampling (int): number of experiences to sample during a sampling iteration
            batch_size (int): size of each training batch
            seed (int): random seed
        """
        self.action_size = action_size
        self.buffer_size = buffer_size
        self.batch_size = batch_size
        self.experiences_per_sampling = experiences_per_sampling
        
        self.alpha = 0.5
        self.alpha_decay_rate = 0.99
        self.beta = 0.5
        self.beta_growth_rate = 1.001
        self.seed = random.seed(seed)
        self.compute_weights = compute_weights
        self.experience_count = 0
        
        self.experience = namedtuple("Experience", 
            field_names=["state", "action", "reward", "next_state", "done"])
        self.data = namedtuple("Data", 
            field_names=["priority", "probability", "weight","index"])

        indexes = []
        datas = []
        for i in range(buffer_size):
            indexes.append(i)
            d = self.data(0,0,0,i)
            datas.append(d)
        
        self.memory = {key: self.experience for key in indexes}
        self.memory_data = {key: data for key,data in zip(indexes, datas)}
        self.sampled_batches = []
        self.current_batch = 0
        self.priorities_sum_alpha = 0
        self.priorities_max = 1
        self.weights_max = 1
    
    def update_priorities(self, tds, indices):
        for td, index in zip(tds, indices):
            N = min(self.experience_count, self.buffer_size)

            updated_priority = td[0]
            if updated_priority > self.priorities_max:
                self.priorities_max = updated_priority
            
            if self.compute_weights:
                updated_weight = ((N * updated_priority)**(-self.beta))/self.weights_max
                if updated_weight > self.weights_max:
                    self.weights_max = updated_weight
            else:
                updated_weight = 1

            old_priority = self.memory_data[index].priority
            self.priorities_sum_alpha += updated_priority**self.alpha - old_priority**self.alpha
            updated_probability = td[0]**self.alpha / self.priorities_sum_alpha
            data = self.data(updated_priority, updated_probability, updated_weight, index) 
            self.memory_data[index] = data

    def update_memory_sampling(self):
        """Randomly sample X batches of experiences from memory."""
        # X is the number of steps before updating memory
        self.current_batch = 0
        values = list(self.memory_data.values())
        random_values = random.choices(self.memory_data, 
                                       [data.probability for data in values], 
                                       k=self.experiences_per_sampling)
        self.sampled_batches = [random_values[i:i + self.batch_size] 
                                    for i in range(0, len(random_values), self.batch_size)]

    def update_parameters(self):
        self.alpha *= self.alpha_decay_rate
        self.beta *= self.beta_growth_rate
        if self.beta > 1:
            self.beta = 1
        N = min(self.experience_count, self.buffer_size)
        self.priorities_sum_alpha = 0
        sum_prob_before = 0
        for element in self.memory_data.values():
            sum_prob_before += element.probability
            self.priorities_sum_alpha += element.priority**self.alpha
        sum_prob_after = 0
        for element in self.memory_data.values():
            probability = element.priority**self.alpha / self.priorities_sum_alpha
            sum_prob_after += probability
            weight = 1
            if self.compute_weights:
                weight = ((N *  element.probability)**(-self.beta))/self.weights_max
            d = self.data(element.priority, probability, weight, element.index)
            self.memory_data[element.index] = d
        print("sum_prob before", sum_prob_before)
        print("sum_prob after : ", sum_prob_after)
    
    def add(self, state, action, reward, next_state, done):
        """Add a new experience to memory."""
        self.experience_count += 1
        index = self.experience_count % self.buffer_size
        state = state.transpose(2,0,1)
        next_state = next_state.transpose(2,0,1)

        if self.experience_count > self.buffer_size:
            temp = self.memory_data[index]
            self.priorities_sum_alpha -= temp.priority**self.alpha
            if temp.priority == self.priorities_max:
                self.memory_data[index].priority = 0
                self.priorities_max = max(self.memory_data.items(), key=operator.itemgetter(1)).priority
            if self.compute_weights:
                if temp.weight == self.weights_max:
                    self.memory_data[index].weight = 0
                    self.weights_max = max(self.memory_data.items(), key=operator.itemgetter(2)).weight

        priority = self.priorities_max
        weight = self.weights_max
        self.priorities_sum_alpha += priority ** self.alpha
        probability = priority ** self.alpha / self.priorities_sum_alpha
        e = self.experience(state, action, reward, next_state, done)
        self.memory[index] = e
        d = self.data(priority, probability, weight, index)
        self.memory_data[index] = d
            
    def sample(self):
        sampled_batch = self.sampled_batches[self.current_batch]
        self.current_batch += 1
        experiences = []
        weights = []
        indices = []
        
        for data in sampled_batch:
            experiences.append(self.memory.get(data.index))
            weights.append(data.weight)
            indices.append(data.index)

        #print("+++++++++++++++",str(type(experiences[1].state)))

        # a = []
        # for _ in range(9):
        #   b = np.array(experiences[0].state)
        #   b = torch.from_numpy(b)
        #   a.append(b)

        

        

        
        
        
        states = torch.stack([torch.from_numpy(np.array(e.state)) for e in experiences if e is not None]).float().to(device)
        actions = torch.stack([torch.from_numpy(np.array(e.action)) for e in experiences if e is not None]).long().to(device)
        rewards = torch.stack([torch.from_numpy(np.array(e.reward)) for e in experiences if e is not None]).float().to(device)
        next_states = torch.stack([torch.from_numpy(np.array(e.next_state)) for e in experiences if e is not None]).float().to(device)
        dones = torch.stack([torch.from_numpy(np.array(e.done)) for e in experiences if e is not None]).float().to(device)
        # states = torch.from_numpy(
        #     np.vstack([e.state for e in experiences if e is not None])).float().to(device)
        # actions = torch.from_numpy(
        #     np.vstack([e.action for e in experiences if e is not None])).long().to(device)
        # rewards = torch.from_numpy(
        #     np.vstack([e.reward for e in experiences if e is not None])).float().to(device)
        # next_states = torch.from_numpy(
        #     np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)
        # dones = torch.from_numpy(
        #     np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)

        #print("-------------------",str(states.shape))

        return (states, actions, rewards, next_states, dones, weights, indices)

    def __len__(self):
        """Return the current size of internal memory."""
        return len(self.memory)

import pickle
objects = []
with (open("/content/drive/MyDrive/Colab Notebooks/RL/expert_tracefirst_final.pkl", "rb")) as openfile:
    while True:
        try:
            objects.append(pickle.load(openfile))
        except EOFError:
            break

obs = objects[0][0]
act = objects[0][1]
rew = objects[0][2]
next = objects[0][3]
done = objects[0][4]

class ImitationAgent(nn.Module):
  def __init__(self, num_actions,batch_size=64):
    super(ImitationAgent, self).__init__()
    self.batch_size = batch_size
    self.layer1 = nn.Sequential(
        nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),
        nn.BatchNorm2d(96),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 3, stride = 2))
    self.layer2 = nn.Sequential(
        nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 3, stride = 2))
    self.layer3 = nn.Sequential(
        nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(384),
        nn.ReLU())
    self.layer4 = nn.Sequential(
        nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(384),
        nn.ReLU())
    self.layer5 = nn.Sequential(
        nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(256),
        nn.ReLU(),
        nn.MaxPool2d(kernel_size = 3, stride = 2))
    self.fc = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(3840, 4096),
        nn.ReLU())
    self.fc1 = nn.Sequential(
        nn.Dropout(0.5),
        nn.Linear(4096, 4096),
        nn.ReLU())
    self.fc2= nn.Sequential(
        nn.Linear(4096, num_actions))

  def forward(self, x):
      out = self.preproc_state(x)
      out = self.layer1(out)
      out = self.layer2(out)
      out = self.layer3(out)
      out = self.layer4(out)
      out = self.layer5(out)
      out = out.reshape(out.size(0), -1)
      out = self.fc(out)
      out = self.fc1(out)
      out = self.fc2(out)
      return out,F.softmax(out, dim=1)

  def act(self, state):
      # Stack 4 states
      #state = torch.vstack([self.preproc_state(state) for i in range(1)]).unsqueeze(0)
      
      # Get Action Probabilities
      probs,_ = self.forward(state)
      
      
      # Return Action and LogProb
      action = probs.argmax(-1)
      return action.item()
    
  def preproc_state(self, state):
      # State Preprocessing
      #state = state.transpose(2,0,1) #Torch wants images in format (channels, height, width)
      #state = torch.from_numpy(state)
      
      return state/255 # normalize

env = gym.make("MontezumaRevenge-v4",render_mode="rgb_array")

BUFFER_SIZE = int(1e5)      # replay buffer size
BATCH_SIZE = 64             # minibatch size
GAMMA = 0.99                # discount factor
TAU = 1e-3                  # for soft update of target parameters
LR = 5e-4                   # learning rate 
UPDATE_NN_EVERY = 1        # how often to update the network

# prioritized experience replay
UPDATE_MEM_EVERY = 20          # how often to update the priorities
UPDATE_MEM_PAR_EVERY = 3000     # how often to update the hyperparameters
EXPERIENCES_PER_SAMPLING = 64
print(EXPERIENCES_PER_SAMPLING)
memory = ReplayBuffer(env.action_space.n, BUFFER_SIZE, BATCH_SIZE, EXPERIENCES_PER_SAMPLING, 0, True)

for i in range(len(obs)//8):
  memory.add(obs[i],act[i],rew[i],next[i],done[i])
memory.update_memory_sampling()

memory.sample()

memory.experience_count

net = ImitationAgent(env.action_space.n)
tgt = deepcopy(net)
net.to(device)
tgt.to(device)

loss = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(),lr=LR)

def expert_train():
  gamma = 0.99
  tgt_update_freq = 10
  epochs = 10
  epsilon = 1.0
  plays = 0

  print("Training started!")

  for elem in range(memory.experience_count//64):
      optimizer.zero_grad()
      memory.update_memory_sampling()
      states, actions, rewards, next_states, dones,weights,tree_idxs = memory.sample()
      
      loss=0

      for i,st in enumerate(states):
        st = st.to(device)
        q_vals,_ = net(st.unsqueeze(0))
        st.detach()

        nx = next_states[i].to(device)
        next_q_vals,_ = tgt(nx.unsqueeze(0))
        nx.detach()
        next_q_vals = torch.max(next_q_vals, dim=-1)[0].reshape(-1, 1)

        target_qvals = rewards[i] + (1 - dones[i])*gamma*next_q_vals
        loss+=torch.mean(((q_vals - target_qvals)**2)[0] * weights[i])



      loss=loss/len(states)
      td_error = torch.abs(q_vals - target_qvals).detach()
      
      memory.update_priorities(td_error,tree_idxs)
      loss.backward()
      optimizer.step()

      if elem % tgt_update_freq == 0:
        tgt.load_state_dict(net.state_dict())
        print("Updated tgt")

      if plays > 200:
        done = True
        plays = 0
      else:
        plays += 1

      
      print("Episode {} | Loss {:.2f}".format(elem,loss))
      
      # print("States: ",states.shape)
      # print("Actions: ",actions.shape)
      # print("Rewards: ",rewards.shape)
      # print("Next_states: ",next_states.shape)
      # print("Dones: ",dones.shape)
      # print("Weights: ",len(weights))
      # print("Indices: ",len(tree_idxs))

expert_train()

def train():
  gamma = 0.99
  tgt_update_freq = 10
  epochs = 10
  epsilon = 1.0
  plays = 0

  print("Training started!")

  for epoch in range(epochs):
    s = env.reset()
    done = False

    while not done:
      p = np.random.random()
      
      if p < epsilon:
        action = env.action_space.sample()
      else:
        s0 = torch.from_numpy(s.transpose(2,0,1))
        s0 = s0.to(device)
        action = net.act(s0.unsqueeze(0))
        s0.detach()

      next_s, r, done, _ = env.step(action)
      memory.add(s,action,r,next_s,done)
      memory.update_memory_sampling()

      optimizer.zero_grad()
      states, actions, rewards, next_states, dones,weights,tree_idxs = memory.sample()
      
      loss=0

      for i,st in enumerate(states):
        st = st.to(device)
        q_vals,_ = net(st.unsqueeze(0))
        st.detach()

        nx = next_states[i].to(device)
        next_q_vals,_ = tgt(nx.unsqueeze(0))
        nx.detach()
        next_q_vals = torch.max(next_q_vals, dim=-1)[0].reshape(-1, 1)

        target_qvals = rewards[i] + (1 - dones[i])*gamma*next_q_vals
        loss+=torch.mean(((q_vals - target_qvals)**2)[0] * weights[i])



      loss=loss/len(states)
      td_error = torch.abs(q_vals - target_qvals).detach()
      
      memory.update_priorities(td_error,tree_idxs)
      loss.backward()
      optimizer.step()

      if epoch % tgt_update_freq == 0:
        tgt.load_state_dict(net.state_dict())

      if plays > 100:
        done = True
        plays = 0
      else:
        plays += 1

      if done:
        print("Episode {} | Epsilon {} | Loss {:.2f}".format(epoch,epsilon,loss))
        if epsilon >= 0.05:
           epsilon *= 0.7
      
      # print("States: ",states.shape)
      # print("Actions: ",actions.shape)
      # print("Rewards: ",rewards.shape)
      # print("Next_states: ",next_states.shape)
      # print("Dones: ",dones.shape)
      # print("Weights: ",len(weights))
      # print("Indices: ",len(tree_idxs))

train()

net_checkpoint = "/content/drive/MyDrive/Colab Notebooks/RL/Project/net.pt"
torch.save(net.state_dict(), net_checkpoint)