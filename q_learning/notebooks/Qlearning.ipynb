{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIGnEvc0tXsz",
        "outputId": "eb0a4f99-e69a-40f1-a8e3-f19ed6e21281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "LslcVcUsiswb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ALE\n",
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51x3TyFqmNxp",
        "outputId": "cfa0eae4-f2b6-47f9-c424-ab92e839ad2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ALE\n",
            "  Downloading Ale-0.8.4.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ALE\n",
            "  Building wheel for ALE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ALE: filename=Ale-0.8.4-py3-none-any.whl size=70176 sha256=8c240d1072e0211f61e2601fc2e1fa75be204e71f669a37ab6fd732e9714f369\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6e/89/be043555e2e48a57e1797b91174868898b7545a305178016cb\n",
            "Successfully built ALE\n",
            "Installing collected packages: ALE\n",
            "Successfully installed ALE-0.8.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[accept-rom-license,atari]==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (2.2.0)\n",
            "Collecting ale-py~=0.7.1\n",
            "  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (5.10.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (6.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.25.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.64.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.5.4.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (3.11.0)\n",
            "Collecting libtorrent\n",
            "  Using cached libtorrent-2.0.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.0.0)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=018730d15188e5d2930b15ed684fe4a45ad1dd35d3fc50644f1a3243a5dd8739\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.5.4-py3-none-any.whl size=441148 sha256=417ff74cd0de78b38ce4d484708799197ce71b50b2071436c621fefd36428d1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/60/90/db006a24f232de90641041430b5913a601345c9efc4cb883ea\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: libtorrent, gym, AutoROM.accept-rom-license, autorom, ale-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed AutoROM.accept-rom-license-0.5.4 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0 libtorrent-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "import gym\n",
        "import gym.spaces\n",
        "import numpy as np\n",
        "import collections\n",
        "import argparse\n",
        "import time\n",
        "import collections\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "7GzB4go8h6f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DQN model"
      ],
      "metadata": {
        "id": "x8EzP66GiwZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, input_shape, n_actions):\n",
        "    super(DQN, self).__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "      nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "      nn.ReLU()\n",
        "    )\n",
        "\n",
        "    conv_out_size = self._get_conv_out(input_shape)\n",
        "    \n",
        "    self.fc = nn.Sequential(\n",
        "      nn.Linear(conv_out_size, 512),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(512, n_actions)\n",
        "    )\n",
        "\n",
        "  def _get_conv_out(self, shape):\n",
        "    o = self.conv(torch.zeros(1, *shape))\n",
        "    return int(np.prod(o.size()))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "    return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "ucgFbdESij4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrappers\n",
        "OpenAI modules that speed up the training"
      ],
      "metadata": {
        "id": "QQVrmaAeizS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(\n",
        "                np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(\n",
        "                np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + \\\n",
        "              img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(\n",
        "            img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=new_shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            old_space.low.repeat(n_steps, axis=0),\n",
        "            old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(\n",
        "            self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "metadata": {
        "id": "3U_tSYnhi5mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Buffer replay"
      ],
      "metadata": {
        "id": "vd9Un51Jknff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward','done', 'new_state'])\n",
        "\n",
        "class ExperienceBuffer:\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "  \n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    indices = np.random.choice(len(self.buffer), batch_size,replace=False)\n",
        "    states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
        "    return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(next_states)"
      ],
      "metadata": {
        "id": "OUxOYE-BkpT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning with DQN"
      ],
      "metadata": {
        "id": "4nGGDk1lkPOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Constants"
      ],
      "metadata": {
        "id": "NiMyNtpDk7-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_ENV_NAME = \"MontezumaRevenge-v4\"\n",
        "MEAN_REWARD_BOUND = 19.0\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "REPLAY_START_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "EPOCHS = 1000000"
      ],
      "metadata": {
        "id": "G0kQFHykkShG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Agent model"
      ],
      "metadata": {
        "id": "Jn53rdcQlBHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env, exp_buffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            state_a = np.array([self.state], copy=False)\n",
        "            state_v = torch.tensor(state_a).to(device)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(self.state, action, reward,\n",
        "                         is_done, new_state)\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "metadata": {
        "id": "6Dty9NCLlDHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(batch, net, tgt_net, device=\"cpu\"):\n",
        "      states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "      states_v = torch.tensor(np.array(states, copy=False)).to(device)\n",
        "      next_states_v = torch.tensor(np.array(next_states, copy=False)).to(device)\n",
        "      actions_v = torch.tensor(actions).to(device)\n",
        "      rewards_v = torch.tensor(rewards).to(device)\n",
        "      done_mask = torch.BoolTensor(dones).to(device)\n",
        "\n",
        "      state_action_values = net(states_v).gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
        "    \n",
        "      with torch.no_grad():\n",
        "        next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "        next_state_values[done_mask] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "      expected_state_action_values = next_state_values * GAMMA + rewards_v\n",
        "      \n",
        "      return nn.MSELoss()(state_action_values,expected_state_action_values)"
      ],
      "metadata": {
        "id": "mbxDOQrDqVEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_env(DEFAULT_ENV_NAME)\n",
        "\n",
        "net = DQN(env.observation_space.shape,env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape,env.action_space.n).to(device)\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "    reward = agent.play_step(net, epsilon, device=device)\n",
        "\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        \n",
        "        print(\"%d: done %d games, reward %.3f, \"\n",
        "                \"eps %.2f, speed %.2f f/s\" % (\n",
        "              frame_idx, len(total_rewards), m_reward, epsilon,\n",
        "              speed\n",
        "          ))\n",
        "         \n",
        "        if best_m_reward is None or best_m_reward < m_reward:\n",
        "            torch.save(net.state_dict(), DEFAULT_ENV_NAME +\n",
        "                        \"-best_%.0f.dat\" % m_reward)\n",
        "            if best_m_reward is not None:\n",
        "                print(\"Best reward updated %.3f -> %.3f\" % (\n",
        "                    best_m_reward, m_reward))\n",
        "            best_m_reward = m_reward\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "        net_checkpoint = \"/content/drive/MyDrive/Colab Notebooks/RL/Project/net\"+str(epoch+1)+\".pt\"\n",
        "        torch.save(net.state_dict(), net_checkpoint)\n",
        "        tgt_checkpoint = \"/content/drive/MyDrive/Colab Notebooks/RL/Project/tgt\"+str(epoch+1)+\".pt\"\n",
        "        torch.save(tgt_net.state_dict(), tgt_checkpoint)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #print(\"loss: %.2f\" % (loss_t.item()\n",
        "          #))"
      ],
      "metadata": {
        "id": "f50WmbJClJHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}