{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lubiJGbs-XMa",
        "outputId": "3828c79d-e778-4b6d-d80d-c26029e7e804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tTiOTfREOlI",
        "outputId": "6adbc631-9613-4ffb-f536-1ef7c64e5277"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ALE in /usr/local/lib/python3.8/dist-packages (0.8.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license,atari]==0.21.0 in /usr/local/lib/python3.8/dist-packages (0.21.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (2.2.1)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (0.4.2)\n",
            "Requirement already satisfied: ale-py~=0.7.1 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (0.7.5)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (6.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (5.10.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.64.1)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (0.5.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (3.12.0)\n",
            "Requirement already satisfied: libtorrent in /usr/local/lib/python3.8/dist-packages (from AutoROM.accept-rom-license->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ALE\n",
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YykInrFDExP2"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQThZJ9hE1i-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset, random_split\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnW6xB_pEz20"
      },
      "source": [
        "## Pytorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5TP7CkuEyj8"
      },
      "outputs": [],
      "source": [
        "class ExpertDataSet(Dataset):\n",
        "    def __init__(self, expert_observations, expert_actions):\n",
        "        self.observations = expert_observations\n",
        "        self.actions = expert_actions\n",
        "        self.img_transforms=transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.img_transforms(self.observations[index]), self.actions[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQWZo_2GFRlY"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PGTLYSvFV2g"
      },
      "outputs": [],
      "source": [
        "class ImitationAgent(nn.Module):\n",
        "  def __init__(self, num_actions, batch_size=64):\n",
        "    super(ImitationAgent, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    self.layer2 = nn.Sequential(\n",
        "        nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    self.layer3 = nn.Sequential(\n",
        "        nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(384),\n",
        "        nn.ReLU())\n",
        "    self.layer4 = nn.Sequential(\n",
        "        nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(384),\n",
        "        nn.ReLU())\n",
        "    self.layer5 = nn.Sequential(\n",
        "        nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(256),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(3840, 4096),\n",
        "        nn.ReLU())\n",
        "    self.fc1 = nn.Sequential(\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(4096, 4096),\n",
        "        nn.ReLU())\n",
        "    self.fc2= nn.Sequential(\n",
        "        nn.Linear(4096, num_actions))\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.layer1(x)\n",
        "      out = self.layer2(out)\n",
        "      out = self.layer3(out)\n",
        "      out = self.layer4(out)\n",
        "      out = self.layer5(out)\n",
        "      out = out.reshape(out.size(0), -1)\n",
        "      out = self.fc(out)\n",
        "      out = self.fc1(out)\n",
        "      out = self.fc2(out)\n",
        "      return out,F.softmax(out, dim=1)\n",
        "\n",
        "  def act(self, state):\n",
        "      # Stack 4 states\n",
        "      state = torch.vstack([self.preproc_state(state) for i in range(1)]).unsqueeze(0)\n",
        "      \n",
        "      # Get Action Probabilities\n",
        "      _,probs = self.forward(state)\n",
        "      \n",
        "      \n",
        "      # Return Action and LogProb\n",
        "      action = probs.argmax(-1)\n",
        "      return action.item()\n",
        "    \n",
        "  def preproc_state(self, state):\n",
        "      # State Preprocessing\n",
        "      state = state.transpose(2,0,1) #Torch wants images in format (channels, height, width)\n",
        "      state = torch.from_numpy(state)\n",
        "      \n",
        "      return state/255 # normalize\n",
        "        \n",
        "        \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyxs7lfkFXUh"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW5truTamNOg"
      },
      "outputs": [],
      "source": [
        "objects = []\n",
        "with (open(\"/content/drive/MyDrive/Colab Notebooks/RL/Project/expert_tracefirst_final.pkl\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            objects.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break\n",
        "\n",
        "obs = objects[0][0]\n",
        "act = objects[0][1]\n",
        "\n",
        "expert_dataset = ExpertDataSet(obs, act)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgf-Z3wduAaH",
        "outputId": "749f15b2-efff-4047-fb68-94d488e886b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action balancing\n",
            "Action: 0 || Execution: 405\n",
            "Action: 1 || Execution: 305\n",
            "Action: 2 || Execution: 1186\n",
            "Action: 3 || Execution: 377\n",
            "Action: 4 || Execution: 597\n",
            "Action: 5 || Execution: 531\n",
            "Action: 6 || Execution: 824\n",
            "Action: 7 || Execution: 396\n",
            "Action: 8 || Execution: 486\n",
            "Action: 9 || Execution: 464\n",
            "Action: 10 || Execution: 193\n",
            "Action: 11 || Execution: 240\n",
            "Action: 12 || Execution: 449\n",
            "Action: 13 || Execution: 522\n",
            "Action: 14 || Execution: 494\n",
            "Action: 15 || Execution: 266\n",
            "Action: 16 || Execution: 368\n",
            "Action: 17 || Execution: 250\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('MontezumaRevengeNoFrameskip-v4', render_mode='rgb_array')\n",
        "print(\"Action balancing\")\n",
        "val = []\n",
        "exe = []\n",
        "\n",
        "for action in range(env.action_space.n):\n",
        "  a = act.count(action)\n",
        "  print(\"Action: {} || Execution: {}\".format(action,act.count(action)))\n",
        "  val.append(action)\n",
        "  exe.append(a) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "bnMzlXLhvXxI",
        "outputId": "7870262a-977d-4ce5-83cb-d20c6848217f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# creating the dataset\n",
        "\n",
        "courses = val\n",
        "values = exe\n",
        "  \n",
        "fig = plt.figure(figsize = (10, 5))\n",
        " \n",
        "# creating the bar plot\n",
        "plt.bar(courses, values, color ='maroon',\n",
        "        width = 0.4)\n",
        " \n",
        "plt.xlabel(\"Courses offered\")\n",
        "plt.ylabel(\"No. of students enrolled\")\n",
        "plt.title(\"Students enrolled in different courses\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xB2-km8MnvQV",
        "outputId": "bbb5499e-232b-458c-dbf6-83ea533a5175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expert interactions size:  8353\n",
            "8353\n"
          ]
        }
      ],
      "source": [
        "print(\"Expert interactions size: \", len(obs))\n",
        "print(len(act))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aD6xQmxy8Erd"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "  act.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jyr-8s3p4fz"
      },
      "outputs": [],
      "source": [
        "# Define relevant variables for the ML task\n",
        "learning_rate = 0.001\n",
        "num_epochs = 30\n",
        "num_workers = 2\n",
        "batch_size = 640\n",
        "train_prop = 0.7\n",
        "train_size = int(train_prop * len(expert_dataset))\n",
        "test_size = len(expert_dataset) - train_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAT6wkv_p7K8"
      },
      "outputs": [],
      "source": [
        "train_expert_dataset, test_expert_dataset = random_split(expert_dataset, [train_size, test_size])\n",
        "train_loader = torch.utils.data.DataLoader(  dataset=train_expert_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(  dataset=test_expert_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
        "batch_limit = len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPKeJtNyFYWH",
        "outputId": "69e3ac00-1c2f-4fdf-b487-0c9263ffd5ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30 | Batch 1/10 | Training Loss: 2.9183\n",
            "Epoch 1/30 | Batch 2/10 | Training Loss: 2.9325\n",
            "Epoch 1/30 | Batch 3/10 | Training Loss: 2.8967\n",
            "Epoch 1/30 | Batch 4/10 | Training Loss: 2.8638\n",
            "Epoch 1/30 | Batch 5/10 | Training Loss: 2.8444\n",
            "Epoch 1/30 | Batch 6/10 | Training Loss: 2.8005\n",
            "Epoch 1/30 | Batch 7/10 | Training Loss: 2.7762\n",
            "Epoch 1/30 | Batch 8/10 | Training Loss: 2.7559\n",
            "Epoch 1/30 | Batch 9/10 | Training Loss: 2.8307\n",
            "Epoch 1/30 | Batch 10/10 | Training Loss: 2.8029\n",
            "Epoch 2/30 | Batch 1/10 | Training Loss: 2.7452\n",
            "Epoch 2/30 | Batch 2/10 | Training Loss: 2.8250\n",
            "Epoch 2/30 | Batch 3/10 | Training Loss: 2.7224\n",
            "Epoch 2/30 | Batch 4/10 | Training Loss: 2.8015\n",
            "Epoch 2/30 | Batch 5/10 | Training Loss: 2.7814\n",
            "Epoch 2/30 | Batch 6/10 | Training Loss: 2.7637\n",
            "Epoch 2/30 | Batch 7/10 | Training Loss: 2.7900\n",
            "Epoch 2/30 | Batch 8/10 | Training Loss: 2.7368\n",
            "Epoch 2/30 | Batch 9/10 | Training Loss: 2.7331\n",
            "Epoch 2/30 | Batch 10/10 | Training Loss: 2.7355\n",
            "Epoch 3/30 | Batch 1/10 | Training Loss: 2.7263\n",
            "Epoch 3/30 | Batch 2/10 | Training Loss: 2.7135\n",
            "Epoch 3/30 | Batch 3/10 | Training Loss: 2.7227\n",
            "Epoch 3/30 | Batch 4/10 | Training Loss: 2.7034\n",
            "Epoch 3/30 | Batch 5/10 | Training Loss: 2.7228\n",
            "Epoch 3/30 | Batch 6/10 | Training Loss: 2.7231\n",
            "Epoch 3/30 | Batch 7/10 | Training Loss: 2.6675\n",
            "Epoch 3/30 | Batch 8/10 | Training Loss: 2.6939\n",
            "Epoch 3/30 | Batch 9/10 | Training Loss: 2.6766\n",
            "Epoch 3/30 | Batch 10/10 | Training Loss: 2.6630\n",
            "Epoch 4/30 | Batch 1/10 | Training Loss: 2.6395\n",
            "Epoch 4/30 | Batch 2/10 | Training Loss: 2.6705\n",
            "Epoch 4/30 | Batch 3/10 | Training Loss: 2.6208\n",
            "Epoch 4/30 | Batch 4/10 | Training Loss: 2.6415\n",
            "Epoch 4/30 | Batch 5/10 | Training Loss: 2.7098\n",
            "Epoch 4/30 | Batch 6/10 | Training Loss: 2.6190\n",
            "Epoch 4/30 | Batch 7/10 | Training Loss: 2.7127\n",
            "Epoch 4/30 | Batch 8/10 | Training Loss: 2.6114\n",
            "Epoch 4/30 | Batch 9/10 | Training Loss: 2.6334\n",
            "Epoch 4/30 | Batch 10/10 | Training Loss: 2.6239\n",
            "Epoch 5/30 | Batch 1/10 | Training Loss: 2.6048\n",
            "Epoch 5/30 | Batch 2/10 | Training Loss: 2.6003\n",
            "Epoch 5/30 | Batch 3/10 | Training Loss: 2.6199\n",
            "Epoch 5/30 | Batch 4/10 | Training Loss: 2.5495\n",
            "Epoch 5/30 | Batch 5/10 | Training Loss: 2.5787\n",
            "Epoch 5/30 | Batch 6/10 | Training Loss: 2.5754\n",
            "Epoch 5/30 | Batch 7/10 | Training Loss: 2.5948\n",
            "Epoch 5/30 | Batch 8/10 | Training Loss: 2.6130\n",
            "Epoch 5/30 | Batch 9/10 | Training Loss: 2.5891\n",
            "Epoch 5/30 | Batch 10/10 | Training Loss: 2.5806\n",
            "Epoch 6/30 | Batch 1/10 | Training Loss: 2.5137\n",
            "Epoch 6/30 | Batch 2/10 | Training Loss: 2.5495\n",
            "Epoch 6/30 | Batch 3/10 | Training Loss: 2.5298\n",
            "Epoch 6/30 | Batch 4/10 | Training Loss: 2.6089\n",
            "Epoch 6/30 | Batch 5/10 | Training Loss: 2.4995\n",
            "Epoch 6/30 | Batch 6/10 | Training Loss: 2.5253\n",
            "Epoch 6/30 | Batch 7/10 | Training Loss: 2.5857\n",
            "Epoch 6/30 | Batch 8/10 | Training Loss: 2.5167\n",
            "Epoch 6/30 | Batch 9/10 | Training Loss: 2.5518\n",
            "Epoch 6/30 | Batch 10/10 | Training Loss: 2.5298\n",
            "Epoch 7/30 | Batch 1/10 | Training Loss: 2.4954\n",
            "Epoch 7/30 | Batch 2/10 | Training Loss: 2.4845\n",
            "Epoch 7/30 | Batch 3/10 | Training Loss: 2.4854\n",
            "Epoch 7/30 | Batch 4/10 | Training Loss: 2.5226\n",
            "Epoch 7/30 | Batch 5/10 | Training Loss: 2.4907\n",
            "Epoch 7/30 | Batch 6/10 | Training Loss: 2.5965\n",
            "Epoch 7/30 | Batch 7/10 | Training Loss: 2.4781\n",
            "Epoch 7/30 | Batch 8/10 | Training Loss: 2.4556\n",
            "Epoch 7/30 | Batch 9/10 | Training Loss: 2.4951\n",
            "Epoch 7/30 | Batch 10/10 | Training Loss: 2.5608\n",
            "Epoch 8/30 | Batch 1/10 | Training Loss: 2.4076\n",
            "Epoch 8/30 | Batch 2/10 | Training Loss: 2.4934\n",
            "Epoch 8/30 | Batch 3/10 | Training Loss: 2.3845\n",
            "Epoch 8/30 | Batch 4/10 | Training Loss: 2.5279\n",
            "Epoch 8/30 | Batch 5/10 | Training Loss: 2.5247\n",
            "Epoch 8/30 | Batch 6/10 | Training Loss: 2.4770\n",
            "Epoch 8/30 | Batch 7/10 | Training Loss: 2.4906\n",
            "Epoch 8/30 | Batch 8/10 | Training Loss: 2.4272\n",
            "Epoch 8/30 | Batch 9/10 | Training Loss: 2.3979\n",
            "Epoch 8/30 | Batch 10/10 | Training Loss: 2.4141\n",
            "Epoch 9/30 | Batch 1/10 | Training Loss: 2.3840\n",
            "Epoch 9/30 | Batch 2/10 | Training Loss: 2.4145\n",
            "Epoch 9/30 | Batch 3/10 | Training Loss: 2.4503\n",
            "Epoch 9/30 | Batch 4/10 | Training Loss: 2.4589\n",
            "Epoch 9/30 | Batch 5/10 | Training Loss: 2.4580\n",
            "Epoch 9/30 | Batch 6/10 | Training Loss: 2.4691\n",
            "Epoch 9/30 | Batch 7/10 | Training Loss: 2.4051\n",
            "Epoch 9/30 | Batch 8/10 | Training Loss: 2.3967\n",
            "Epoch 9/30 | Batch 9/10 | Training Loss: 2.3897\n",
            "Epoch 9/30 | Batch 10/10 | Training Loss: 2.3937\n",
            "Epoch 10/30 | Batch 1/10 | Training Loss: 2.4199\n",
            "Epoch 10/30 | Batch 2/10 | Training Loss: 2.3518\n",
            "Epoch 10/30 | Batch 3/10 | Training Loss: 2.4265\n",
            "Epoch 10/30 | Batch 4/10 | Training Loss: 2.3581\n",
            "Epoch 10/30 | Batch 5/10 | Training Loss: 2.3561\n",
            "Epoch 10/30 | Batch 6/10 | Training Loss: 2.4798\n",
            "Epoch 10/30 | Batch 7/10 | Training Loss: 2.3484\n",
            "Epoch 10/30 | Batch 8/10 | Training Loss: 2.4244\n",
            "Epoch 10/30 | Batch 9/10 | Training Loss: 2.3875\n",
            "Epoch 10/30 | Batch 10/10 | Training Loss: 2.3702\n",
            "Epoch 11/30 | Batch 1/10 | Training Loss: 2.3862\n",
            "Epoch 11/30 | Batch 2/10 | Training Loss: 2.4349\n",
            "Epoch 11/30 | Batch 3/10 | Training Loss: 2.3249\n",
            "Epoch 11/30 | Batch 4/10 | Training Loss: 2.2976\n",
            "Epoch 11/30 | Batch 5/10 | Training Loss: 2.3208\n",
            "Epoch 11/30 | Batch 6/10 | Training Loss: 2.3990\n",
            "Epoch 11/30 | Batch 7/10 | Training Loss: 2.3533\n",
            "Epoch 11/30 | Batch 8/10 | Training Loss: 2.3872\n",
            "Epoch 11/30 | Batch 9/10 | Training Loss: 2.3352\n",
            "Epoch 11/30 | Batch 10/10 | Training Loss: 2.2707\n",
            "Epoch 12/30 | Batch 1/10 | Training Loss: 2.3240\n",
            "Epoch 12/30 | Batch 2/10 | Training Loss: 2.3519\n",
            "Epoch 12/30 | Batch 3/10 | Training Loss: 2.3804\n",
            "Epoch 12/30 | Batch 4/10 | Training Loss: 2.2994\n",
            "Epoch 12/30 | Batch 5/10 | Training Loss: 2.3833\n",
            "Epoch 12/30 | Batch 6/10 | Training Loss: 2.3100\n",
            "Epoch 12/30 | Batch 7/10 | Training Loss: 2.3243\n",
            "Epoch 12/30 | Batch 8/10 | Training Loss: 2.3484\n",
            "Epoch 12/30 | Batch 9/10 | Training Loss: 2.2701\n",
            "Epoch 12/30 | Batch 10/10 | Training Loss: 2.4806\n",
            "Epoch 13/30 | Batch 1/10 | Training Loss: 2.3682\n",
            "Epoch 13/30 | Batch 2/10 | Training Loss: 2.3077\n",
            "Epoch 13/30 | Batch 3/10 | Training Loss: 2.2957\n",
            "Epoch 13/30 | Batch 4/10 | Training Loss: 2.3296\n",
            "Epoch 13/30 | Batch 5/10 | Training Loss: 2.2849\n",
            "Epoch 13/30 | Batch 6/10 | Training Loss: 2.2813\n",
            "Epoch 13/30 | Batch 7/10 | Training Loss: 2.2332\n",
            "Epoch 13/30 | Batch 8/10 | Training Loss: 2.3209\n",
            "Epoch 13/30 | Batch 9/10 | Training Loss: 2.2880\n",
            "Epoch 13/30 | Batch 10/10 | Training Loss: 2.3873\n",
            "Epoch 14/30 | Batch 1/10 | Training Loss: 2.2979\n",
            "Epoch 14/30 | Batch 2/10 | Training Loss: 2.2678\n",
            "Epoch 14/30 | Batch 3/10 | Training Loss: 2.3187\n",
            "Epoch 14/30 | Batch 4/10 | Training Loss: 2.2789\n",
            "Epoch 14/30 | Batch 5/10 | Training Loss: 2.2485\n",
            "Epoch 14/30 | Batch 6/10 | Training Loss: 2.3421\n",
            "Epoch 14/30 | Batch 7/10 | Training Loss: 2.2654\n",
            "Epoch 14/30 | Batch 8/10 | Training Loss: 2.2490\n",
            "Epoch 14/30 | Batch 9/10 | Training Loss: 2.2086\n",
            "Epoch 14/30 | Batch 10/10 | Training Loss: 2.2998\n",
            "Epoch 15/30 | Batch 1/10 | Training Loss: 2.2418\n",
            "Epoch 15/30 | Batch 2/10 | Training Loss: 2.3304\n",
            "Epoch 15/30 | Batch 3/10 | Training Loss: 2.2727\n",
            "Epoch 15/30 | Batch 4/10 | Training Loss: 2.2389\n",
            "Epoch 15/30 | Batch 5/10 | Training Loss: 2.2418\n",
            "Epoch 15/30 | Batch 6/10 | Training Loss: 2.2896\n",
            "Epoch 15/30 | Batch 7/10 | Training Loss: 2.2360\n",
            "Epoch 15/30 | Batch 8/10 | Training Loss: 2.2943\n",
            "Epoch 15/30 | Batch 9/10 | Training Loss: 2.2107\n",
            "Epoch 15/30 | Batch 10/10 | Training Loss: 2.1391\n",
            "Epoch 16/30 | Batch 1/10 | Training Loss: 2.2948\n",
            "Epoch 16/30 | Batch 2/10 | Training Loss: 2.2006\n",
            "Epoch 16/30 | Batch 3/10 | Training Loss: 2.1512\n",
            "Epoch 16/30 | Batch 4/10 | Training Loss: 2.2722\n",
            "Epoch 16/30 | Batch 5/10 | Training Loss: 2.2130\n",
            "Epoch 16/30 | Batch 6/10 | Training Loss: 2.3528\n",
            "Epoch 16/30 | Batch 7/10 | Training Loss: 2.2199\n",
            "Epoch 16/30 | Batch 8/10 | Training Loss: 2.2101\n",
            "Epoch 16/30 | Batch 9/10 | Training Loss: 2.1888\n",
            "Epoch 16/30 | Batch 10/10 | Training Loss: 2.1547\n",
            "Epoch 17/30 | Batch 1/10 | Training Loss: 2.2420\n",
            "Epoch 17/30 | Batch 2/10 | Training Loss: 2.1919\n",
            "Epoch 17/30 | Batch 3/10 | Training Loss: 2.2082\n",
            "Epoch 17/30 | Batch 4/10 | Training Loss: 2.1941\n",
            "Epoch 17/30 | Batch 5/10 | Training Loss: 2.2330\n",
            "Epoch 17/30 | Batch 6/10 | Training Loss: 2.1591\n",
            "Epoch 17/30 | Batch 7/10 | Training Loss: 2.2555\n",
            "Epoch 17/30 | Batch 8/10 | Training Loss: 2.2125\n",
            "Epoch 17/30 | Batch 9/10 | Training Loss: 2.1949\n",
            "Epoch 17/30 | Batch 10/10 | Training Loss: 2.1586\n",
            "Epoch 18/30 | Batch 1/10 | Training Loss: 2.2216\n",
            "Epoch 18/30 | Batch 2/10 | Training Loss: 2.2246\n",
            "Epoch 18/30 | Batch 3/10 | Training Loss: 2.2568\n",
            "Epoch 18/30 | Batch 4/10 | Training Loss: 2.1409\n",
            "Epoch 18/30 | Batch 5/10 | Training Loss: 2.2895\n"
          ]
        }
      ],
      "source": [
        "student = ImitationAgent(env.action_space.n)\n",
        "student.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(student.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch, (images,labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        results,_ = student(images)\n",
        "        loss = criterion(results, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Epoch {}/{} | Batch {}/{} | Training Loss: {:.4f}'.format(epoch+1, num_epochs, batch+1, batch_limit, loss.item()))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "          net_checkpoint = \"/content/drive/MyDrive/Colab Notebooks/RL/Project/imit\"+str(epoch+1)+\".pt\"\n",
        "          torch.save(student.state_dict(), net_checkpoint)\n",
        "\n",
        "net_checkpoint = \"/content/drive/MyDrive/Colab Notebooks/RL/Project/imit_final.pt\"\n",
        "torch.save(student.state_dict(), net_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsReTH7st3OU",
        "outputId": "1ae99de2-58d8-4317-dc33-886e1dde569f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        ...,\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
              "\n",
              "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          ...,\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images.detach()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
