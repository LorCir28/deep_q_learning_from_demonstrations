{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lubiJGbs-XMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415518f2-e800-4679-a015-2726fbed35ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ALE\n",
        "!pip install gym[atari,accept-rom-license]==0.21.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tTiOTfREOlI",
        "outputId": "950b8072-5f47-48b4-8c9b-ecbc157898f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ALE\n",
            "  Downloading Ale-0.8.4.tar.gz (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ALE\n",
            "  Building wheel for ALE (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ALE: filename=Ale-0.8.4-py3-none-any.whl size=70176 sha256=aa67f6bc0d0c473c5a5fa4661f3252741bef72f8faec184ac06e11982f7d0b10\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6e/89/be043555e2e48a57e1797b91174868898b7545a305178016cb\n",
            "Successfully built ALE\n",
            "Installing collected packages: ALE\n",
            "Successfully installed ALE-0.8.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gym[accept-rom-license,atari]==0.21.0\n",
            "  Downloading gym-0.21.0.tar.gz (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[accept-rom-license,atari]==0.21.0) (2.2.1)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Collecting ale-py~=0.7.1\n",
            "  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.10.0 in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (6.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (5.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (7.1.2)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.5.4.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.10.0->ale-py~=0.7.1->gym[accept-rom-license,atari]==0.21.0) (3.12.0)\n",
            "Collecting libtorrent\n",
            "  Using cached libtorrent-2.0.7-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (8.6 MB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]==0.21.0) (4.0.0)\n",
            "Building wheels for collected packages: gym, AutoROM.accept-rom-license\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616821 sha256=a9c46ddb88f5ddc46e793aac51b9301dab0d56307a32281d6643b19d47ad9f0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/6d/b3/a3a6e10704795c9b9000f1ab2dc480dfe7bed42f5972806e73\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.5.4-py3-none-any.whl size=441148 sha256=fcf80e79a706c3d174395bca06d299257de5d0f7ca64221ae0612b70f9d62347\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/60/90/db006a24f232de90641041430b5913a601345c9efc4cb883ea\n",
            "Successfully built gym AutoROM.accept-rom-license\n",
            "Installing collected packages: libtorrent, gym, AutoROM.accept-rom-license, autorom, ale-py\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "Successfully installed AutoROM.accept-rom-license-0.5.4 ale-py-0.7.5 autorom-0.4.2 gym-0.21.0 libtorrent-2.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "YykInrFDExP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "import torch.utils\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset, random_split\n",
        "import gym\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EQThZJ9hE1i-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch Dataset"
      ],
      "metadata": {
        "id": "nnW6xB_pEz20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExpertDataSet(Dataset):\n",
        "    def __init__(self, expert_observations, expert_actions):\n",
        "        self.observations = expert_observations\n",
        "        self.actions = expert_actions\n",
        "        self.img_transforms=transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.img_transforms(self.observations[index]), self.actions[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)"
      ],
      "metadata": {
        "id": "V5TP7CkuEyj8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VFvfdQF1HeTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "YQWZo_2GFRlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImitationAgent(nn.Module):\n",
        "  def __init__(self, num_actions):\n",
        "    super(ImitationAgent, self).__init__()\n",
        "    self.gs = transforms.Grayscale()\n",
        "    self.rs = transforms.Resize((64,64))\n",
        "    \n",
        "    ## Activation functions\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    ## Convo Layers\n",
        "    self.c1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7)\n",
        "    \n",
        "\n",
        "    self.c2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5)\n",
        "    \n",
        "\n",
        "    ## FC Layers\n",
        "    self.fc1 = nn.Linear(in_features=32, out_features=32)\n",
        "    self.fc2 = nn.Linear(in_features=32, out_features=num_actions)\n",
        "\n",
        "  def forward(self, x):\n",
        "    ## 1st Convo Layer\n",
        "    x = self.c1(x)\n",
        "    x = self.relu(x)\n",
        "    \n",
        "\n",
        "    ## 2nd Convo Layer\n",
        "    x = self.c2(x)\n",
        "    x = self.relu(x)\n",
        "    \n",
        "     \n",
        "    ## 1st FC Layer\n",
        "    batch_size = x.shape[0]\n",
        "    x = x.reshape(batch_size, 32, -1).max(axis=2).values\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    ## 2nd FC Layer\n",
        "    x = self.fc2(x)\n",
        "    p = F.softmax(x, dim=1)\n",
        "\n",
        "    return x,p\n",
        "\n",
        "  def act(self, state):\n",
        "    # Stack 4 states\n",
        "    #state = torch.vstack([self.preproc_state(state) for state in states]).unsqueeze(0).to(self.device)\n",
        "    \n",
        "    # Get Action Probabilities\n",
        "    probs = self.forward(state).cpu()\n",
        "    \n",
        "    # Return Action and LogProb\n",
        "    action = probs.argmax(-1)\n",
        "    return action.item()\n",
        "    \n",
        "  def preproc_state(self, state):\n",
        "    # State Preprocessing\n",
        "    state = state.transpose(2,0,1) #Torch wants images in format (channels, height, width)\n",
        "    state = torch.from_numpy(state)\n",
        "    \n",
        "    return state # normalize"
      ],
      "metadata": {
        "id": "2PGTLYSvFV2g"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "oyxs7lfkFXUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "objects = []\n",
        "with (open(\"/content/drive/MyDrive/Colab Notebooks/RL/Project/expert_trace.pkl\", \"rb\")) as openfile:\n",
        "    while True:\n",
        "        try:\n",
        "            objects.append(pickle.load(openfile))\n",
        "        except EOFError:\n",
        "            break\n",
        "\n",
        "obs = objects[0][0]\n",
        "act = objects[0][1]\n",
        "expert_dataset = ExpertDataSet(obs, act)\n",
        "\n",
        "env = gym.make('MontezumaRevenge-v4', render_mode='rgb_array')\n",
        "# Define relevant variables for the ML task\n",
        "learning_rate = 0.005\n",
        "num_epochs = 200\n",
        "num_workers = 2\n",
        "batch_size = 64\n",
        "train_prop = 0.8\n",
        "train_size = int(train_prop * len(expert_dataset))\n",
        "test_size = len(expert_dataset) - train_size\n",
        "\n",
        "train_expert_dataset, test_expert_dataset = random_split(expert_dataset, [train_size, test_size])\n",
        "train_loader = torch.utils.data.DataLoader(  dataset=train_expert_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
        "test_loader = torch.utils.data.DataLoader(  dataset=test_expert_dataset, batch_size=batch_size, shuffle=True,num_workers=num_workers)\n",
        "\n",
        "student = ImitationAgent(env.action_space.n)\n",
        "student.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(student.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch, (images,labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        results,_ = student(images)\n",
        "        loss = criterion(results, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('Epoch {}/{} | Batch {}/{} | Training Loss: {:.4f}'.format(epoch+1, num_epochs, batch+1, batch_size, loss.item()))\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "          net_checkpoint = \"/content/drive/MyDrive/Colab Notebooks/RL/Project/imit\"+str(epoch+1)+\".pt\"\n",
        "          torch.save(student.state_dict(), net_checkpoint)\n",
        "\n",
        "net_checkpoint = \"/content/drive/MyDrive/Colab Notebooks/RL/Project/imit_final.pt\"\n",
        "torch.save(student.state_dict(), net_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPKeJtNyFYWH",
        "outputId": "0cf9e0e7-bfbf-4526-fca8-7ecf02590093"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200 | Batch 1/64 | Training Loss: 2.9698\n",
            "Epoch 1/200 | Batch 2/64 | Training Loss: 2.9398\n",
            "Epoch 1/200 | Batch 3/64 | Training Loss: 2.9680\n",
            "Epoch 1/200 | Batch 4/64 | Training Loss: 2.9399\n",
            "Epoch 1/200 | Batch 5/64 | Training Loss: 2.9247\n",
            "Epoch 1/200 | Batch 6/64 | Training Loss: 2.8951\n",
            "Epoch 1/200 | Batch 7/64 | Training Loss: 2.8846\n",
            "Epoch 1/200 | Batch 8/64 | Training Loss: 2.8708\n",
            "Epoch 2/200 | Batch 1/64 | Training Loss: 2.8574\n",
            "Epoch 2/200 | Batch 2/64 | Training Loss: 2.8171\n",
            "Epoch 2/200 | Batch 3/64 | Training Loss: 2.7967\n",
            "Epoch 2/200 | Batch 4/64 | Training Loss: 2.7722\n",
            "Epoch 2/200 | Batch 5/64 | Training Loss: 2.7199\n",
            "Epoch 2/200 | Batch 6/64 | Training Loss: 2.6986\n",
            "Epoch 2/200 | Batch 7/64 | Training Loss: 2.6642\n",
            "Epoch 2/200 | Batch 8/64 | Training Loss: 2.5632\n",
            "Epoch 3/200 | Batch 1/64 | Training Loss: 2.5027\n",
            "Epoch 3/200 | Batch 2/64 | Training Loss: 2.4962\n",
            "Epoch 3/200 | Batch 3/64 | Training Loss: 2.2782\n",
            "Epoch 3/200 | Batch 4/64 | Training Loss: 2.4069\n",
            "Epoch 3/200 | Batch 5/64 | Training Loss: 2.2533\n",
            "Epoch 3/200 | Batch 6/64 | Training Loss: 2.2045\n",
            "Epoch 3/200 | Batch 7/64 | Training Loss: 2.2389\n",
            "Epoch 3/200 | Batch 8/64 | Training Loss: 2.2645\n",
            "Epoch 4/200 | Batch 1/64 | Training Loss: 2.0019\n",
            "Epoch 4/200 | Batch 2/64 | Training Loss: 2.1732\n",
            "Epoch 4/200 | Batch 3/64 | Training Loss: 2.0966\n",
            "Epoch 4/200 | Batch 4/64 | Training Loss: 2.1090\n",
            "Epoch 4/200 | Batch 5/64 | Training Loss: 2.0450\n",
            "Epoch 4/200 | Batch 6/64 | Training Loss: 1.9020\n",
            "Epoch 4/200 | Batch 7/64 | Training Loss: 1.8138\n",
            "Epoch 4/200 | Batch 8/64 | Training Loss: 2.0654\n",
            "Epoch 5/200 | Batch 1/64 | Training Loss: 1.8895\n",
            "Epoch 5/200 | Batch 2/64 | Training Loss: 1.8508\n",
            "Epoch 5/200 | Batch 3/64 | Training Loss: 1.8551\n",
            "Epoch 5/200 | Batch 4/64 | Training Loss: 1.9108\n",
            "Epoch 5/200 | Batch 5/64 | Training Loss: 1.6839\n",
            "Epoch 5/200 | Batch 6/64 | Training Loss: 1.7909\n",
            "Epoch 5/200 | Batch 7/64 | Training Loss: 1.7995\n",
            "Epoch 5/200 | Batch 8/64 | Training Loss: 1.3890\n",
            "Epoch 6/200 | Batch 1/64 | Training Loss: 1.5669\n",
            "Epoch 6/200 | Batch 2/64 | Training Loss: 1.9975\n",
            "Epoch 6/200 | Batch 3/64 | Training Loss: 1.9505\n",
            "Epoch 6/200 | Batch 4/64 | Training Loss: 1.5516\n",
            "Epoch 6/200 | Batch 5/64 | Training Loss: 1.5183\n",
            "Epoch 6/200 | Batch 6/64 | Training Loss: 1.8589\n",
            "Epoch 6/200 | Batch 7/64 | Training Loss: 1.8529\n",
            "Epoch 6/200 | Batch 8/64 | Training Loss: 1.5725\n",
            "Epoch 7/200 | Batch 1/64 | Training Loss: 1.5023\n",
            "Epoch 7/200 | Batch 2/64 | Training Loss: 1.7949\n",
            "Epoch 7/200 | Batch 3/64 | Training Loss: 1.5858\n",
            "Epoch 7/200 | Batch 4/64 | Training Loss: 1.7849\n",
            "Epoch 7/200 | Batch 5/64 | Training Loss: 1.7576\n",
            "Epoch 7/200 | Batch 6/64 | Training Loss: 1.6539\n",
            "Epoch 7/200 | Batch 7/64 | Training Loss: 1.7146\n",
            "Epoch 7/200 | Batch 8/64 | Training Loss: 1.5882\n",
            "Epoch 8/200 | Batch 1/64 | Training Loss: 1.8946\n",
            "Epoch 8/200 | Batch 2/64 | Training Loss: 1.4523\n",
            "Epoch 8/200 | Batch 3/64 | Training Loss: 1.7542\n",
            "Epoch 8/200 | Batch 4/64 | Training Loss: 1.6875\n",
            "Epoch 8/200 | Batch 5/64 | Training Loss: 1.7102\n",
            "Epoch 8/200 | Batch 6/64 | Training Loss: 1.5979\n",
            "Epoch 8/200 | Batch 7/64 | Training Loss: 1.6646\n",
            "Epoch 8/200 | Batch 8/64 | Training Loss: 2.0555\n",
            "Epoch 9/200 | Batch 1/64 | Training Loss: 1.6751\n",
            "Epoch 9/200 | Batch 2/64 | Training Loss: 1.7252\n",
            "Epoch 9/200 | Batch 3/64 | Training Loss: 1.6976\n",
            "Epoch 9/200 | Batch 4/64 | Training Loss: 1.6181\n",
            "Epoch 9/200 | Batch 5/64 | Training Loss: 1.6807\n",
            "Epoch 9/200 | Batch 6/64 | Training Loss: 1.7368\n",
            "Epoch 9/200 | Batch 7/64 | Training Loss: 1.7560\n",
            "Epoch 9/200 | Batch 8/64 | Training Loss: 1.6588\n",
            "Epoch 10/200 | Batch 1/64 | Training Loss: 1.5606\n",
            "Epoch 10/200 | Batch 2/64 | Training Loss: 1.6678\n",
            "Epoch 10/200 | Batch 3/64 | Training Loss: 1.6155\n",
            "Epoch 10/200 | Batch 4/64 | Training Loss: 1.5871\n",
            "Epoch 10/200 | Batch 5/64 | Training Loss: 1.6854\n",
            "Epoch 10/200 | Batch 6/64 | Training Loss: 1.6321\n",
            "Epoch 10/200 | Batch 7/64 | Training Loss: 2.1134\n",
            "Epoch 10/200 | Batch 8/64 | Training Loss: 1.5124\n",
            "Epoch 11/200 | Batch 1/64 | Training Loss: 1.8275\n",
            "Epoch 11/200 | Batch 2/64 | Training Loss: 1.6826\n",
            "Epoch 11/200 | Batch 3/64 | Training Loss: 1.9028\n",
            "Epoch 11/200 | Batch 4/64 | Training Loss: 1.6356\n",
            "Epoch 11/200 | Batch 5/64 | Training Loss: 1.6883\n",
            "Epoch 11/200 | Batch 6/64 | Training Loss: 1.6226\n",
            "Epoch 11/200 | Batch 7/64 | Training Loss: 1.5592\n",
            "Epoch 11/200 | Batch 8/64 | Training Loss: 1.6604\n",
            "Epoch 12/200 | Batch 1/64 | Training Loss: 1.6941\n",
            "Epoch 12/200 | Batch 2/64 | Training Loss: 1.7902\n",
            "Epoch 12/200 | Batch 3/64 | Training Loss: 1.6475\n",
            "Epoch 12/200 | Batch 4/64 | Training Loss: 1.7205\n",
            "Epoch 12/200 | Batch 5/64 | Training Loss: 1.4847\n",
            "Epoch 12/200 | Batch 6/64 | Training Loss: 1.7760\n",
            "Epoch 12/200 | Batch 7/64 | Training Loss: 1.6596\n",
            "Epoch 12/200 | Batch 8/64 | Training Loss: 1.5604\n",
            "Epoch 13/200 | Batch 1/64 | Training Loss: 1.5416\n",
            "Epoch 13/200 | Batch 2/64 | Training Loss: 1.6152\n",
            "Epoch 13/200 | Batch 3/64 | Training Loss: 1.5035\n",
            "Epoch 13/200 | Batch 4/64 | Training Loss: 1.7700\n",
            "Epoch 13/200 | Batch 5/64 | Training Loss: 1.6652\n",
            "Epoch 13/200 | Batch 6/64 | Training Loss: 1.8712\n",
            "Epoch 13/200 | Batch 7/64 | Training Loss: 1.6836\n",
            "Epoch 13/200 | Batch 8/64 | Training Loss: 1.7191\n",
            "Epoch 14/200 | Batch 1/64 | Training Loss: 1.7740\n",
            "Epoch 14/200 | Batch 2/64 | Training Loss: 1.7435\n",
            "Epoch 14/200 | Batch 3/64 | Training Loss: 1.6014\n",
            "Epoch 14/200 | Batch 4/64 | Training Loss: 1.7154\n",
            "Epoch 14/200 | Batch 5/64 | Training Loss: 1.6663\n",
            "Epoch 14/200 | Batch 6/64 | Training Loss: 1.5159\n",
            "Epoch 14/200 | Batch 7/64 | Training Loss: 1.6609\n",
            "Epoch 14/200 | Batch 8/64 | Training Loss: 1.5681\n",
            "Epoch 15/200 | Batch 1/64 | Training Loss: 1.8552\n",
            "Epoch 15/200 | Batch 2/64 | Training Loss: 1.6269\n",
            "Epoch 15/200 | Batch 3/64 | Training Loss: 1.8225\n",
            "Epoch 15/200 | Batch 4/64 | Training Loss: 1.7455\n",
            "Epoch 15/200 | Batch 5/64 | Training Loss: 1.5787\n",
            "Epoch 15/200 | Batch 6/64 | Training Loss: 1.6456\n",
            "Epoch 15/200 | Batch 7/64 | Training Loss: 1.3925\n",
            "Epoch 15/200 | Batch 8/64 | Training Loss: 1.6029\n",
            "Epoch 16/200 | Batch 1/64 | Training Loss: 1.5755\n",
            "Epoch 16/200 | Batch 2/64 | Training Loss: 1.7222\n",
            "Epoch 16/200 | Batch 3/64 | Training Loss: 1.4679\n",
            "Epoch 16/200 | Batch 4/64 | Training Loss: 1.7893\n",
            "Epoch 16/200 | Batch 5/64 | Training Loss: 1.7142\n",
            "Epoch 16/200 | Batch 6/64 | Training Loss: 1.5978\n",
            "Epoch 16/200 | Batch 7/64 | Training Loss: 1.7821\n",
            "Epoch 16/200 | Batch 8/64 | Training Loss: 1.5844\n",
            "Epoch 17/200 | Batch 1/64 | Training Loss: 1.5475\n",
            "Epoch 17/200 | Batch 2/64 | Training Loss: 1.5545\n",
            "Epoch 17/200 | Batch 3/64 | Training Loss: 1.7366\n",
            "Epoch 17/200 | Batch 4/64 | Training Loss: 1.7159\n",
            "Epoch 17/200 | Batch 5/64 | Training Loss: 1.6608\n",
            "Epoch 17/200 | Batch 6/64 | Training Loss: 1.7661\n",
            "Epoch 17/200 | Batch 7/64 | Training Loss: 1.6929\n",
            "Epoch 17/200 | Batch 8/64 | Training Loss: 1.6169\n",
            "Epoch 18/200 | Batch 1/64 | Training Loss: 1.5694\n",
            "Epoch 18/200 | Batch 2/64 | Training Loss: 1.5761\n",
            "Epoch 18/200 | Batch 3/64 | Training Loss: 1.6823\n",
            "Epoch 18/200 | Batch 4/64 | Training Loss: 1.8325\n",
            "Epoch 18/200 | Batch 5/64 | Training Loss: 1.5399\n",
            "Epoch 18/200 | Batch 6/64 | Training Loss: 1.6845\n",
            "Epoch 18/200 | Batch 7/64 | Training Loss: 1.6017\n",
            "Epoch 18/200 | Batch 8/64 | Training Loss: 1.9474\n",
            "Epoch 19/200 | Batch 1/64 | Training Loss: 1.5861\n",
            "Epoch 19/200 | Batch 2/64 | Training Loss: 1.8134\n",
            "Epoch 19/200 | Batch 3/64 | Training Loss: 1.5805\n",
            "Epoch 19/200 | Batch 4/64 | Training Loss: 1.7855\n",
            "Epoch 19/200 | Batch 5/64 | Training Loss: 1.6993\n",
            "Epoch 19/200 | Batch 6/64 | Training Loss: 1.5594\n",
            "Epoch 19/200 | Batch 7/64 | Training Loss: 1.5608\n",
            "Epoch 19/200 | Batch 8/64 | Training Loss: 1.7697\n",
            "Epoch 20/200 | Batch 1/64 | Training Loss: 1.6122\n",
            "Epoch 20/200 | Batch 2/64 | Training Loss: 1.8652\n",
            "Epoch 20/200 | Batch 3/64 | Training Loss: 1.6335\n",
            "Epoch 20/200 | Batch 4/64 | Training Loss: 1.6908\n",
            "Epoch 20/200 | Batch 5/64 | Training Loss: 1.5974\n",
            "Epoch 20/200 | Batch 6/64 | Training Loss: 1.4939\n",
            "Epoch 20/200 | Batch 7/64 | Training Loss: 1.7480\n",
            "Epoch 20/200 | Batch 8/64 | Training Loss: 1.3640\n",
            "Epoch 21/200 | Batch 1/64 | Training Loss: 1.6701\n",
            "Epoch 21/200 | Batch 2/64 | Training Loss: 1.7708\n",
            "Epoch 21/200 | Batch 3/64 | Training Loss: 1.6872\n",
            "Epoch 21/200 | Batch 4/64 | Training Loss: 1.8699\n",
            "Epoch 21/200 | Batch 5/64 | Training Loss: 1.5971\n",
            "Epoch 21/200 | Batch 6/64 | Training Loss: 1.5070\n",
            "Epoch 21/200 | Batch 7/64 | Training Loss: 1.5661\n",
            "Epoch 21/200 | Batch 8/64 | Training Loss: 1.5092\n",
            "Epoch 22/200 | Batch 1/64 | Training Loss: 1.5629\n",
            "Epoch 22/200 | Batch 2/64 | Training Loss: 1.6792\n",
            "Epoch 22/200 | Batch 3/64 | Training Loss: 1.4462\n",
            "Epoch 22/200 | Batch 4/64 | Training Loss: 1.6154\n",
            "Epoch 22/200 | Batch 5/64 | Training Loss: 1.7979\n",
            "Epoch 22/200 | Batch 6/64 | Training Loss: 1.5149\n",
            "Epoch 22/200 | Batch 7/64 | Training Loss: 1.9025\n",
            "Epoch 22/200 | Batch 8/64 | Training Loss: 2.1737\n",
            "Epoch 23/200 | Batch 1/64 | Training Loss: 1.7308\n",
            "Epoch 23/200 | Batch 2/64 | Training Loss: 1.7463\n",
            "Epoch 23/200 | Batch 3/64 | Training Loss: 1.7599\n",
            "Epoch 23/200 | Batch 4/64 | Training Loss: 1.6136\n",
            "Epoch 23/200 | Batch 5/64 | Training Loss: 1.6567\n",
            "Epoch 23/200 | Batch 6/64 | Training Loss: 1.6531\n",
            "Epoch 23/200 | Batch 7/64 | Training Loss: 1.6979\n",
            "Epoch 23/200 | Batch 8/64 | Training Loss: 1.8838\n",
            "Epoch 24/200 | Batch 1/64 | Training Loss: 1.7105\n",
            "Epoch 24/200 | Batch 2/64 | Training Loss: 1.7820\n",
            "Epoch 24/200 | Batch 3/64 | Training Loss: 1.5884\n",
            "Epoch 24/200 | Batch 4/64 | Training Loss: 1.7350\n",
            "Epoch 24/200 | Batch 5/64 | Training Loss: 1.5562\n",
            "Epoch 24/200 | Batch 6/64 | Training Loss: 1.6710\n",
            "Epoch 24/200 | Batch 7/64 | Training Loss: 1.6777\n",
            "Epoch 24/200 | Batch 8/64 | Training Loss: 1.2238\n",
            "Epoch 25/200 | Batch 1/64 | Training Loss: 1.5328\n",
            "Epoch 25/200 | Batch 2/64 | Training Loss: 1.6305\n",
            "Epoch 25/200 | Batch 3/64 | Training Loss: 1.6836\n",
            "Epoch 25/200 | Batch 4/64 | Training Loss: 1.8478\n",
            "Epoch 25/200 | Batch 5/64 | Training Loss: 1.6740\n",
            "Epoch 25/200 | Batch 6/64 | Training Loss: 1.6332\n",
            "Epoch 25/200 | Batch 7/64 | Training Loss: 1.5421\n",
            "Epoch 25/200 | Batch 8/64 | Training Loss: 1.8210\n",
            "Epoch 26/200 | Batch 1/64 | Training Loss: 1.7376\n",
            "Epoch 26/200 | Batch 2/64 | Training Loss: 1.5517\n",
            "Epoch 26/200 | Batch 3/64 | Training Loss: 1.5783\n",
            "Epoch 26/200 | Batch 4/64 | Training Loss: 1.6692\n",
            "Epoch 26/200 | Batch 5/64 | Training Loss: 1.5568\n",
            "Epoch 26/200 | Batch 6/64 | Training Loss: 1.7124\n",
            "Epoch 26/200 | Batch 7/64 | Training Loss: 1.6542\n",
            "Epoch 26/200 | Batch 8/64 | Training Loss: 1.9839\n",
            "Epoch 27/200 | Batch 1/64 | Training Loss: 1.7739\n",
            "Epoch 27/200 | Batch 2/64 | Training Loss: 1.6156\n",
            "Epoch 27/200 | Batch 3/64 | Training Loss: 1.5305\n",
            "Epoch 27/200 | Batch 4/64 | Training Loss: 1.5074\n",
            "Epoch 27/200 | Batch 5/64 | Training Loss: 1.6523\n",
            "Epoch 27/200 | Batch 6/64 | Training Loss: 1.8388\n",
            "Epoch 27/200 | Batch 7/64 | Training Loss: 1.6184\n",
            "Epoch 27/200 | Batch 8/64 | Training Loss: 1.7085\n",
            "Epoch 28/200 | Batch 1/64 | Training Loss: 1.7742\n",
            "Epoch 28/200 | Batch 2/64 | Training Loss: 1.4943\n",
            "Epoch 28/200 | Batch 3/64 | Training Loss: 1.6737\n",
            "Epoch 28/200 | Batch 4/64 | Training Loss: 1.5626\n",
            "Epoch 28/200 | Batch 5/64 | Training Loss: 1.5639\n",
            "Epoch 28/200 | Batch 6/64 | Training Loss: 1.6735\n",
            "Epoch 28/200 | Batch 7/64 | Training Loss: 1.7907\n",
            "Epoch 28/200 | Batch 8/64 | Training Loss: 1.6476\n",
            "Epoch 29/200 | Batch 1/64 | Training Loss: 1.6606\n",
            "Epoch 29/200 | Batch 2/64 | Training Loss: 1.5849\n",
            "Epoch 29/200 | Batch 3/64 | Training Loss: 1.6228\n",
            "Epoch 29/200 | Batch 4/64 | Training Loss: 1.7516\n",
            "Epoch 29/200 | Batch 5/64 | Training Loss: 1.6153\n",
            "Epoch 29/200 | Batch 6/64 | Training Loss: 1.6571\n",
            "Epoch 29/200 | Batch 7/64 | Training Loss: 1.5736\n",
            "Epoch 29/200 | Batch 8/64 | Training Loss: 1.9166\n",
            "Epoch 30/200 | Batch 1/64 | Training Loss: 1.6990\n",
            "Epoch 30/200 | Batch 2/64 | Training Loss: 1.4841\n",
            "Epoch 30/200 | Batch 3/64 | Training Loss: 1.7538\n",
            "Epoch 30/200 | Batch 4/64 | Training Loss: 1.5836\n",
            "Epoch 30/200 | Batch 5/64 | Training Loss: 1.5963\n",
            "Epoch 30/200 | Batch 6/64 | Training Loss: 1.6343\n",
            "Epoch 30/200 | Batch 7/64 | Training Loss: 1.8410\n",
            "Epoch 30/200 | Batch 8/64 | Training Loss: 1.4124\n",
            "Epoch 31/200 | Batch 1/64 | Training Loss: 1.5889\n",
            "Epoch 31/200 | Batch 2/64 | Training Loss: 1.8724\n",
            "Epoch 31/200 | Batch 3/64 | Training Loss: 1.5359\n",
            "Epoch 31/200 | Batch 4/64 | Training Loss: 1.7114\n",
            "Epoch 31/200 | Batch 5/64 | Training Loss: 1.5028\n",
            "Epoch 31/200 | Batch 6/64 | Training Loss: 1.6429\n",
            "Epoch 31/200 | Batch 7/64 | Training Loss: 1.6898\n",
            "Epoch 31/200 | Batch 8/64 | Training Loss: 1.6420\n",
            "Epoch 32/200 | Batch 1/64 | Training Loss: 1.7249\n",
            "Epoch 32/200 | Batch 2/64 | Training Loss: 1.5727\n",
            "Epoch 32/200 | Batch 3/64 | Training Loss: 1.5692\n",
            "Epoch 32/200 | Batch 4/64 | Training Loss: 1.6977\n",
            "Epoch 32/200 | Batch 5/64 | Training Loss: 1.5942\n",
            "Epoch 32/200 | Batch 6/64 | Training Loss: 1.7963\n",
            "Epoch 32/200 | Batch 7/64 | Training Loss: 1.6640\n",
            "Epoch 32/200 | Batch 8/64 | Training Loss: 1.1692\n",
            "Epoch 33/200 | Batch 1/64 | Training Loss: 1.6549\n",
            "Epoch 33/200 | Batch 2/64 | Training Loss: 1.7805\n",
            "Epoch 33/200 | Batch 3/64 | Training Loss: 1.5089\n",
            "Epoch 33/200 | Batch 4/64 | Training Loss: 1.5750\n",
            "Epoch 33/200 | Batch 5/64 | Training Loss: 1.4971\n",
            "Epoch 33/200 | Batch 6/64 | Training Loss: 1.7492\n",
            "Epoch 33/200 | Batch 7/64 | Training Loss: 1.6981\n",
            "Epoch 33/200 | Batch 8/64 | Training Loss: 1.9042\n",
            "Epoch 34/200 | Batch 1/64 | Training Loss: 1.6433\n",
            "Epoch 34/200 | Batch 2/64 | Training Loss: 1.5111\n",
            "Epoch 34/200 | Batch 3/64 | Training Loss: 1.7307\n",
            "Epoch 34/200 | Batch 4/64 | Training Loss: 1.6344\n",
            "Epoch 34/200 | Batch 5/64 | Training Loss: 1.5855\n",
            "Epoch 34/200 | Batch 6/64 | Training Loss: 1.7202\n",
            "Epoch 34/200 | Batch 7/64 | Training Loss: 1.7292\n",
            "Epoch 34/200 | Batch 8/64 | Training Loss: 1.3639\n",
            "Epoch 35/200 | Batch 1/64 | Training Loss: 1.5188\n",
            "Epoch 35/200 | Batch 2/64 | Training Loss: 1.6293\n",
            "Epoch 35/200 | Batch 3/64 | Training Loss: 1.8513\n",
            "Epoch 35/200 | Batch 4/64 | Training Loss: 1.6496\n",
            "Epoch 35/200 | Batch 5/64 | Training Loss: 1.8243\n",
            "Epoch 35/200 | Batch 6/64 | Training Loss: 1.5896\n",
            "Epoch 35/200 | Batch 7/64 | Training Loss: 1.4876\n",
            "Epoch 35/200 | Batch 8/64 | Training Loss: 1.5499\n",
            "Epoch 36/200 | Batch 1/64 | Training Loss: 1.8635\n",
            "Epoch 36/200 | Batch 2/64 | Training Loss: 1.6056\n",
            "Epoch 36/200 | Batch 3/64 | Training Loss: 1.5528\n",
            "Epoch 36/200 | Batch 4/64 | Training Loss: 1.8441\n",
            "Epoch 36/200 | Batch 5/64 | Training Loss: 1.4875\n",
            "Epoch 36/200 | Batch 6/64 | Training Loss: 1.5386\n",
            "Epoch 36/200 | Batch 7/64 | Training Loss: 1.5679\n",
            "Epoch 36/200 | Batch 8/64 | Training Loss: 1.7032\n",
            "Epoch 37/200 | Batch 1/64 | Training Loss: 1.7247\n",
            "Epoch 37/200 | Batch 2/64 | Training Loss: 1.7002\n",
            "Epoch 37/200 | Batch 3/64 | Training Loss: 1.6352\n",
            "Epoch 37/200 | Batch 4/64 | Training Loss: 1.5071\n",
            "Epoch 37/200 | Batch 5/64 | Training Loss: 1.6716\n",
            "Epoch 37/200 | Batch 6/64 | Training Loss: 1.5915\n",
            "Epoch 37/200 | Batch 7/64 | Training Loss: 1.6347\n",
            "Epoch 37/200 | Batch 8/64 | Training Loss: 1.7399\n",
            "Epoch 38/200 | Batch 1/64 | Training Loss: 1.5356\n",
            "Epoch 38/200 | Batch 2/64 | Training Loss: 1.5437\n",
            "Epoch 38/200 | Batch 3/64 | Training Loss: 1.4805\n",
            "Epoch 38/200 | Batch 4/64 | Training Loss: 1.7596\n",
            "Epoch 38/200 | Batch 5/64 | Training Loss: 1.4879\n",
            "Epoch 38/200 | Batch 6/64 | Training Loss: 1.7408\n",
            "Epoch 38/200 | Batch 7/64 | Training Loss: 1.8970\n",
            "Epoch 38/200 | Batch 8/64 | Training Loss: 2.0676\n",
            "Epoch 39/200 | Batch 1/64 | Training Loss: 1.4655\n",
            "Epoch 39/200 | Batch 2/64 | Training Loss: 1.5685\n",
            "Epoch 39/200 | Batch 3/64 | Training Loss: 1.7836\n",
            "Epoch 39/200 | Batch 4/64 | Training Loss: 1.6505\n",
            "Epoch 39/200 | Batch 5/64 | Training Loss: 1.7066\n",
            "Epoch 39/200 | Batch 6/64 | Training Loss: 1.6381\n",
            "Epoch 39/200 | Batch 7/64 | Training Loss: 1.6691\n",
            "Epoch 39/200 | Batch 8/64 | Training Loss: 1.5990\n",
            "Epoch 40/200 | Batch 1/64 | Training Loss: 1.6765\n",
            "Epoch 40/200 | Batch 2/64 | Training Loss: 1.6371\n",
            "Epoch 40/200 | Batch 3/64 | Training Loss: 1.6178\n",
            "Epoch 40/200 | Batch 4/64 | Training Loss: 1.6631\n",
            "Epoch 40/200 | Batch 5/64 | Training Loss: 1.6328\n",
            "Epoch 40/200 | Batch 6/64 | Training Loss: 1.6907\n",
            "Epoch 40/200 | Batch 7/64 | Training Loss: 1.4872\n",
            "Epoch 40/200 | Batch 8/64 | Training Loss: 1.8853\n",
            "Epoch 41/200 | Batch 1/64 | Training Loss: 1.5140\n",
            "Epoch 41/200 | Batch 2/64 | Training Loss: 1.7933\n",
            "Epoch 41/200 | Batch 3/64 | Training Loss: 1.5473\n",
            "Epoch 41/200 | Batch 4/64 | Training Loss: 1.6500\n",
            "Epoch 41/200 | Batch 5/64 | Training Loss: 1.6551\n",
            "Epoch 41/200 | Batch 6/64 | Training Loss: 1.5521\n",
            "Epoch 41/200 | Batch 7/64 | Training Loss: 1.6500\n",
            "Epoch 41/200 | Batch 8/64 | Training Loss: 1.8849\n",
            "Epoch 42/200 | Batch 1/64 | Training Loss: 1.4429\n",
            "Epoch 42/200 | Batch 2/64 | Training Loss: 1.5467\n",
            "Epoch 42/200 | Batch 3/64 | Training Loss: 1.6432\n",
            "Epoch 42/200 | Batch 4/64 | Training Loss: 1.6731\n",
            "Epoch 42/200 | Batch 5/64 | Training Loss: 1.5369\n",
            "Epoch 42/200 | Batch 6/64 | Training Loss: 1.7765\n",
            "Epoch 42/200 | Batch 7/64 | Training Loss: 1.8075\n",
            "Epoch 42/200 | Batch 8/64 | Training Loss: 1.7136\n",
            "Epoch 43/200 | Batch 1/64 | Training Loss: 1.7598\n",
            "Epoch 43/200 | Batch 2/64 | Training Loss: 1.4836\n",
            "Epoch 43/200 | Batch 3/64 | Training Loss: 1.6321\n",
            "Epoch 43/200 | Batch 4/64 | Training Loss: 1.5883\n",
            "Epoch 43/200 | Batch 5/64 | Training Loss: 1.7154\n",
            "Epoch 43/200 | Batch 6/64 | Training Loss: 1.6482\n",
            "Epoch 43/200 | Batch 7/64 | Training Loss: 1.7526\n",
            "Epoch 43/200 | Batch 8/64 | Training Loss: 1.3285\n",
            "Epoch 44/200 | Batch 1/64 | Training Loss: 1.3963\n",
            "Epoch 44/200 | Batch 2/64 | Training Loss: 1.5617\n",
            "Epoch 44/200 | Batch 3/64 | Training Loss: 1.6724\n",
            "Epoch 44/200 | Batch 4/64 | Training Loss: 1.6823\n",
            "Epoch 44/200 | Batch 5/64 | Training Loss: 1.8092\n",
            "Epoch 44/200 | Batch 6/64 | Training Loss: 1.5623\n",
            "Epoch 44/200 | Batch 7/64 | Training Loss: 1.7024\n",
            "Epoch 44/200 | Batch 8/64 | Training Loss: 1.7168\n",
            "Epoch 45/200 | Batch 1/64 | Training Loss: 1.8020\n",
            "Epoch 45/200 | Batch 2/64 | Training Loss: 1.6532\n",
            "Epoch 45/200 | Batch 3/64 | Training Loss: 1.5742\n",
            "Epoch 45/200 | Batch 4/64 | Training Loss: 1.5765\n",
            "Epoch 45/200 | Batch 5/64 | Training Loss: 1.6704\n",
            "Epoch 45/200 | Batch 6/64 | Training Loss: 1.4575\n",
            "Epoch 45/200 | Batch 7/64 | Training Loss: 1.8104\n",
            "Epoch 45/200 | Batch 8/64 | Training Loss: 1.4296\n",
            "Epoch 46/200 | Batch 1/64 | Training Loss: 1.4718\n",
            "Epoch 46/200 | Batch 2/64 | Training Loss: 1.6604\n",
            "Epoch 46/200 | Batch 3/64 | Training Loss: 1.6810\n",
            "Epoch 46/200 | Batch 4/64 | Training Loss: 1.6253\n",
            "Epoch 46/200 | Batch 5/64 | Training Loss: 1.7261\n",
            "Epoch 46/200 | Batch 6/64 | Training Loss: 1.6354\n",
            "Epoch 46/200 | Batch 7/64 | Training Loss: 1.7211\n",
            "Epoch 46/200 | Batch 8/64 | Training Loss: 1.4014\n",
            "Epoch 47/200 | Batch 1/64 | Training Loss: 1.6268\n",
            "Epoch 47/200 | Batch 2/64 | Training Loss: 1.4465\n",
            "Epoch 47/200 | Batch 3/64 | Training Loss: 1.6095\n",
            "Epoch 47/200 | Batch 4/64 | Training Loss: 1.5635\n",
            "Epoch 47/200 | Batch 5/64 | Training Loss: 1.6074\n",
            "Epoch 47/200 | Batch 6/64 | Training Loss: 1.8614\n",
            "Epoch 47/200 | Batch 7/64 | Training Loss: 1.6938\n",
            "Epoch 47/200 | Batch 8/64 | Training Loss: 2.1080\n",
            "Epoch 48/200 | Batch 1/64 | Training Loss: 1.5231\n",
            "Epoch 48/200 | Batch 2/64 | Training Loss: 1.6360\n",
            "Epoch 48/200 | Batch 3/64 | Training Loss: 1.6357\n",
            "Epoch 48/200 | Batch 4/64 | Training Loss: 1.6584\n",
            "Epoch 48/200 | Batch 5/64 | Training Loss: 1.5910\n",
            "Epoch 48/200 | Batch 6/64 | Training Loss: 1.5723\n",
            "Epoch 48/200 | Batch 7/64 | Training Loss: 1.7482\n",
            "Epoch 48/200 | Batch 8/64 | Training Loss: 1.7953\n",
            "Epoch 49/200 | Batch 1/64 | Training Loss: 1.6920\n",
            "Epoch 49/200 | Batch 2/64 | Training Loss: 1.4909\n",
            "Epoch 49/200 | Batch 3/64 | Training Loss: 1.5742\n",
            "Epoch 49/200 | Batch 4/64 | Training Loss: 1.6031\n",
            "Epoch 49/200 | Batch 5/64 | Training Loss: 1.7871\n",
            "Epoch 49/200 | Batch 6/64 | Training Loss: 1.5761\n",
            "Epoch 49/200 | Batch 7/64 | Training Loss: 1.6365\n",
            "Epoch 49/200 | Batch 8/64 | Training Loss: 1.6021\n",
            "Epoch 50/200 | Batch 1/64 | Training Loss: 1.7405\n",
            "Epoch 50/200 | Batch 2/64 | Training Loss: 1.6836\n",
            "Epoch 50/200 | Batch 3/64 | Training Loss: 1.4217\n",
            "Epoch 50/200 | Batch 4/64 | Training Loss: 1.6595\n",
            "Epoch 50/200 | Batch 5/64 | Training Loss: 1.6370\n",
            "Epoch 50/200 | Batch 6/64 | Training Loss: 1.6031\n",
            "Epoch 50/200 | Batch 7/64 | Training Loss: 1.5549\n",
            "Epoch 50/200 | Batch 8/64 | Training Loss: 1.7236\n",
            "Epoch 51/200 | Batch 1/64 | Training Loss: 1.5316\n",
            "Epoch 51/200 | Batch 2/64 | Training Loss: 1.6608\n",
            "Epoch 51/200 | Batch 3/64 | Training Loss: 1.6953\n",
            "Epoch 51/200 | Batch 4/64 | Training Loss: 1.5215\n",
            "Epoch 51/200 | Batch 5/64 | Training Loss: 1.6312\n",
            "Epoch 51/200 | Batch 6/64 | Training Loss: 1.7232\n",
            "Epoch 51/200 | Batch 7/64 | Training Loss: 1.6486\n",
            "Epoch 51/200 | Batch 8/64 | Training Loss: 1.3608\n",
            "Epoch 52/200 | Batch 1/64 | Training Loss: 1.5890\n",
            "Epoch 52/200 | Batch 2/64 | Training Loss: 1.4977\n",
            "Epoch 52/200 | Batch 3/64 | Training Loss: 1.7996\n",
            "Epoch 52/200 | Batch 4/64 | Training Loss: 1.7494\n",
            "Epoch 52/200 | Batch 5/64 | Training Loss: 1.6457\n",
            "Epoch 52/200 | Batch 6/64 | Training Loss: 1.6420\n",
            "Epoch 52/200 | Batch 7/64 | Training Loss: 1.4861\n",
            "Epoch 52/200 | Batch 8/64 | Training Loss: 1.2511\n",
            "Epoch 53/200 | Batch 1/64 | Training Loss: 1.6210\n",
            "Epoch 53/200 | Batch 2/64 | Training Loss: 1.7239\n",
            "Epoch 53/200 | Batch 3/64 | Training Loss: 1.5097\n",
            "Epoch 53/200 | Batch 4/64 | Training Loss: 1.5587\n",
            "Epoch 53/200 | Batch 5/64 | Training Loss: 1.5430\n",
            "Epoch 53/200 | Batch 6/64 | Training Loss: 1.6613\n",
            "Epoch 53/200 | Batch 7/64 | Training Loss: 1.5886\n",
            "Epoch 53/200 | Batch 8/64 | Training Loss: 1.9643\n",
            "Epoch 54/200 | Batch 1/64 | Training Loss: 1.6052\n",
            "Epoch 54/200 | Batch 2/64 | Training Loss: 1.6032\n",
            "Epoch 54/200 | Batch 3/64 | Training Loss: 1.6378\n",
            "Epoch 54/200 | Batch 4/64 | Training Loss: 1.4619\n",
            "Epoch 54/200 | Batch 5/64 | Training Loss: 1.6272\n",
            "Epoch 54/200 | Batch 6/64 | Training Loss: 1.7145\n",
            "Epoch 54/200 | Batch 7/64 | Training Loss: 1.6110\n",
            "Epoch 54/200 | Batch 8/64 | Training Loss: 1.6527\n",
            "Epoch 55/200 | Batch 1/64 | Training Loss: 1.4925\n",
            "Epoch 55/200 | Batch 2/64 | Training Loss: 1.6768\n",
            "Epoch 55/200 | Batch 3/64 | Training Loss: 1.5730\n",
            "Epoch 55/200 | Batch 4/64 | Training Loss: 1.7543\n",
            "Epoch 55/200 | Batch 5/64 | Training Loss: 1.7363\n",
            "Epoch 55/200 | Batch 6/64 | Training Loss: 1.4012\n",
            "Epoch 55/200 | Batch 7/64 | Training Loss: 1.6526\n",
            "Epoch 55/200 | Batch 8/64 | Training Loss: 1.5503\n",
            "Epoch 56/200 | Batch 1/64 | Training Loss: 1.5892\n",
            "Epoch 56/200 | Batch 2/64 | Training Loss: 1.5201\n",
            "Epoch 56/200 | Batch 3/64 | Training Loss: 1.4998\n",
            "Epoch 56/200 | Batch 4/64 | Training Loss: 1.7059\n",
            "Epoch 56/200 | Batch 5/64 | Training Loss: 1.5860\n",
            "Epoch 56/200 | Batch 6/64 | Training Loss: 1.6799\n",
            "Epoch 56/200 | Batch 7/64 | Training Loss: 1.6368\n",
            "Epoch 56/200 | Batch 8/64 | Training Loss: 1.5199\n",
            "Epoch 57/200 | Batch 1/64 | Training Loss: 1.4810\n",
            "Epoch 57/200 | Batch 2/64 | Training Loss: 1.4714\n",
            "Epoch 57/200 | Batch 3/64 | Training Loss: 1.6146\n",
            "Epoch 57/200 | Batch 4/64 | Training Loss: 1.8283\n",
            "Epoch 57/200 | Batch 5/64 | Training Loss: 1.5466\n",
            "Epoch 57/200 | Batch 6/64 | Training Loss: 1.6018\n",
            "Epoch 57/200 | Batch 7/64 | Training Loss: 1.6337\n",
            "Epoch 57/200 | Batch 8/64 | Training Loss: 1.5433\n",
            "Epoch 58/200 | Batch 1/64 | Training Loss: 1.6365\n",
            "Epoch 58/200 | Batch 2/64 | Training Loss: 1.4465\n",
            "Epoch 58/200 | Batch 3/64 | Training Loss: 1.6648\n",
            "Epoch 58/200 | Batch 4/64 | Training Loss: 1.5965\n",
            "Epoch 58/200 | Batch 5/64 | Training Loss: 1.2439\n",
            "Epoch 58/200 | Batch 6/64 | Training Loss: 1.8210\n",
            "Epoch 58/200 | Batch 7/64 | Training Loss: 1.7192\n",
            "Epoch 58/200 | Batch 8/64 | Training Loss: 1.7361\n",
            "Epoch 59/200 | Batch 1/64 | Training Loss: 1.7469\n",
            "Epoch 59/200 | Batch 2/64 | Training Loss: 1.8213\n",
            "Epoch 59/200 | Batch 3/64 | Training Loss: 1.5391\n",
            "Epoch 59/200 | Batch 4/64 | Training Loss: 1.6524\n",
            "Epoch 59/200 | Batch 5/64 | Training Loss: 1.5033\n",
            "Epoch 59/200 | Batch 6/64 | Training Loss: 1.5678\n",
            "Epoch 59/200 | Batch 7/64 | Training Loss: 1.4739\n",
            "Epoch 59/200 | Batch 8/64 | Training Loss: 1.4112\n",
            "Epoch 60/200 | Batch 1/64 | Training Loss: 1.6351\n",
            "Epoch 60/200 | Batch 2/64 | Training Loss: 1.4686\n",
            "Epoch 60/200 | Batch 3/64 | Training Loss: 1.6954\n",
            "Epoch 60/200 | Batch 4/64 | Training Loss: 1.8093\n",
            "Epoch 60/200 | Batch 5/64 | Training Loss: 1.5521\n",
            "Epoch 60/200 | Batch 6/64 | Training Loss: 1.5471\n",
            "Epoch 60/200 | Batch 7/64 | Training Loss: 1.5515\n",
            "Epoch 60/200 | Batch 8/64 | Training Loss: 1.6269\n",
            "Epoch 61/200 | Batch 1/64 | Training Loss: 1.6225\n",
            "Epoch 61/200 | Batch 2/64 | Training Loss: 1.7137\n",
            "Epoch 61/200 | Batch 3/64 | Training Loss: 1.5856\n",
            "Epoch 61/200 | Batch 4/64 | Training Loss: 1.5541\n",
            "Epoch 61/200 | Batch 5/64 | Training Loss: 1.3903\n",
            "Epoch 61/200 | Batch 6/64 | Training Loss: 1.4469\n",
            "Epoch 61/200 | Batch 7/64 | Training Loss: 1.7418\n",
            "Epoch 61/200 | Batch 8/64 | Training Loss: 1.7648\n",
            "Epoch 62/200 | Batch 1/64 | Training Loss: 1.6860\n",
            "Epoch 62/200 | Batch 2/64 | Training Loss: 1.7136\n",
            "Epoch 62/200 | Batch 3/64 | Training Loss: 1.6583\n",
            "Epoch 62/200 | Batch 4/64 | Training Loss: 1.3766\n",
            "Epoch 62/200 | Batch 5/64 | Training Loss: 1.5496\n",
            "Epoch 62/200 | Batch 6/64 | Training Loss: 1.5792\n",
            "Epoch 62/200 | Batch 7/64 | Training Loss: 1.5608\n",
            "Epoch 62/200 | Batch 8/64 | Training Loss: 1.7701\n",
            "Epoch 63/200 | Batch 1/64 | Training Loss: 1.4585\n",
            "Epoch 63/200 | Batch 2/64 | Training Loss: 1.6169\n",
            "Epoch 63/200 | Batch 3/64 | Training Loss: 1.6243\n",
            "Epoch 63/200 | Batch 4/64 | Training Loss: 1.3139\n",
            "Epoch 63/200 | Batch 5/64 | Training Loss: 1.7287\n",
            "Epoch 63/200 | Batch 6/64 | Training Loss: 1.6086\n",
            "Epoch 63/200 | Batch 7/64 | Training Loss: 1.6419\n",
            "Epoch 63/200 | Batch 8/64 | Training Loss: 1.8907\n",
            "Epoch 64/200 | Batch 1/64 | Training Loss: 1.5703\n",
            "Epoch 64/200 | Batch 2/64 | Training Loss: 1.4779\n",
            "Epoch 64/200 | Batch 3/64 | Training Loss: 1.5976\n",
            "Epoch 64/200 | Batch 4/64 | Training Loss: 1.6113\n",
            "Epoch 64/200 | Batch 5/64 | Training Loss: 1.4696\n",
            "Epoch 64/200 | Batch 6/64 | Training Loss: 1.7587\n",
            "Epoch 64/200 | Batch 7/64 | Training Loss: 1.5920\n",
            "Epoch 64/200 | Batch 8/64 | Training Loss: 1.6989\n",
            "Epoch 65/200 | Batch 1/64 | Training Loss: 1.6715\n",
            "Epoch 65/200 | Batch 2/64 | Training Loss: 1.6757\n",
            "Epoch 65/200 | Batch 3/64 | Training Loss: 1.4671\n",
            "Epoch 65/200 | Batch 4/64 | Training Loss: 1.5353\n",
            "Epoch 65/200 | Batch 5/64 | Training Loss: 1.4347\n",
            "Epoch 65/200 | Batch 6/64 | Training Loss: 1.5969\n",
            "Epoch 65/200 | Batch 7/64 | Training Loss: 1.6635\n",
            "Epoch 65/200 | Batch 8/64 | Training Loss: 1.6127\n",
            "Epoch 66/200 | Batch 1/64 | Training Loss: 1.5153\n",
            "Epoch 66/200 | Batch 2/64 | Training Loss: 1.4568\n",
            "Epoch 66/200 | Batch 3/64 | Training Loss: 1.5339\n",
            "Epoch 66/200 | Batch 4/64 | Training Loss: 1.5784\n",
            "Epoch 66/200 | Batch 5/64 | Training Loss: 1.6652\n",
            "Epoch 66/200 | Batch 6/64 | Training Loss: 1.5458\n",
            "Epoch 66/200 | Batch 7/64 | Training Loss: 1.5936\n",
            "Epoch 66/200 | Batch 8/64 | Training Loss: 1.7654\n",
            "Epoch 67/200 | Batch 1/64 | Training Loss: 1.5488\n",
            "Epoch 67/200 | Batch 2/64 | Training Loss: 1.6096\n",
            "Epoch 67/200 | Batch 3/64 | Training Loss: 1.4882\n",
            "Epoch 67/200 | Batch 4/64 | Training Loss: 1.5419\n",
            "Epoch 67/200 | Batch 5/64 | Training Loss: 1.5065\n",
            "Epoch 67/200 | Batch 6/64 | Training Loss: 1.5095\n",
            "Epoch 67/200 | Batch 7/64 | Training Loss: 1.6912\n",
            "Epoch 67/200 | Batch 8/64 | Training Loss: 1.8251\n",
            "Epoch 68/200 | Batch 1/64 | Training Loss: 1.3947\n",
            "Epoch 68/200 | Batch 2/64 | Training Loss: 1.5831\n",
            "Epoch 68/200 | Batch 3/64 | Training Loss: 1.4976\n",
            "Epoch 68/200 | Batch 4/64 | Training Loss: 1.5456\n",
            "Epoch 68/200 | Batch 5/64 | Training Loss: 1.5911\n",
            "Epoch 68/200 | Batch 6/64 | Training Loss: 1.6604\n",
            "Epoch 68/200 | Batch 7/64 | Training Loss: 1.5519\n",
            "Epoch 68/200 | Batch 8/64 | Training Loss: 1.8248\n",
            "Epoch 69/200 | Batch 1/64 | Training Loss: 1.3242\n",
            "Epoch 69/200 | Batch 2/64 | Training Loss: 1.3994\n",
            "Epoch 69/200 | Batch 3/64 | Training Loss: 1.5401\n",
            "Epoch 69/200 | Batch 4/64 | Training Loss: 1.6079\n",
            "Epoch 69/200 | Batch 5/64 | Training Loss: 1.8426\n",
            "Epoch 69/200 | Batch 6/64 | Training Loss: 1.5811\n",
            "Epoch 69/200 | Batch 7/64 | Training Loss: 1.6218\n",
            "Epoch 69/200 | Batch 8/64 | Training Loss: 1.4316\n",
            "Epoch 70/200 | Batch 1/64 | Training Loss: 1.3746\n",
            "Epoch 70/200 | Batch 2/64 | Training Loss: 1.5748\n",
            "Epoch 70/200 | Batch 3/64 | Training Loss: 1.7903\n",
            "Epoch 70/200 | Batch 4/64 | Training Loss: 1.6692\n",
            "Epoch 70/200 | Batch 5/64 | Training Loss: 1.6366\n",
            "Epoch 70/200 | Batch 6/64 | Training Loss: 1.3326\n",
            "Epoch 70/200 | Batch 7/64 | Training Loss: 1.5028\n",
            "Epoch 70/200 | Batch 8/64 | Training Loss: 1.4659\n",
            "Epoch 71/200 | Batch 1/64 | Training Loss: 1.4777\n",
            "Epoch 71/200 | Batch 2/64 | Training Loss: 1.6653\n",
            "Epoch 71/200 | Batch 3/64 | Training Loss: 1.4940\n",
            "Epoch 71/200 | Batch 4/64 | Training Loss: 1.5275\n",
            "Epoch 71/200 | Batch 5/64 | Training Loss: 1.6421\n",
            "Epoch 71/200 | Batch 6/64 | Training Loss: 1.5572\n",
            "Epoch 71/200 | Batch 7/64 | Training Loss: 1.4389\n",
            "Epoch 71/200 | Batch 8/64 | Training Loss: 1.4588\n",
            "Epoch 72/200 | Batch 1/64 | Training Loss: 1.5498\n",
            "Epoch 72/200 | Batch 2/64 | Training Loss: 1.6556\n",
            "Epoch 72/200 | Batch 3/64 | Training Loss: 1.4514\n",
            "Epoch 72/200 | Batch 4/64 | Training Loss: 1.4919\n",
            "Epoch 72/200 | Batch 5/64 | Training Loss: 1.4391\n",
            "Epoch 72/200 | Batch 6/64 | Training Loss: 1.5080\n",
            "Epoch 72/200 | Batch 7/64 | Training Loss: 1.5233\n",
            "Epoch 72/200 | Batch 8/64 | Training Loss: 1.9595\n",
            "Epoch 73/200 | Batch 1/64 | Training Loss: 1.5318\n",
            "Epoch 73/200 | Batch 2/64 | Training Loss: 1.5828\n",
            "Epoch 73/200 | Batch 3/64 | Training Loss: 1.6302\n",
            "Epoch 73/200 | Batch 4/64 | Training Loss: 1.4322\n",
            "Epoch 73/200 | Batch 5/64 | Training Loss: 1.6548\n",
            "Epoch 73/200 | Batch 6/64 | Training Loss: 1.4352\n",
            "Epoch 73/200 | Batch 7/64 | Training Loss: 1.4061\n",
            "Epoch 73/200 | Batch 8/64 | Training Loss: 1.6335\n",
            "Epoch 74/200 | Batch 1/64 | Training Loss: 1.4617\n",
            "Epoch 74/200 | Batch 2/64 | Training Loss: 1.4987\n",
            "Epoch 74/200 | Batch 3/64 | Training Loss: 1.5414\n",
            "Epoch 74/200 | Batch 4/64 | Training Loss: 1.4551\n",
            "Epoch 74/200 | Batch 5/64 | Training Loss: 1.5409\n",
            "Epoch 74/200 | Batch 6/64 | Training Loss: 1.5330\n",
            "Epoch 74/200 | Batch 7/64 | Training Loss: 1.6028\n",
            "Epoch 74/200 | Batch 8/64 | Training Loss: 1.7947\n",
            "Epoch 75/200 | Batch 1/64 | Training Loss: 1.3749\n",
            "Epoch 75/200 | Batch 2/64 | Training Loss: 1.4188\n",
            "Epoch 75/200 | Batch 3/64 | Training Loss: 1.4931\n",
            "Epoch 75/200 | Batch 4/64 | Training Loss: 1.5463\n",
            "Epoch 75/200 | Batch 5/64 | Training Loss: 1.5259\n",
            "Epoch 75/200 | Batch 6/64 | Training Loss: 1.5181\n",
            "Epoch 75/200 | Batch 7/64 | Training Loss: 1.6612\n",
            "Epoch 75/200 | Batch 8/64 | Training Loss: 1.6376\n",
            "Epoch 76/200 | Batch 1/64 | Training Loss: 1.4943\n",
            "Epoch 76/200 | Batch 2/64 | Training Loss: 1.4838\n",
            "Epoch 76/200 | Batch 3/64 | Training Loss: 1.6153\n",
            "Epoch 76/200 | Batch 4/64 | Training Loss: 1.4977\n",
            "Epoch 76/200 | Batch 5/64 | Training Loss: 1.6279\n",
            "Epoch 76/200 | Batch 6/64 | Training Loss: 1.3846\n",
            "Epoch 76/200 | Batch 7/64 | Training Loss: 1.4976\n",
            "Epoch 76/200 | Batch 8/64 | Training Loss: 1.2858\n",
            "Epoch 77/200 | Batch 1/64 | Training Loss: 1.4036\n",
            "Epoch 77/200 | Batch 2/64 | Training Loss: 1.3258\n",
            "Epoch 77/200 | Batch 3/64 | Training Loss: 1.6618\n",
            "Epoch 77/200 | Batch 4/64 | Training Loss: 1.4984\n",
            "Epoch 77/200 | Batch 5/64 | Training Loss: 1.5760\n",
            "Epoch 77/200 | Batch 6/64 | Training Loss: 1.5113\n",
            "Epoch 77/200 | Batch 7/64 | Training Loss: 1.6037\n",
            "Epoch 77/200 | Batch 8/64 | Training Loss: 1.4996\n",
            "Epoch 78/200 | Batch 1/64 | Training Loss: 1.5438\n",
            "Epoch 78/200 | Batch 2/64 | Training Loss: 1.7346\n",
            "Epoch 78/200 | Batch 3/64 | Training Loss: 1.3005\n",
            "Epoch 78/200 | Batch 4/64 | Training Loss: 1.5485\n",
            "Epoch 78/200 | Batch 5/64 | Training Loss: 1.4902\n",
            "Epoch 78/200 | Batch 6/64 | Training Loss: 1.3745\n",
            "Epoch 78/200 | Batch 7/64 | Training Loss: 1.3325\n",
            "Epoch 78/200 | Batch 8/64 | Training Loss: 1.7862\n",
            "Epoch 79/200 | Batch 1/64 | Training Loss: 1.5133\n",
            "Epoch 79/200 | Batch 2/64 | Training Loss: 1.4370\n",
            "Epoch 79/200 | Batch 3/64 | Training Loss: 1.4755\n",
            "Epoch 79/200 | Batch 4/64 | Training Loss: 1.5142\n",
            "Epoch 79/200 | Batch 5/64 | Training Loss: 1.3772\n",
            "Epoch 79/200 | Batch 6/64 | Training Loss: 1.5778\n",
            "Epoch 79/200 | Batch 7/64 | Training Loss: 1.4612\n",
            "Epoch 79/200 | Batch 8/64 | Training Loss: 1.7782\n",
            "Epoch 80/200 | Batch 1/64 | Training Loss: 1.4688\n",
            "Epoch 80/200 | Batch 2/64 | Training Loss: 1.5759\n",
            "Epoch 80/200 | Batch 3/64 | Training Loss: 1.3923\n",
            "Epoch 80/200 | Batch 4/64 | Training Loss: 1.4040\n",
            "Epoch 80/200 | Batch 5/64 | Training Loss: 1.5071\n",
            "Epoch 80/200 | Batch 6/64 | Training Loss: 1.5199\n",
            "Epoch 80/200 | Batch 7/64 | Training Loss: 1.5572\n",
            "Epoch 80/200 | Batch 8/64 | Training Loss: 1.5049\n",
            "Epoch 81/200 | Batch 1/64 | Training Loss: 1.5661\n",
            "Epoch 81/200 | Batch 2/64 | Training Loss: 1.3443\n",
            "Epoch 81/200 | Batch 3/64 | Training Loss: 1.4469\n",
            "Epoch 81/200 | Batch 4/64 | Training Loss: 1.4783\n",
            "Epoch 81/200 | Batch 5/64 | Training Loss: 1.3518\n",
            "Epoch 81/200 | Batch 6/64 | Training Loss: 1.4743\n",
            "Epoch 81/200 | Batch 7/64 | Training Loss: 1.5722\n",
            "Epoch 81/200 | Batch 8/64 | Training Loss: 1.8300\n",
            "Epoch 82/200 | Batch 1/64 | Training Loss: 1.4100\n",
            "Epoch 82/200 | Batch 2/64 | Training Loss: 1.4917\n",
            "Epoch 82/200 | Batch 3/64 | Training Loss: 1.4997\n",
            "Epoch 82/200 | Batch 4/64 | Training Loss: 1.5078\n",
            "Epoch 82/200 | Batch 5/64 | Training Loss: 1.4998\n",
            "Epoch 82/200 | Batch 6/64 | Training Loss: 1.4439\n",
            "Epoch 82/200 | Batch 7/64 | Training Loss: 1.3778\n",
            "Epoch 82/200 | Batch 8/64 | Training Loss: 1.2829\n",
            "Epoch 83/200 | Batch 1/64 | Training Loss: 1.4807\n",
            "Epoch 83/200 | Batch 2/64 | Training Loss: 1.3753\n",
            "Epoch 83/200 | Batch 3/64 | Training Loss: 1.3934\n",
            "Epoch 83/200 | Batch 4/64 | Training Loss: 1.4106\n",
            "Epoch 83/200 | Batch 5/64 | Training Loss: 1.5205\n",
            "Epoch 83/200 | Batch 6/64 | Training Loss: 1.5243\n",
            "Epoch 83/200 | Batch 7/64 | Training Loss: 1.4366\n",
            "Epoch 83/200 | Batch 8/64 | Training Loss: 1.6481\n",
            "Epoch 84/200 | Batch 1/64 | Training Loss: 1.3588\n",
            "Epoch 84/200 | Batch 2/64 | Training Loss: 1.4003\n",
            "Epoch 84/200 | Batch 3/64 | Training Loss: 1.4172\n",
            "Epoch 84/200 | Batch 4/64 | Training Loss: 1.3864\n",
            "Epoch 84/200 | Batch 5/64 | Training Loss: 1.6517\n",
            "Epoch 84/200 | Batch 6/64 | Training Loss: 1.4216\n",
            "Epoch 84/200 | Batch 7/64 | Training Loss: 1.4413\n",
            "Epoch 84/200 | Batch 8/64 | Training Loss: 1.7482\n",
            "Epoch 85/200 | Batch 1/64 | Training Loss: 1.6668\n",
            "Epoch 85/200 | Batch 2/64 | Training Loss: 1.4627\n",
            "Epoch 85/200 | Batch 3/64 | Training Loss: 1.4642\n",
            "Epoch 85/200 | Batch 4/64 | Training Loss: 1.4620\n",
            "Epoch 85/200 | Batch 5/64 | Training Loss: 1.3590\n",
            "Epoch 85/200 | Batch 6/64 | Training Loss: 1.4566\n",
            "Epoch 85/200 | Batch 7/64 | Training Loss: 1.4918\n",
            "Epoch 85/200 | Batch 8/64 | Training Loss: 1.4163\n",
            "Epoch 86/200 | Batch 1/64 | Training Loss: 1.1988\n",
            "Epoch 86/200 | Batch 2/64 | Training Loss: 1.3341\n",
            "Epoch 86/200 | Batch 3/64 | Training Loss: 1.4890\n",
            "Epoch 86/200 | Batch 4/64 | Training Loss: 1.5207\n",
            "Epoch 86/200 | Batch 5/64 | Training Loss: 1.5222\n",
            "Epoch 86/200 | Batch 6/64 | Training Loss: 1.5353\n",
            "Epoch 86/200 | Batch 7/64 | Training Loss: 1.4266\n",
            "Epoch 86/200 | Batch 8/64 | Training Loss: 1.4371\n",
            "Epoch 87/200 | Batch 1/64 | Training Loss: 1.3276\n",
            "Epoch 87/200 | Batch 2/64 | Training Loss: 1.3514\n",
            "Epoch 87/200 | Batch 3/64 | Training Loss: 1.5176\n",
            "Epoch 87/200 | Batch 4/64 | Training Loss: 1.2747\n",
            "Epoch 87/200 | Batch 5/64 | Training Loss: 1.5339\n",
            "Epoch 87/200 | Batch 6/64 | Training Loss: 1.3909\n",
            "Epoch 87/200 | Batch 7/64 | Training Loss: 1.4932\n",
            "Epoch 87/200 | Batch 8/64 | Training Loss: 1.6490\n",
            "Epoch 88/200 | Batch 1/64 | Training Loss: 1.3485\n",
            "Epoch 88/200 | Batch 2/64 | Training Loss: 1.4757\n",
            "Epoch 88/200 | Batch 3/64 | Training Loss: 1.3961\n",
            "Epoch 88/200 | Batch 4/64 | Training Loss: 1.5094\n",
            "Epoch 88/200 | Batch 5/64 | Training Loss: 1.4243\n",
            "Epoch 88/200 | Batch 6/64 | Training Loss: 1.3876\n",
            "Epoch 88/200 | Batch 7/64 | Training Loss: 1.2721\n",
            "Epoch 88/200 | Batch 8/64 | Training Loss: 1.7354\n",
            "Epoch 89/200 | Batch 1/64 | Training Loss: 1.3872\n",
            "Epoch 89/200 | Batch 2/64 | Training Loss: 1.4924\n",
            "Epoch 89/200 | Batch 3/64 | Training Loss: 1.2482\n",
            "Epoch 89/200 | Batch 4/64 | Training Loss: 1.2940\n",
            "Epoch 89/200 | Batch 5/64 | Training Loss: 1.3509\n",
            "Epoch 89/200 | Batch 6/64 | Training Loss: 1.4856\n",
            "Epoch 89/200 | Batch 7/64 | Training Loss: 1.4923\n",
            "Epoch 89/200 | Batch 8/64 | Training Loss: 1.7025\n",
            "Epoch 90/200 | Batch 1/64 | Training Loss: 1.3825\n",
            "Epoch 90/200 | Batch 2/64 | Training Loss: 1.3064\n",
            "Epoch 90/200 | Batch 3/64 | Training Loss: 1.3476\n",
            "Epoch 90/200 | Batch 4/64 | Training Loss: 1.4946\n",
            "Epoch 90/200 | Batch 5/64 | Training Loss: 1.4665\n",
            "Epoch 90/200 | Batch 6/64 | Training Loss: 1.3773\n",
            "Epoch 90/200 | Batch 7/64 | Training Loss: 1.3322\n",
            "Epoch 90/200 | Batch 8/64 | Training Loss: 1.6033\n",
            "Epoch 91/200 | Batch 1/64 | Training Loss: 1.3079\n",
            "Epoch 91/200 | Batch 2/64 | Training Loss: 1.3856\n",
            "Epoch 91/200 | Batch 3/64 | Training Loss: 1.3745\n",
            "Epoch 91/200 | Batch 4/64 | Training Loss: 1.3560\n",
            "Epoch 91/200 | Batch 5/64 | Training Loss: 1.3950\n",
            "Epoch 91/200 | Batch 6/64 | Training Loss: 1.2771\n",
            "Epoch 91/200 | Batch 7/64 | Training Loss: 1.5529\n",
            "Epoch 91/200 | Batch 8/64 | Training Loss: 1.7393\n",
            "Epoch 92/200 | Batch 1/64 | Training Loss: 1.3284\n",
            "Epoch 92/200 | Batch 2/64 | Training Loss: 1.1817\n",
            "Epoch 92/200 | Batch 3/64 | Training Loss: 1.3564\n",
            "Epoch 92/200 | Batch 4/64 | Training Loss: 1.3657\n",
            "Epoch 92/200 | Batch 5/64 | Training Loss: 1.3357\n",
            "Epoch 92/200 | Batch 6/64 | Training Loss: 1.5657\n",
            "Epoch 92/200 | Batch 7/64 | Training Loss: 1.4879\n",
            "Epoch 92/200 | Batch 8/64 | Training Loss: 1.2606\n",
            "Epoch 93/200 | Batch 1/64 | Training Loss: 1.2168\n",
            "Epoch 93/200 | Batch 2/64 | Training Loss: 1.2429\n",
            "Epoch 93/200 | Batch 3/64 | Training Loss: 1.2741\n",
            "Epoch 93/200 | Batch 4/64 | Training Loss: 1.5462\n",
            "Epoch 93/200 | Batch 5/64 | Training Loss: 1.3067\n",
            "Epoch 93/200 | Batch 6/64 | Training Loss: 1.4068\n",
            "Epoch 93/200 | Batch 7/64 | Training Loss: 1.4108\n",
            "Epoch 93/200 | Batch 8/64 | Training Loss: 1.2727\n",
            "Epoch 94/200 | Batch 1/64 | Training Loss: 1.4203\n",
            "Epoch 94/200 | Batch 2/64 | Training Loss: 1.2520\n",
            "Epoch 94/200 | Batch 3/64 | Training Loss: 1.2659\n",
            "Epoch 94/200 | Batch 4/64 | Training Loss: 1.3248\n",
            "Epoch 94/200 | Batch 5/64 | Training Loss: 1.4337\n",
            "Epoch 94/200 | Batch 6/64 | Training Loss: 1.4153\n",
            "Epoch 94/200 | Batch 7/64 | Training Loss: 1.3984\n",
            "Epoch 94/200 | Batch 8/64 | Training Loss: 1.2101\n",
            "Epoch 95/200 | Batch 1/64 | Training Loss: 1.2296\n",
            "Epoch 95/200 | Batch 2/64 | Training Loss: 1.6713\n",
            "Epoch 95/200 | Batch 3/64 | Training Loss: 1.1857\n",
            "Epoch 95/200 | Batch 4/64 | Training Loss: 1.4808\n",
            "Epoch 95/200 | Batch 5/64 | Training Loss: 1.3429\n",
            "Epoch 95/200 | Batch 6/64 | Training Loss: 1.3390\n",
            "Epoch 95/200 | Batch 7/64 | Training Loss: 1.2566\n",
            "Epoch 95/200 | Batch 8/64 | Training Loss: 1.2921\n",
            "Epoch 96/200 | Batch 1/64 | Training Loss: 1.3173\n",
            "Epoch 96/200 | Batch 2/64 | Training Loss: 1.1538\n",
            "Epoch 96/200 | Batch 3/64 | Training Loss: 1.4870\n",
            "Epoch 96/200 | Batch 4/64 | Training Loss: 1.3350\n",
            "Epoch 96/200 | Batch 5/64 | Training Loss: 1.3162\n",
            "Epoch 96/200 | Batch 6/64 | Training Loss: 1.3489\n",
            "Epoch 96/200 | Batch 7/64 | Training Loss: 1.2052\n",
            "Epoch 96/200 | Batch 8/64 | Training Loss: 1.1094\n",
            "Epoch 97/200 | Batch 1/64 | Training Loss: 1.3337\n",
            "Epoch 97/200 | Batch 2/64 | Training Loss: 1.3662\n",
            "Epoch 97/200 | Batch 3/64 | Training Loss: 1.0971\n",
            "Epoch 97/200 | Batch 4/64 | Training Loss: 1.5066\n",
            "Epoch 97/200 | Batch 5/64 | Training Loss: 1.2555\n",
            "Epoch 97/200 | Batch 6/64 | Training Loss: 1.3419\n",
            "Epoch 97/200 | Batch 7/64 | Training Loss: 1.0473\n",
            "Epoch 97/200 | Batch 8/64 | Training Loss: 1.4293\n",
            "Epoch 98/200 | Batch 1/64 | Training Loss: 1.3536\n",
            "Epoch 98/200 | Batch 2/64 | Training Loss: 1.1863\n",
            "Epoch 98/200 | Batch 3/64 | Training Loss: 1.3855\n",
            "Epoch 98/200 | Batch 4/64 | Training Loss: 1.3423\n",
            "Epoch 98/200 | Batch 5/64 | Training Loss: 1.1750\n",
            "Epoch 98/200 | Batch 6/64 | Training Loss: 1.1386\n",
            "Epoch 98/200 | Batch 7/64 | Training Loss: 1.2989\n",
            "Epoch 98/200 | Batch 8/64 | Training Loss: 1.1984\n",
            "Epoch 99/200 | Batch 1/64 | Training Loss: 1.2727\n",
            "Epoch 99/200 | Batch 2/64 | Training Loss: 1.1190\n",
            "Epoch 99/200 | Batch 3/64 | Training Loss: 1.2941\n",
            "Epoch 99/200 | Batch 4/64 | Training Loss: 1.2756\n",
            "Epoch 99/200 | Batch 5/64 | Training Loss: 1.5234\n",
            "Epoch 99/200 | Batch 6/64 | Training Loss: 1.1306\n",
            "Epoch 99/200 | Batch 7/64 | Training Loss: 1.1211\n",
            "Epoch 99/200 | Batch 8/64 | Training Loss: 1.4459\n",
            "Epoch 100/200 | Batch 1/64 | Training Loss: 1.2482\n",
            "Epoch 100/200 | Batch 2/64 | Training Loss: 1.4112\n",
            "Epoch 100/200 | Batch 3/64 | Training Loss: 1.2311\n",
            "Epoch 100/200 | Batch 4/64 | Training Loss: 1.2102\n",
            "Epoch 100/200 | Batch 5/64 | Training Loss: 1.1591\n",
            "Epoch 100/200 | Batch 6/64 | Training Loss: 1.3599\n",
            "Epoch 100/200 | Batch 7/64 | Training Loss: 1.3853\n",
            "Epoch 100/200 | Batch 8/64 | Training Loss: 1.1028\n",
            "Epoch 101/200 | Batch 1/64 | Training Loss: 1.2692\n",
            "Epoch 101/200 | Batch 2/64 | Training Loss: 1.2213\n",
            "Epoch 101/200 | Batch 3/64 | Training Loss: 1.3109\n",
            "Epoch 101/200 | Batch 4/64 | Training Loss: 1.2357\n",
            "Epoch 101/200 | Batch 5/64 | Training Loss: 1.2068\n",
            "Epoch 101/200 | Batch 6/64 | Training Loss: 1.2957\n",
            "Epoch 101/200 | Batch 7/64 | Training Loss: 1.1049\n",
            "Epoch 101/200 | Batch 8/64 | Training Loss: 1.4743\n",
            "Epoch 102/200 | Batch 1/64 | Training Loss: 1.2642\n",
            "Epoch 102/200 | Batch 2/64 | Training Loss: 1.2295\n",
            "Epoch 102/200 | Batch 3/64 | Training Loss: 1.0992\n",
            "Epoch 102/200 | Batch 4/64 | Training Loss: 1.2313\n",
            "Epoch 102/200 | Batch 5/64 | Training Loss: 1.3696\n",
            "Epoch 102/200 | Batch 6/64 | Training Loss: 1.1247\n",
            "Epoch 102/200 | Batch 7/64 | Training Loss: 1.2929\n",
            "Epoch 102/200 | Batch 8/64 | Training Loss: 1.0305\n",
            "Epoch 103/200 | Batch 1/64 | Training Loss: 1.1808\n",
            "Epoch 103/200 | Batch 2/64 | Training Loss: 1.2081\n",
            "Epoch 103/200 | Batch 3/64 | Training Loss: 1.2962\n",
            "Epoch 103/200 | Batch 4/64 | Training Loss: 1.1113\n",
            "Epoch 103/200 | Batch 5/64 | Training Loss: 1.2911\n",
            "Epoch 103/200 | Batch 6/64 | Training Loss: 1.2343\n",
            "Epoch 103/200 | Batch 7/64 | Training Loss: 1.2439\n",
            "Epoch 103/200 | Batch 8/64 | Training Loss: 1.4470\n",
            "Epoch 104/200 | Batch 1/64 | Training Loss: 1.2617\n",
            "Epoch 104/200 | Batch 2/64 | Training Loss: 1.0991\n",
            "Epoch 104/200 | Batch 3/64 | Training Loss: 1.1630\n",
            "Epoch 104/200 | Batch 4/64 | Training Loss: 1.3644\n",
            "Epoch 104/200 | Batch 5/64 | Training Loss: 1.2311\n",
            "Epoch 104/200 | Batch 6/64 | Training Loss: 1.1726\n",
            "Epoch 104/200 | Batch 7/64 | Training Loss: 1.2831\n",
            "Epoch 104/200 | Batch 8/64 | Training Loss: 0.8644\n",
            "Epoch 105/200 | Batch 1/64 | Training Loss: 1.1508\n",
            "Epoch 105/200 | Batch 2/64 | Training Loss: 1.2143\n",
            "Epoch 105/200 | Batch 3/64 | Training Loss: 1.1341\n",
            "Epoch 105/200 | Batch 4/64 | Training Loss: 1.1874\n",
            "Epoch 105/200 | Batch 5/64 | Training Loss: 1.0983\n",
            "Epoch 105/200 | Batch 6/64 | Training Loss: 1.2514\n",
            "Epoch 105/200 | Batch 7/64 | Training Loss: 1.1870\n",
            "Epoch 105/200 | Batch 8/64 | Training Loss: 1.0238\n",
            "Epoch 106/200 | Batch 1/64 | Training Loss: 1.2880\n",
            "Epoch 106/200 | Batch 2/64 | Training Loss: 1.1014\n",
            "Epoch 106/200 | Batch 3/64 | Training Loss: 1.1077\n",
            "Epoch 106/200 | Batch 4/64 | Training Loss: 1.0750\n",
            "Epoch 106/200 | Batch 5/64 | Training Loss: 1.2588\n",
            "Epoch 106/200 | Batch 6/64 | Training Loss: 1.1697\n",
            "Epoch 106/200 | Batch 7/64 | Training Loss: 1.0901\n",
            "Epoch 106/200 | Batch 8/64 | Training Loss: 0.9470\n",
            "Epoch 107/200 | Batch 1/64 | Training Loss: 1.1328\n",
            "Epoch 107/200 | Batch 2/64 | Training Loss: 1.0151\n",
            "Epoch 107/200 | Batch 3/64 | Training Loss: 0.9888\n",
            "Epoch 107/200 | Batch 4/64 | Training Loss: 1.1647\n",
            "Epoch 107/200 | Batch 5/64 | Training Loss: 1.3263\n",
            "Epoch 107/200 | Batch 6/64 | Training Loss: 1.0137\n",
            "Epoch 107/200 | Batch 7/64 | Training Loss: 1.3351\n",
            "Epoch 107/200 | Batch 8/64 | Training Loss: 1.0871\n",
            "Epoch 108/200 | Batch 1/64 | Training Loss: 1.1217\n",
            "Epoch 108/200 | Batch 2/64 | Training Loss: 1.0599\n",
            "Epoch 108/200 | Batch 3/64 | Training Loss: 1.0206\n",
            "Epoch 108/200 | Batch 4/64 | Training Loss: 1.1295\n",
            "Epoch 108/200 | Batch 5/64 | Training Loss: 1.2418\n",
            "Epoch 108/200 | Batch 6/64 | Training Loss: 1.0954\n",
            "Epoch 108/200 | Batch 7/64 | Training Loss: 1.0376\n",
            "Epoch 108/200 | Batch 8/64 | Training Loss: 1.3334\n",
            "Epoch 109/200 | Batch 1/64 | Training Loss: 0.9632\n",
            "Epoch 109/200 | Batch 2/64 | Training Loss: 1.4137\n",
            "Epoch 109/200 | Batch 3/64 | Training Loss: 0.9542\n",
            "Epoch 109/200 | Batch 4/64 | Training Loss: 1.0497\n",
            "Epoch 109/200 | Batch 5/64 | Training Loss: 1.2867\n",
            "Epoch 109/200 | Batch 6/64 | Training Loss: 1.0392\n",
            "Epoch 109/200 | Batch 7/64 | Training Loss: 0.9801\n",
            "Epoch 109/200 | Batch 8/64 | Training Loss: 1.1087\n",
            "Epoch 110/200 | Batch 1/64 | Training Loss: 1.1711\n",
            "Epoch 110/200 | Batch 2/64 | Training Loss: 1.0280\n",
            "Epoch 110/200 | Batch 3/64 | Training Loss: 1.2352\n",
            "Epoch 110/200 | Batch 4/64 | Training Loss: 1.3163\n",
            "Epoch 110/200 | Batch 5/64 | Training Loss: 0.9767\n",
            "Epoch 110/200 | Batch 6/64 | Training Loss: 0.9870\n",
            "Epoch 110/200 | Batch 7/64 | Training Loss: 1.2768\n",
            "Epoch 110/200 | Batch 8/64 | Training Loss: 1.0903\n",
            "Epoch 111/200 | Batch 1/64 | Training Loss: 1.1532\n",
            "Epoch 111/200 | Batch 2/64 | Training Loss: 1.0454\n",
            "Epoch 111/200 | Batch 3/64 | Training Loss: 0.9525\n",
            "Epoch 111/200 | Batch 4/64 | Training Loss: 1.1218\n",
            "Epoch 111/200 | Batch 5/64 | Training Loss: 1.0812\n",
            "Epoch 111/200 | Batch 6/64 | Training Loss: 0.9763\n",
            "Epoch 111/200 | Batch 7/64 | Training Loss: 1.0940\n",
            "Epoch 111/200 | Batch 8/64 | Training Loss: 1.2025\n",
            "Epoch 112/200 | Batch 1/64 | Training Loss: 0.9050\n",
            "Epoch 112/200 | Batch 2/64 | Training Loss: 1.0918\n",
            "Epoch 112/200 | Batch 3/64 | Training Loss: 0.9921\n",
            "Epoch 112/200 | Batch 4/64 | Training Loss: 1.2575\n",
            "Epoch 112/200 | Batch 5/64 | Training Loss: 1.0555\n",
            "Epoch 112/200 | Batch 6/64 | Training Loss: 1.1373\n",
            "Epoch 112/200 | Batch 7/64 | Training Loss: 1.0722\n",
            "Epoch 112/200 | Batch 8/64 | Training Loss: 1.0162\n",
            "Epoch 113/200 | Batch 1/64 | Training Loss: 1.1375\n",
            "Epoch 113/200 | Batch 2/64 | Training Loss: 1.1264\n",
            "Epoch 113/200 | Batch 3/64 | Training Loss: 1.2012\n",
            "Epoch 113/200 | Batch 4/64 | Training Loss: 1.0559\n",
            "Epoch 113/200 | Batch 5/64 | Training Loss: 1.1439\n",
            "Epoch 113/200 | Batch 6/64 | Training Loss: 1.0844\n",
            "Epoch 113/200 | Batch 7/64 | Training Loss: 0.8780\n",
            "Epoch 113/200 | Batch 8/64 | Training Loss: 0.5760\n",
            "Epoch 114/200 | Batch 1/64 | Training Loss: 1.0342\n",
            "Epoch 114/200 | Batch 2/64 | Training Loss: 1.0553\n",
            "Epoch 114/200 | Batch 3/64 | Training Loss: 1.1002\n",
            "Epoch 114/200 | Batch 4/64 | Training Loss: 0.9548\n",
            "Epoch 114/200 | Batch 5/64 | Training Loss: 1.1509\n",
            "Epoch 114/200 | Batch 6/64 | Training Loss: 1.1568\n",
            "Epoch 114/200 | Batch 7/64 | Training Loss: 1.0631\n",
            "Epoch 114/200 | Batch 8/64 | Training Loss: 1.0349\n",
            "Epoch 115/200 | Batch 1/64 | Training Loss: 1.1552\n",
            "Epoch 115/200 | Batch 2/64 | Training Loss: 1.3276\n",
            "Epoch 115/200 | Batch 3/64 | Training Loss: 1.0841\n",
            "Epoch 115/200 | Batch 4/64 | Training Loss: 0.8711\n",
            "Epoch 115/200 | Batch 5/64 | Training Loss: 0.9314\n",
            "Epoch 115/200 | Batch 6/64 | Training Loss: 1.0394\n",
            "Epoch 115/200 | Batch 7/64 | Training Loss: 1.0863\n",
            "Epoch 115/200 | Batch 8/64 | Training Loss: 1.1632\n",
            "Epoch 116/200 | Batch 1/64 | Training Loss: 1.1198\n",
            "Epoch 116/200 | Batch 2/64 | Training Loss: 0.9413\n",
            "Epoch 116/200 | Batch 3/64 | Training Loss: 0.8576\n",
            "Epoch 116/200 | Batch 4/64 | Training Loss: 0.9868\n",
            "Epoch 116/200 | Batch 5/64 | Training Loss: 1.0679\n",
            "Epoch 116/200 | Batch 6/64 | Training Loss: 1.0294\n",
            "Epoch 116/200 | Batch 7/64 | Training Loss: 1.2414\n",
            "Epoch 116/200 | Batch 8/64 | Training Loss: 1.4334\n",
            "Epoch 117/200 | Batch 1/64 | Training Loss: 1.0462\n",
            "Epoch 117/200 | Batch 2/64 | Training Loss: 0.8931\n",
            "Epoch 117/200 | Batch 3/64 | Training Loss: 0.9801\n",
            "Epoch 117/200 | Batch 4/64 | Training Loss: 1.1028\n",
            "Epoch 117/200 | Batch 5/64 | Training Loss: 1.3359\n",
            "Epoch 117/200 | Batch 6/64 | Training Loss: 0.8671\n",
            "Epoch 117/200 | Batch 7/64 | Training Loss: 1.0354\n",
            "Epoch 117/200 | Batch 8/64 | Training Loss: 1.0592\n",
            "Epoch 118/200 | Batch 1/64 | Training Loss: 1.0902\n",
            "Epoch 118/200 | Batch 2/64 | Training Loss: 0.8779\n",
            "Epoch 118/200 | Batch 3/64 | Training Loss: 0.8351\n",
            "Epoch 118/200 | Batch 4/64 | Training Loss: 1.0920\n",
            "Epoch 118/200 | Batch 5/64 | Training Loss: 0.9547\n",
            "Epoch 118/200 | Batch 6/64 | Training Loss: 1.1075\n",
            "Epoch 118/200 | Batch 7/64 | Training Loss: 1.1326\n",
            "Epoch 118/200 | Batch 8/64 | Training Loss: 0.9323\n",
            "Epoch 119/200 | Batch 1/64 | Training Loss: 1.1178\n",
            "Epoch 119/200 | Batch 2/64 | Training Loss: 0.9379\n",
            "Epoch 119/200 | Batch 3/64 | Training Loss: 0.8374\n",
            "Epoch 119/200 | Batch 4/64 | Training Loss: 1.0481\n",
            "Epoch 119/200 | Batch 5/64 | Training Loss: 0.8267\n",
            "Epoch 119/200 | Batch 6/64 | Training Loss: 0.8554\n",
            "Epoch 119/200 | Batch 7/64 | Training Loss: 1.2404\n",
            "Epoch 119/200 | Batch 8/64 | Training Loss: 0.9304\n",
            "Epoch 120/200 | Batch 1/64 | Training Loss: 0.9667\n",
            "Epoch 120/200 | Batch 2/64 | Training Loss: 1.0525\n",
            "Epoch 120/200 | Batch 3/64 | Training Loss: 1.1545\n",
            "Epoch 120/200 | Batch 4/64 | Training Loss: 0.8554\n",
            "Epoch 120/200 | Batch 5/64 | Training Loss: 0.9830\n",
            "Epoch 120/200 | Batch 6/64 | Training Loss: 1.0342\n",
            "Epoch 120/200 | Batch 7/64 | Training Loss: 0.8260\n",
            "Epoch 120/200 | Batch 8/64 | Training Loss: 0.9921\n",
            "Epoch 121/200 | Batch 1/64 | Training Loss: 0.8971\n",
            "Epoch 121/200 | Batch 2/64 | Training Loss: 1.1583\n",
            "Epoch 121/200 | Batch 3/64 | Training Loss: 1.0063\n",
            "Epoch 121/200 | Batch 4/64 | Training Loss: 1.1256\n",
            "Epoch 121/200 | Batch 5/64 | Training Loss: 0.8850\n",
            "Epoch 121/200 | Batch 6/64 | Training Loss: 0.8503\n",
            "Epoch 121/200 | Batch 7/64 | Training Loss: 0.9886\n",
            "Epoch 121/200 | Batch 8/64 | Training Loss: 1.3105\n",
            "Epoch 122/200 | Batch 1/64 | Training Loss: 1.0107\n",
            "Epoch 122/200 | Batch 2/64 | Training Loss: 0.9127\n",
            "Epoch 122/200 | Batch 3/64 | Training Loss: 0.9579\n",
            "Epoch 122/200 | Batch 4/64 | Training Loss: 0.8812\n",
            "Epoch 122/200 | Batch 5/64 | Training Loss: 1.0068\n",
            "Epoch 122/200 | Batch 6/64 | Training Loss: 1.0745\n",
            "Epoch 122/200 | Batch 7/64 | Training Loss: 0.8049\n",
            "Epoch 122/200 | Batch 8/64 | Training Loss: 1.1320\n",
            "Epoch 123/200 | Batch 1/64 | Training Loss: 0.9247\n",
            "Epoch 123/200 | Batch 2/64 | Training Loss: 0.8607\n",
            "Epoch 123/200 | Batch 3/64 | Training Loss: 0.9989\n",
            "Epoch 123/200 | Batch 4/64 | Training Loss: 0.9638\n",
            "Epoch 123/200 | Batch 5/64 | Training Loss: 0.8212\n",
            "Epoch 123/200 | Batch 6/64 | Training Loss: 1.1524\n",
            "Epoch 123/200 | Batch 7/64 | Training Loss: 0.9574\n",
            "Epoch 123/200 | Batch 8/64 | Training Loss: 0.7230\n",
            "Epoch 124/200 | Batch 1/64 | Training Loss: 1.2009\n",
            "Epoch 124/200 | Batch 2/64 | Training Loss: 0.8562\n",
            "Epoch 124/200 | Batch 3/64 | Training Loss: 0.9990\n",
            "Epoch 124/200 | Batch 4/64 | Training Loss: 0.8387\n",
            "Epoch 124/200 | Batch 5/64 | Training Loss: 0.9025\n",
            "Epoch 124/200 | Batch 6/64 | Training Loss: 1.0260\n",
            "Epoch 124/200 | Batch 7/64 | Training Loss: 0.7796\n",
            "Epoch 124/200 | Batch 8/64 | Training Loss: 0.8541\n",
            "Epoch 125/200 | Batch 1/64 | Training Loss: 0.8972\n",
            "Epoch 125/200 | Batch 2/64 | Training Loss: 0.9829\n",
            "Epoch 125/200 | Batch 3/64 | Training Loss: 1.1546\n",
            "Epoch 125/200 | Batch 4/64 | Training Loss: 0.7233\n",
            "Epoch 125/200 | Batch 5/64 | Training Loss: 0.9233\n",
            "Epoch 125/200 | Batch 6/64 | Training Loss: 0.9580\n",
            "Epoch 125/200 | Batch 7/64 | Training Loss: 0.9523\n",
            "Epoch 125/200 | Batch 8/64 | Training Loss: 0.6748\n",
            "Epoch 126/200 | Batch 1/64 | Training Loss: 1.0403\n",
            "Epoch 126/200 | Batch 2/64 | Training Loss: 0.9131\n",
            "Epoch 126/200 | Batch 3/64 | Training Loss: 0.8456\n",
            "Epoch 126/200 | Batch 4/64 | Training Loss: 0.9099\n",
            "Epoch 126/200 | Batch 5/64 | Training Loss: 0.9199\n",
            "Epoch 126/200 | Batch 6/64 | Training Loss: 0.9221\n",
            "Epoch 126/200 | Batch 7/64 | Training Loss: 0.9004\n",
            "Epoch 126/200 | Batch 8/64 | Training Loss: 0.6647\n",
            "Epoch 127/200 | Batch 1/64 | Training Loss: 0.9048\n",
            "Epoch 127/200 | Batch 2/64 | Training Loss: 0.8478\n",
            "Epoch 127/200 | Batch 3/64 | Training Loss: 0.8389\n",
            "Epoch 127/200 | Batch 4/64 | Training Loss: 0.9370\n",
            "Epoch 127/200 | Batch 5/64 | Training Loss: 0.9799\n",
            "Epoch 127/200 | Batch 6/64 | Training Loss: 0.9188\n",
            "Epoch 127/200 | Batch 7/64 | Training Loss: 0.9348\n",
            "Epoch 127/200 | Batch 8/64 | Training Loss: 0.8616\n",
            "Epoch 128/200 | Batch 1/64 | Training Loss: 0.9659\n",
            "Epoch 128/200 | Batch 2/64 | Training Loss: 0.9657\n",
            "Epoch 128/200 | Batch 3/64 | Training Loss: 0.9825\n",
            "Epoch 128/200 | Batch 4/64 | Training Loss: 0.9808\n",
            "Epoch 128/200 | Batch 5/64 | Training Loss: 0.7306\n",
            "Epoch 128/200 | Batch 6/64 | Training Loss: 0.8079\n",
            "Epoch 128/200 | Batch 7/64 | Training Loss: 0.7465\n",
            "Epoch 128/200 | Batch 8/64 | Training Loss: 1.1857\n",
            "Epoch 129/200 | Batch 1/64 | Training Loss: 0.8080\n",
            "Epoch 129/200 | Batch 2/64 | Training Loss: 1.0062\n",
            "Epoch 129/200 | Batch 3/64 | Training Loss: 1.0435\n",
            "Epoch 129/200 | Batch 4/64 | Training Loss: 0.7655\n",
            "Epoch 129/200 | Batch 5/64 | Training Loss: 0.8009\n",
            "Epoch 129/200 | Batch 6/64 | Training Loss: 0.9315\n",
            "Epoch 129/200 | Batch 7/64 | Training Loss: 1.0174\n",
            "Epoch 129/200 | Batch 8/64 | Training Loss: 1.0918\n",
            "Epoch 130/200 | Batch 1/64 | Training Loss: 0.9977\n",
            "Epoch 130/200 | Batch 2/64 | Training Loss: 1.0032\n",
            "Epoch 130/200 | Batch 3/64 | Training Loss: 0.7933\n",
            "Epoch 130/200 | Batch 4/64 | Training Loss: 0.7899\n",
            "Epoch 130/200 | Batch 5/64 | Training Loss: 1.0467\n",
            "Epoch 130/200 | Batch 6/64 | Training Loss: 1.0403\n",
            "Epoch 130/200 | Batch 7/64 | Training Loss: 0.9328\n",
            "Epoch 130/200 | Batch 8/64 | Training Loss: 1.2964\n",
            "Epoch 131/200 | Batch 1/64 | Training Loss: 0.9568\n",
            "Epoch 131/200 | Batch 2/64 | Training Loss: 0.9091\n",
            "Epoch 131/200 | Batch 3/64 | Training Loss: 0.8200\n",
            "Epoch 131/200 | Batch 4/64 | Training Loss: 1.0004\n",
            "Epoch 131/200 | Batch 5/64 | Training Loss: 1.1456\n",
            "Epoch 131/200 | Batch 6/64 | Training Loss: 1.0802\n",
            "Epoch 131/200 | Batch 7/64 | Training Loss: 0.8299\n",
            "Epoch 131/200 | Batch 8/64 | Training Loss: 0.9026\n",
            "Epoch 132/200 | Batch 1/64 | Training Loss: 0.8128\n",
            "Epoch 132/200 | Batch 2/64 | Training Loss: 0.8206\n",
            "Epoch 132/200 | Batch 3/64 | Training Loss: 0.8150\n",
            "Epoch 132/200 | Batch 4/64 | Training Loss: 1.1862\n",
            "Epoch 132/200 | Batch 5/64 | Training Loss: 1.0481\n",
            "Epoch 132/200 | Batch 6/64 | Training Loss: 0.8670\n",
            "Epoch 132/200 | Batch 7/64 | Training Loss: 0.9453\n",
            "Epoch 132/200 | Batch 8/64 | Training Loss: 0.9581\n",
            "Epoch 133/200 | Batch 1/64 | Training Loss: 1.0386\n",
            "Epoch 133/200 | Batch 2/64 | Training Loss: 0.9429\n",
            "Epoch 133/200 | Batch 3/64 | Training Loss: 0.8023\n",
            "Epoch 133/200 | Batch 4/64 | Training Loss: 1.1663\n",
            "Epoch 133/200 | Batch 5/64 | Training Loss: 0.6759\n",
            "Epoch 133/200 | Batch 6/64 | Training Loss: 0.7435\n",
            "Epoch 133/200 | Batch 7/64 | Training Loss: 0.9284\n",
            "Epoch 133/200 | Batch 8/64 | Training Loss: 1.0165\n",
            "Epoch 134/200 | Batch 1/64 | Training Loss: 0.8039\n",
            "Epoch 134/200 | Batch 2/64 | Training Loss: 0.7451\n",
            "Epoch 134/200 | Batch 3/64 | Training Loss: 0.8793\n",
            "Epoch 134/200 | Batch 4/64 | Training Loss: 0.7143\n",
            "Epoch 134/200 | Batch 5/64 | Training Loss: 1.0804\n",
            "Epoch 134/200 | Batch 6/64 | Training Loss: 0.8700\n",
            "Epoch 134/200 | Batch 7/64 | Training Loss: 0.9255\n",
            "Epoch 134/200 | Batch 8/64 | Training Loss: 1.1151\n",
            "Epoch 135/200 | Batch 1/64 | Training Loss: 0.8636\n",
            "Epoch 135/200 | Batch 2/64 | Training Loss: 1.0957\n",
            "Epoch 135/200 | Batch 3/64 | Training Loss: 0.9373\n",
            "Epoch 135/200 | Batch 4/64 | Training Loss: 0.8872\n",
            "Epoch 135/200 | Batch 5/64 | Training Loss: 0.7473\n",
            "Epoch 135/200 | Batch 6/64 | Training Loss: 1.1492\n",
            "Epoch 135/200 | Batch 7/64 | Training Loss: 0.8330\n",
            "Epoch 135/200 | Batch 8/64 | Training Loss: 0.6705\n",
            "Epoch 136/200 | Batch 1/64 | Training Loss: 1.0272\n",
            "Epoch 136/200 | Batch 2/64 | Training Loss: 0.8110\n",
            "Epoch 136/200 | Batch 3/64 | Training Loss: 0.8411\n",
            "Epoch 136/200 | Batch 4/64 | Training Loss: 0.8526\n",
            "Epoch 136/200 | Batch 5/64 | Training Loss: 0.9403\n",
            "Epoch 136/200 | Batch 6/64 | Training Loss: 0.9403\n",
            "Epoch 136/200 | Batch 7/64 | Training Loss: 0.9043\n",
            "Epoch 136/200 | Batch 8/64 | Training Loss: 1.0864\n",
            "Epoch 137/200 | Batch 1/64 | Training Loss: 0.7697\n",
            "Epoch 137/200 | Batch 2/64 | Training Loss: 0.8686\n",
            "Epoch 137/200 | Batch 3/64 | Training Loss: 0.8429\n",
            "Epoch 137/200 | Batch 4/64 | Training Loss: 1.0086\n",
            "Epoch 137/200 | Batch 5/64 | Training Loss: 0.6797\n",
            "Epoch 137/200 | Batch 6/64 | Training Loss: 0.8275\n",
            "Epoch 137/200 | Batch 7/64 | Training Loss: 0.9548\n",
            "Epoch 137/200 | Batch 8/64 | Training Loss: 1.5584\n",
            "Epoch 138/200 | Batch 1/64 | Training Loss: 0.7648\n",
            "Epoch 138/200 | Batch 2/64 | Training Loss: 0.8799\n",
            "Epoch 138/200 | Batch 3/64 | Training Loss: 0.9915\n",
            "Epoch 138/200 | Batch 4/64 | Training Loss: 0.9507\n",
            "Epoch 138/200 | Batch 5/64 | Training Loss: 0.8485\n",
            "Epoch 138/200 | Batch 6/64 | Training Loss: 0.9409\n",
            "Epoch 138/200 | Batch 7/64 | Training Loss: 0.8915\n",
            "Epoch 138/200 | Batch 8/64 | Training Loss: 0.9604\n",
            "Epoch 139/200 | Batch 1/64 | Training Loss: 0.8775\n",
            "Epoch 139/200 | Batch 2/64 | Training Loss: 0.7549\n",
            "Epoch 139/200 | Batch 3/64 | Training Loss: 0.8520\n",
            "Epoch 139/200 | Batch 4/64 | Training Loss: 0.8940\n",
            "Epoch 139/200 | Batch 5/64 | Training Loss: 0.8888\n",
            "Epoch 139/200 | Batch 6/64 | Training Loss: 0.9736\n",
            "Epoch 139/200 | Batch 7/64 | Training Loss: 1.0721\n",
            "Epoch 139/200 | Batch 8/64 | Training Loss: 0.8729\n",
            "Epoch 140/200 | Batch 1/64 | Training Loss: 0.8660\n",
            "Epoch 140/200 | Batch 2/64 | Training Loss: 0.8790\n",
            "Epoch 140/200 | Batch 3/64 | Training Loss: 0.8973\n",
            "Epoch 140/200 | Batch 4/64 | Training Loss: 0.7866\n",
            "Epoch 140/200 | Batch 5/64 | Training Loss: 0.8142\n",
            "Epoch 140/200 | Batch 6/64 | Training Loss: 0.9695\n",
            "Epoch 140/200 | Batch 7/64 | Training Loss: 0.8455\n",
            "Epoch 140/200 | Batch 8/64 | Training Loss: 0.5921\n",
            "Epoch 141/200 | Batch 1/64 | Training Loss: 0.7342\n",
            "Epoch 141/200 | Batch 2/64 | Training Loss: 0.8553\n",
            "Epoch 141/200 | Batch 3/64 | Training Loss: 0.8194\n",
            "Epoch 141/200 | Batch 4/64 | Training Loss: 0.8578\n",
            "Epoch 141/200 | Batch 5/64 | Training Loss: 1.0116\n",
            "Epoch 141/200 | Batch 6/64 | Training Loss: 0.8852\n",
            "Epoch 141/200 | Batch 7/64 | Training Loss: 0.7932\n",
            "Epoch 141/200 | Batch 8/64 | Training Loss: 0.7979\n",
            "Epoch 142/200 | Batch 1/64 | Training Loss: 0.9529\n",
            "Epoch 142/200 | Batch 2/64 | Training Loss: 0.6942\n",
            "Epoch 142/200 | Batch 3/64 | Training Loss: 0.8092\n",
            "Epoch 142/200 | Batch 4/64 | Training Loss: 0.8689\n",
            "Epoch 142/200 | Batch 5/64 | Training Loss: 0.6623\n",
            "Epoch 142/200 | Batch 6/64 | Training Loss: 0.8918\n",
            "Epoch 142/200 | Batch 7/64 | Training Loss: 0.8615\n",
            "Epoch 142/200 | Batch 8/64 | Training Loss: 0.8576\n",
            "Epoch 143/200 | Batch 1/64 | Training Loss: 0.7695\n",
            "Epoch 143/200 | Batch 2/64 | Training Loss: 0.7610\n",
            "Epoch 143/200 | Batch 3/64 | Training Loss: 0.6647\n",
            "Epoch 143/200 | Batch 4/64 | Training Loss: 0.9838\n",
            "Epoch 143/200 | Batch 5/64 | Training Loss: 0.7788\n",
            "Epoch 143/200 | Batch 6/64 | Training Loss: 0.7984\n",
            "Epoch 143/200 | Batch 7/64 | Training Loss: 0.9660\n",
            "Epoch 143/200 | Batch 8/64 | Training Loss: 0.6824\n",
            "Epoch 144/200 | Batch 1/64 | Training Loss: 0.7622\n",
            "Epoch 144/200 | Batch 2/64 | Training Loss: 0.8398\n",
            "Epoch 144/200 | Batch 3/64 | Training Loss: 0.7326\n",
            "Epoch 144/200 | Batch 4/64 | Training Loss: 0.9416\n",
            "Epoch 144/200 | Batch 5/64 | Training Loss: 0.7280\n",
            "Epoch 144/200 | Batch 6/64 | Training Loss: 0.6880\n",
            "Epoch 144/200 | Batch 7/64 | Training Loss: 0.8777\n",
            "Epoch 144/200 | Batch 8/64 | Training Loss: 1.1518\n",
            "Epoch 145/200 | Batch 1/64 | Training Loss: 0.7303\n",
            "Epoch 145/200 | Batch 2/64 | Training Loss: 0.8447\n",
            "Epoch 145/200 | Batch 3/64 | Training Loss: 1.0131\n",
            "Epoch 145/200 | Batch 4/64 | Training Loss: 0.7522\n",
            "Epoch 145/200 | Batch 5/64 | Training Loss: 0.9111\n",
            "Epoch 145/200 | Batch 6/64 | Training Loss: 0.6697\n",
            "Epoch 145/200 | Batch 7/64 | Training Loss: 0.8275\n",
            "Epoch 145/200 | Batch 8/64 | Training Loss: 0.6658\n",
            "Epoch 146/200 | Batch 1/64 | Training Loss: 0.9909\n",
            "Epoch 146/200 | Batch 2/64 | Training Loss: 0.7749\n",
            "Epoch 146/200 | Batch 3/64 | Training Loss: 0.7066\n",
            "Epoch 146/200 | Batch 4/64 | Training Loss: 0.8906\n",
            "Epoch 146/200 | Batch 5/64 | Training Loss: 0.8552\n",
            "Epoch 146/200 | Batch 6/64 | Training Loss: 0.8107\n",
            "Epoch 146/200 | Batch 7/64 | Training Loss: 0.7511\n",
            "Epoch 146/200 | Batch 8/64 | Training Loss: 0.9177\n",
            "Epoch 147/200 | Batch 1/64 | Training Loss: 0.7573\n",
            "Epoch 147/200 | Batch 2/64 | Training Loss: 0.7866\n",
            "Epoch 147/200 | Batch 3/64 | Training Loss: 0.9227\n",
            "Epoch 147/200 | Batch 4/64 | Training Loss: 0.8152\n",
            "Epoch 147/200 | Batch 5/64 | Training Loss: 0.6153\n",
            "Epoch 147/200 | Batch 6/64 | Training Loss: 0.7799\n",
            "Epoch 147/200 | Batch 7/64 | Training Loss: 0.7688\n",
            "Epoch 147/200 | Batch 8/64 | Training Loss: 1.0554\n",
            "Epoch 148/200 | Batch 1/64 | Training Loss: 0.7942\n",
            "Epoch 148/200 | Batch 2/64 | Training Loss: 0.8688\n",
            "Epoch 148/200 | Batch 3/64 | Training Loss: 0.7404\n",
            "Epoch 148/200 | Batch 4/64 | Training Loss: 0.9253\n",
            "Epoch 148/200 | Batch 5/64 | Training Loss: 0.5550\n",
            "Epoch 148/200 | Batch 6/64 | Training Loss: 0.7760\n",
            "Epoch 148/200 | Batch 7/64 | Training Loss: 0.8892\n",
            "Epoch 148/200 | Batch 8/64 | Training Loss: 1.0504\n",
            "Epoch 149/200 | Batch 1/64 | Training Loss: 0.7585\n",
            "Epoch 149/200 | Batch 2/64 | Training Loss: 1.0062\n",
            "Epoch 149/200 | Batch 3/64 | Training Loss: 0.7354\n",
            "Epoch 149/200 | Batch 4/64 | Training Loss: 0.8029\n",
            "Epoch 149/200 | Batch 5/64 | Training Loss: 0.7958\n",
            "Epoch 149/200 | Batch 6/64 | Training Loss: 0.6854\n",
            "Epoch 149/200 | Batch 7/64 | Training Loss: 0.7528\n",
            "Epoch 149/200 | Batch 8/64 | Training Loss: 0.8343\n",
            "Epoch 150/200 | Batch 1/64 | Training Loss: 0.9710\n",
            "Epoch 150/200 | Batch 2/64 | Training Loss: 0.7373\n",
            "Epoch 150/200 | Batch 3/64 | Training Loss: 0.9043\n",
            "Epoch 150/200 | Batch 4/64 | Training Loss: 0.9821\n",
            "Epoch 150/200 | Batch 5/64 | Training Loss: 0.7303\n",
            "Epoch 150/200 | Batch 6/64 | Training Loss: 0.6642\n",
            "Epoch 150/200 | Batch 7/64 | Training Loss: 0.6868\n",
            "Epoch 150/200 | Batch 8/64 | Training Loss: 0.5664\n",
            "Epoch 151/200 | Batch 1/64 | Training Loss: 0.6900\n",
            "Epoch 151/200 | Batch 2/64 | Training Loss: 0.7302\n",
            "Epoch 151/200 | Batch 3/64 | Training Loss: 0.6461\n",
            "Epoch 151/200 | Batch 4/64 | Training Loss: 0.7089\n",
            "Epoch 151/200 | Batch 5/64 | Training Loss: 0.7177\n",
            "Epoch 151/200 | Batch 6/64 | Training Loss: 0.8181\n",
            "Epoch 151/200 | Batch 7/64 | Training Loss: 0.8405\n",
            "Epoch 151/200 | Batch 8/64 | Training Loss: 0.7500\n",
            "Epoch 152/200 | Batch 1/64 | Training Loss: 0.6563\n",
            "Epoch 152/200 | Batch 2/64 | Training Loss: 0.8730\n",
            "Epoch 152/200 | Batch 3/64 | Training Loss: 0.6341\n",
            "Epoch 152/200 | Batch 4/64 | Training Loss: 0.6490\n",
            "Epoch 152/200 | Batch 5/64 | Training Loss: 0.6510\n",
            "Epoch 152/200 | Batch 6/64 | Training Loss: 0.7772\n",
            "Epoch 152/200 | Batch 7/64 | Training Loss: 0.7207\n",
            "Epoch 152/200 | Batch 8/64 | Training Loss: 0.8874\n",
            "Epoch 153/200 | Batch 1/64 | Training Loss: 0.6162\n",
            "Epoch 153/200 | Batch 2/64 | Training Loss: 0.7576\n",
            "Epoch 153/200 | Batch 3/64 | Training Loss: 0.7091\n",
            "Epoch 153/200 | Batch 4/64 | Training Loss: 0.8094\n",
            "Epoch 153/200 | Batch 5/64 | Training Loss: 0.7751\n",
            "Epoch 153/200 | Batch 6/64 | Training Loss: 0.6704\n",
            "Epoch 153/200 | Batch 7/64 | Training Loss: 0.6535\n",
            "Epoch 153/200 | Batch 8/64 | Training Loss: 0.6936\n",
            "Epoch 154/200 | Batch 1/64 | Training Loss: 0.5765\n",
            "Epoch 154/200 | Batch 2/64 | Training Loss: 0.6635\n",
            "Epoch 154/200 | Batch 3/64 | Training Loss: 0.6921\n",
            "Epoch 154/200 | Batch 4/64 | Training Loss: 0.7912\n",
            "Epoch 154/200 | Batch 5/64 | Training Loss: 0.6096\n",
            "Epoch 154/200 | Batch 6/64 | Training Loss: 0.5903\n",
            "Epoch 154/200 | Batch 7/64 | Training Loss: 0.9277\n",
            "Epoch 154/200 | Batch 8/64 | Training Loss: 0.8032\n",
            "Epoch 155/200 | Batch 1/64 | Training Loss: 0.7576\n",
            "Epoch 155/200 | Batch 2/64 | Training Loss: 0.6437\n",
            "Epoch 155/200 | Batch 3/64 | Training Loss: 0.6876\n",
            "Epoch 155/200 | Batch 4/64 | Training Loss: 0.7339\n",
            "Epoch 155/200 | Batch 5/64 | Training Loss: 0.6294\n",
            "Epoch 155/200 | Batch 6/64 | Training Loss: 0.7452\n",
            "Epoch 155/200 | Batch 7/64 | Training Loss: 0.6763\n",
            "Epoch 155/200 | Batch 8/64 | Training Loss: 0.5390\n",
            "Epoch 156/200 | Batch 1/64 | Training Loss: 0.5011\n",
            "Epoch 156/200 | Batch 2/64 | Training Loss: 0.9211\n",
            "Epoch 156/200 | Batch 3/64 | Training Loss: 0.6215\n",
            "Epoch 156/200 | Batch 4/64 | Training Loss: 0.6638\n",
            "Epoch 156/200 | Batch 5/64 | Training Loss: 0.8336\n",
            "Epoch 156/200 | Batch 6/64 | Training Loss: 0.7478\n",
            "Epoch 156/200 | Batch 7/64 | Training Loss: 0.6446\n",
            "Epoch 156/200 | Batch 8/64 | Training Loss: 0.4549\n",
            "Epoch 157/200 | Batch 1/64 | Training Loss: 0.6299\n",
            "Epoch 157/200 | Batch 2/64 | Training Loss: 0.9198\n",
            "Epoch 157/200 | Batch 3/64 | Training Loss: 0.7297\n",
            "Epoch 157/200 | Batch 4/64 | Training Loss: 0.6299\n",
            "Epoch 157/200 | Batch 5/64 | Training Loss: 0.6696\n",
            "Epoch 157/200 | Batch 6/64 | Training Loss: 0.5493\n",
            "Epoch 157/200 | Batch 7/64 | Training Loss: 0.6787\n",
            "Epoch 157/200 | Batch 8/64 | Training Loss: 0.4908\n",
            "Epoch 158/200 | Batch 1/64 | Training Loss: 0.8981\n",
            "Epoch 158/200 | Batch 2/64 | Training Loss: 0.7250\n",
            "Epoch 158/200 | Batch 3/64 | Training Loss: 0.6187\n",
            "Epoch 158/200 | Batch 4/64 | Training Loss: 0.6811\n",
            "Epoch 158/200 | Batch 5/64 | Training Loss: 0.7995\n",
            "Epoch 158/200 | Batch 6/64 | Training Loss: 0.5112\n",
            "Epoch 158/200 | Batch 7/64 | Training Loss: 0.7321\n",
            "Epoch 158/200 | Batch 8/64 | Training Loss: 0.8401\n",
            "Epoch 159/200 | Batch 1/64 | Training Loss: 0.7243\n",
            "Epoch 159/200 | Batch 2/64 | Training Loss: 0.6761\n",
            "Epoch 159/200 | Batch 3/64 | Training Loss: 0.7129\n",
            "Epoch 159/200 | Batch 4/64 | Training Loss: 0.6942\n",
            "Epoch 159/200 | Batch 5/64 | Training Loss: 0.7924\n",
            "Epoch 159/200 | Batch 6/64 | Training Loss: 0.5908\n",
            "Epoch 159/200 | Batch 7/64 | Training Loss: 0.7066\n",
            "Epoch 159/200 | Batch 8/64 | Training Loss: 0.8735\n",
            "Epoch 160/200 | Batch 1/64 | Training Loss: 0.7250\n",
            "Epoch 160/200 | Batch 2/64 | Training Loss: 0.6019\n",
            "Epoch 160/200 | Batch 3/64 | Training Loss: 0.5910\n",
            "Epoch 160/200 | Batch 4/64 | Training Loss: 0.7868\n",
            "Epoch 160/200 | Batch 5/64 | Training Loss: 0.8303\n",
            "Epoch 160/200 | Batch 6/64 | Training Loss: 0.7367\n",
            "Epoch 160/200 | Batch 7/64 | Training Loss: 0.5658\n",
            "Epoch 160/200 | Batch 8/64 | Training Loss: 1.2686\n",
            "Epoch 161/200 | Batch 1/64 | Training Loss: 0.7761\n",
            "Epoch 161/200 | Batch 2/64 | Training Loss: 0.5784\n",
            "Epoch 161/200 | Batch 3/64 | Training Loss: 0.6714\n",
            "Epoch 161/200 | Batch 4/64 | Training Loss: 0.6639\n",
            "Epoch 161/200 | Batch 5/64 | Training Loss: 0.6972\n",
            "Epoch 161/200 | Batch 6/64 | Training Loss: 0.7387\n",
            "Epoch 161/200 | Batch 7/64 | Training Loss: 0.7115\n",
            "Epoch 161/200 | Batch 8/64 | Training Loss: 0.6014\n",
            "Epoch 162/200 | Batch 1/64 | Training Loss: 0.7724\n",
            "Epoch 162/200 | Batch 2/64 | Training Loss: 0.6208\n",
            "Epoch 162/200 | Batch 3/64 | Training Loss: 0.8179\n",
            "Epoch 162/200 | Batch 4/64 | Training Loss: 0.7158\n",
            "Epoch 162/200 | Batch 5/64 | Training Loss: 0.5812\n",
            "Epoch 162/200 | Batch 6/64 | Training Loss: 0.6067\n",
            "Epoch 162/200 | Batch 7/64 | Training Loss: 0.6928\n",
            "Epoch 162/200 | Batch 8/64 | Training Loss: 0.9708\n",
            "Epoch 163/200 | Batch 1/64 | Training Loss: 0.7031\n",
            "Epoch 163/200 | Batch 2/64 | Training Loss: 0.8740\n",
            "Epoch 163/200 | Batch 3/64 | Training Loss: 0.8825\n",
            "Epoch 163/200 | Batch 4/64 | Training Loss: 0.7985\n",
            "Epoch 163/200 | Batch 5/64 | Training Loss: 0.7498\n",
            "Epoch 163/200 | Batch 6/64 | Training Loss: 0.7754\n",
            "Epoch 163/200 | Batch 7/64 | Training Loss: 0.6635\n",
            "Epoch 163/200 | Batch 8/64 | Training Loss: 0.7616\n",
            "Epoch 164/200 | Batch 1/64 | Training Loss: 0.7355\n",
            "Epoch 164/200 | Batch 2/64 | Training Loss: 0.5821\n",
            "Epoch 164/200 | Batch 3/64 | Training Loss: 0.8960\n",
            "Epoch 164/200 | Batch 4/64 | Training Loss: 0.6071\n",
            "Epoch 164/200 | Batch 5/64 | Training Loss: 0.7069\n",
            "Epoch 164/200 | Batch 6/64 | Training Loss: 0.6286\n",
            "Epoch 164/200 | Batch 7/64 | Training Loss: 0.7562\n",
            "Epoch 164/200 | Batch 8/64 | Training Loss: 0.9222\n",
            "Epoch 165/200 | Batch 1/64 | Training Loss: 0.5496\n",
            "Epoch 165/200 | Batch 2/64 | Training Loss: 0.6803\n",
            "Epoch 165/200 | Batch 3/64 | Training Loss: 0.7842\n",
            "Epoch 165/200 | Batch 4/64 | Training Loss: 0.6025\n",
            "Epoch 165/200 | Batch 5/64 | Training Loss: 0.7708\n",
            "Epoch 165/200 | Batch 6/64 | Training Loss: 0.5600\n",
            "Epoch 165/200 | Batch 7/64 | Training Loss: 0.6990\n",
            "Epoch 165/200 | Batch 8/64 | Training Loss: 0.4343\n",
            "Epoch 166/200 | Batch 1/64 | Training Loss: 0.5459\n",
            "Epoch 166/200 | Batch 2/64 | Training Loss: 0.5259\n",
            "Epoch 166/200 | Batch 3/64 | Training Loss: 0.7666\n",
            "Epoch 166/200 | Batch 4/64 | Training Loss: 0.6145\n",
            "Epoch 166/200 | Batch 5/64 | Training Loss: 0.6364\n",
            "Epoch 166/200 | Batch 6/64 | Training Loss: 0.5772\n",
            "Epoch 166/200 | Batch 7/64 | Training Loss: 0.6126\n",
            "Epoch 166/200 | Batch 8/64 | Training Loss: 0.6182\n",
            "Epoch 167/200 | Batch 1/64 | Training Loss: 0.6238\n",
            "Epoch 167/200 | Batch 2/64 | Training Loss: 0.5918\n",
            "Epoch 167/200 | Batch 3/64 | Training Loss: 0.7297\n",
            "Epoch 167/200 | Batch 4/64 | Training Loss: 0.5388\n",
            "Epoch 167/200 | Batch 5/64 | Training Loss: 0.5692\n",
            "Epoch 167/200 | Batch 6/64 | Training Loss: 0.5312\n",
            "Epoch 167/200 | Batch 7/64 | Training Loss: 0.6587\n",
            "Epoch 167/200 | Batch 8/64 | Training Loss: 0.5536\n",
            "Epoch 168/200 | Batch 1/64 | Training Loss: 0.7624\n",
            "Epoch 168/200 | Batch 2/64 | Training Loss: 0.6468\n",
            "Epoch 168/200 | Batch 3/64 | Training Loss: 0.5959\n",
            "Epoch 168/200 | Batch 4/64 | Training Loss: 0.5855\n",
            "Epoch 168/200 | Batch 5/64 | Training Loss: 0.6256\n",
            "Epoch 168/200 | Batch 6/64 | Training Loss: 0.6818\n",
            "Epoch 168/200 | Batch 7/64 | Training Loss: 0.5028\n",
            "Epoch 168/200 | Batch 8/64 | Training Loss: 0.5395\n",
            "Epoch 169/200 | Batch 1/64 | Training Loss: 0.5661\n",
            "Epoch 169/200 | Batch 2/64 | Training Loss: 0.5647\n",
            "Epoch 169/200 | Batch 3/64 | Training Loss: 0.6625\n",
            "Epoch 169/200 | Batch 4/64 | Training Loss: 0.7296\n",
            "Epoch 169/200 | Batch 5/64 | Training Loss: 0.4660\n",
            "Epoch 169/200 | Batch 6/64 | Training Loss: 0.6225\n",
            "Epoch 169/200 | Batch 7/64 | Training Loss: 0.8711\n",
            "Epoch 169/200 | Batch 8/64 | Training Loss: 0.4057\n",
            "Epoch 170/200 | Batch 1/64 | Training Loss: 0.5664\n",
            "Epoch 170/200 | Batch 2/64 | Training Loss: 0.6881\n",
            "Epoch 170/200 | Batch 3/64 | Training Loss: 0.4220\n",
            "Epoch 170/200 | Batch 4/64 | Training Loss: 0.7416\n",
            "Epoch 170/200 | Batch 5/64 | Training Loss: 0.4727\n",
            "Epoch 170/200 | Batch 6/64 | Training Loss: 0.5965\n",
            "Epoch 170/200 | Batch 7/64 | Training Loss: 0.5375\n",
            "Epoch 170/200 | Batch 8/64 | Training Loss: 0.5329\n",
            "Epoch 171/200 | Batch 1/64 | Training Loss: 0.4604\n",
            "Epoch 171/200 | Batch 2/64 | Training Loss: 0.5489\n",
            "Epoch 171/200 | Batch 3/64 | Training Loss: 0.5664\n",
            "Epoch 171/200 | Batch 4/64 | Training Loss: 0.5753\n",
            "Epoch 171/200 | Batch 5/64 | Training Loss: 0.7608\n",
            "Epoch 171/200 | Batch 6/64 | Training Loss: 0.6299\n",
            "Epoch 171/200 | Batch 7/64 | Training Loss: 0.5516\n",
            "Epoch 171/200 | Batch 8/64 | Training Loss: 0.7196\n",
            "Epoch 172/200 | Batch 1/64 | Training Loss: 0.6111\n",
            "Epoch 172/200 | Batch 2/64 | Training Loss: 0.6178\n",
            "Epoch 172/200 | Batch 3/64 | Training Loss: 0.4658\n",
            "Epoch 172/200 | Batch 4/64 | Training Loss: 0.8057\n",
            "Epoch 172/200 | Batch 5/64 | Training Loss: 0.7111\n",
            "Epoch 172/200 | Batch 6/64 | Training Loss: 0.6432\n",
            "Epoch 172/200 | Batch 7/64 | Training Loss: 0.5495\n",
            "Epoch 172/200 | Batch 8/64 | Training Loss: 0.3085\n",
            "Epoch 173/200 | Batch 1/64 | Training Loss: 0.6415\n",
            "Epoch 173/200 | Batch 2/64 | Training Loss: 0.5933\n",
            "Epoch 173/200 | Batch 3/64 | Training Loss: 0.6162\n",
            "Epoch 173/200 | Batch 4/64 | Training Loss: 0.4620\n",
            "Epoch 173/200 | Batch 5/64 | Training Loss: 0.5412\n",
            "Epoch 173/200 | Batch 6/64 | Training Loss: 0.6104\n",
            "Epoch 173/200 | Batch 7/64 | Training Loss: 0.5997\n",
            "Epoch 173/200 | Batch 8/64 | Training Loss: 0.6999\n",
            "Epoch 174/200 | Batch 1/64 | Training Loss: 0.6397\n",
            "Epoch 174/200 | Batch 2/64 | Training Loss: 0.4822\n",
            "Epoch 174/200 | Batch 3/64 | Training Loss: 0.4961\n",
            "Epoch 174/200 | Batch 4/64 | Training Loss: 0.6579\n",
            "Epoch 174/200 | Batch 5/64 | Training Loss: 0.5199\n",
            "Epoch 174/200 | Batch 6/64 | Training Loss: 0.4864\n",
            "Epoch 174/200 | Batch 7/64 | Training Loss: 0.7219\n",
            "Epoch 174/200 | Batch 8/64 | Training Loss: 0.4728\n",
            "Epoch 175/200 | Batch 1/64 | Training Loss: 0.6204\n",
            "Epoch 175/200 | Batch 2/64 | Training Loss: 0.6157\n",
            "Epoch 175/200 | Batch 3/64 | Training Loss: 0.5488\n",
            "Epoch 175/200 | Batch 4/64 | Training Loss: 0.4498\n",
            "Epoch 175/200 | Batch 5/64 | Training Loss: 0.5075\n",
            "Epoch 175/200 | Batch 6/64 | Training Loss: 0.5534\n",
            "Epoch 175/200 | Batch 7/64 | Training Loss: 0.6816\n",
            "Epoch 175/200 | Batch 8/64 | Training Loss: 0.6837\n",
            "Epoch 176/200 | Batch 1/64 | Training Loss: 0.5494\n",
            "Epoch 176/200 | Batch 2/64 | Training Loss: 0.5884\n",
            "Epoch 176/200 | Batch 3/64 | Training Loss: 0.6602\n",
            "Epoch 176/200 | Batch 4/64 | Training Loss: 0.4775\n",
            "Epoch 176/200 | Batch 5/64 | Training Loss: 0.6552\n",
            "Epoch 176/200 | Batch 6/64 | Training Loss: 0.5103\n",
            "Epoch 176/200 | Batch 7/64 | Training Loss: 0.5610\n",
            "Epoch 176/200 | Batch 8/64 | Training Loss: 0.5959\n",
            "Epoch 177/200 | Batch 1/64 | Training Loss: 0.6162\n",
            "Epoch 177/200 | Batch 2/64 | Training Loss: 0.8049\n",
            "Epoch 177/200 | Batch 3/64 | Training Loss: 0.4328\n",
            "Epoch 177/200 | Batch 4/64 | Training Loss: 0.5744\n",
            "Epoch 177/200 | Batch 5/64 | Training Loss: 0.5434\n",
            "Epoch 177/200 | Batch 6/64 | Training Loss: 0.6810\n",
            "Epoch 177/200 | Batch 7/64 | Training Loss: 0.3871\n",
            "Epoch 177/200 | Batch 8/64 | Training Loss: 0.5985\n",
            "Epoch 178/200 | Batch 1/64 | Training Loss: 0.6294\n",
            "Epoch 178/200 | Batch 2/64 | Training Loss: 0.4758\n",
            "Epoch 178/200 | Batch 3/64 | Training Loss: 0.5737\n",
            "Epoch 178/200 | Batch 4/64 | Training Loss: 0.6035\n",
            "Epoch 178/200 | Batch 5/64 | Training Loss: 0.5217\n",
            "Epoch 178/200 | Batch 6/64 | Training Loss: 0.5078\n",
            "Epoch 178/200 | Batch 7/64 | Training Loss: 0.5434\n",
            "Epoch 178/200 | Batch 8/64 | Training Loss: 0.4149\n",
            "Epoch 179/200 | Batch 1/64 | Training Loss: 0.5082\n",
            "Epoch 179/200 | Batch 2/64 | Training Loss: 0.5375\n",
            "Epoch 179/200 | Batch 3/64 | Training Loss: 0.5482\n",
            "Epoch 179/200 | Batch 4/64 | Training Loss: 0.6547\n",
            "Epoch 179/200 | Batch 5/64 | Training Loss: 0.5684\n",
            "Epoch 179/200 | Batch 6/64 | Training Loss: 0.4658\n",
            "Epoch 179/200 | Batch 7/64 | Training Loss: 0.5041\n",
            "Epoch 179/200 | Batch 8/64 | Training Loss: 0.4825\n",
            "Epoch 180/200 | Batch 1/64 | Training Loss: 0.4631\n",
            "Epoch 180/200 | Batch 2/64 | Training Loss: 0.5637\n",
            "Epoch 180/200 | Batch 3/64 | Training Loss: 0.6656\n",
            "Epoch 180/200 | Batch 4/64 | Training Loss: 0.3325\n",
            "Epoch 180/200 | Batch 5/64 | Training Loss: 0.5094\n",
            "Epoch 180/200 | Batch 6/64 | Training Loss: 0.4461\n",
            "Epoch 180/200 | Batch 7/64 | Training Loss: 0.7024\n",
            "Epoch 180/200 | Batch 8/64 | Training Loss: 0.7573\n",
            "Epoch 181/200 | Batch 1/64 | Training Loss: 0.5048\n",
            "Epoch 181/200 | Batch 2/64 | Training Loss: 0.4651\n",
            "Epoch 181/200 | Batch 3/64 | Training Loss: 0.5471\n",
            "Epoch 181/200 | Batch 4/64 | Training Loss: 0.5403\n",
            "Epoch 181/200 | Batch 5/64 | Training Loss: 0.5552\n",
            "Epoch 181/200 | Batch 6/64 | Training Loss: 0.6857\n",
            "Epoch 181/200 | Batch 7/64 | Training Loss: 0.5554\n",
            "Epoch 181/200 | Batch 8/64 | Training Loss: 0.6658\n",
            "Epoch 182/200 | Batch 1/64 | Training Loss: 0.5950\n",
            "Epoch 182/200 | Batch 2/64 | Training Loss: 0.5556\n",
            "Epoch 182/200 | Batch 3/64 | Training Loss: 0.5032\n",
            "Epoch 182/200 | Batch 4/64 | Training Loss: 0.5185\n",
            "Epoch 182/200 | Batch 5/64 | Training Loss: 0.5358\n",
            "Epoch 182/200 | Batch 6/64 | Training Loss: 0.5325\n",
            "Epoch 182/200 | Batch 7/64 | Training Loss: 0.6939\n",
            "Epoch 182/200 | Batch 8/64 | Training Loss: 0.4131\n",
            "Epoch 183/200 | Batch 1/64 | Training Loss: 0.6119\n",
            "Epoch 183/200 | Batch 2/64 | Training Loss: 0.6227\n",
            "Epoch 183/200 | Batch 3/64 | Training Loss: 0.5045\n",
            "Epoch 183/200 | Batch 4/64 | Training Loss: 0.5547\n",
            "Epoch 183/200 | Batch 5/64 | Training Loss: 0.5533\n",
            "Epoch 183/200 | Batch 6/64 | Training Loss: 0.4622\n",
            "Epoch 183/200 | Batch 7/64 | Training Loss: 0.5536\n",
            "Epoch 183/200 | Batch 8/64 | Training Loss: 0.2894\n",
            "Epoch 184/200 | Batch 1/64 | Training Loss: 0.6183\n",
            "Epoch 184/200 | Batch 2/64 | Training Loss: 0.5098\n",
            "Epoch 184/200 | Batch 3/64 | Training Loss: 0.6498\n",
            "Epoch 184/200 | Batch 4/64 | Training Loss: 0.6539\n",
            "Epoch 184/200 | Batch 5/64 | Training Loss: 0.3895\n",
            "Epoch 184/200 | Batch 6/64 | Training Loss: 0.4449\n",
            "Epoch 184/200 | Batch 7/64 | Training Loss: 0.4300\n",
            "Epoch 184/200 | Batch 8/64 | Training Loss: 0.7888\n",
            "Epoch 185/200 | Batch 1/64 | Training Loss: 0.4633\n",
            "Epoch 185/200 | Batch 2/64 | Training Loss: 0.5429\n",
            "Epoch 185/200 | Batch 3/64 | Training Loss: 0.5729\n",
            "Epoch 185/200 | Batch 4/64 | Training Loss: 0.4546\n",
            "Epoch 185/200 | Batch 5/64 | Training Loss: 0.5723\n",
            "Epoch 185/200 | Batch 6/64 | Training Loss: 0.4940\n",
            "Epoch 185/200 | Batch 7/64 | Training Loss: 0.4958\n",
            "Epoch 185/200 | Batch 8/64 | Training Loss: 1.0510\n",
            "Epoch 186/200 | Batch 1/64 | Training Loss: 0.4154\n",
            "Epoch 186/200 | Batch 2/64 | Training Loss: 0.6997\n",
            "Epoch 186/200 | Batch 3/64 | Training Loss: 0.3800\n",
            "Epoch 186/200 | Batch 4/64 | Training Loss: 0.6299\n",
            "Epoch 186/200 | Batch 5/64 | Training Loss: 0.5319\n",
            "Epoch 186/200 | Batch 6/64 | Training Loss: 0.5210\n",
            "Epoch 186/200 | Batch 7/64 | Training Loss: 0.4925\n",
            "Epoch 186/200 | Batch 8/64 | Training Loss: 0.5638\n",
            "Epoch 187/200 | Batch 1/64 | Training Loss: 0.4585\n",
            "Epoch 187/200 | Batch 2/64 | Training Loss: 0.6149\n",
            "Epoch 187/200 | Batch 3/64 | Training Loss: 0.4463\n",
            "Epoch 187/200 | Batch 4/64 | Training Loss: 0.5279\n",
            "Epoch 187/200 | Batch 5/64 | Training Loss: 0.5102\n",
            "Epoch 187/200 | Batch 6/64 | Training Loss: 0.5614\n",
            "Epoch 187/200 | Batch 7/64 | Training Loss: 0.6122\n",
            "Epoch 187/200 | Batch 8/64 | Training Loss: 0.6090\n",
            "Epoch 188/200 | Batch 1/64 | Training Loss: 0.5123\n",
            "Epoch 188/200 | Batch 2/64 | Training Loss: 0.5647\n",
            "Epoch 188/200 | Batch 3/64 | Training Loss: 0.5849\n",
            "Epoch 188/200 | Batch 4/64 | Training Loss: 0.4168\n",
            "Epoch 188/200 | Batch 5/64 | Training Loss: 0.5394\n",
            "Epoch 188/200 | Batch 6/64 | Training Loss: 0.5391\n",
            "Epoch 188/200 | Batch 7/64 | Training Loss: 0.4597\n",
            "Epoch 188/200 | Batch 8/64 | Training Loss: 0.3956\n",
            "Epoch 189/200 | Batch 1/64 | Training Loss: 0.5113\n",
            "Epoch 189/200 | Batch 2/64 | Training Loss: 0.5867\n",
            "Epoch 189/200 | Batch 3/64 | Training Loss: 0.5933\n",
            "Epoch 189/200 | Batch 4/64 | Training Loss: 0.5865\n",
            "Epoch 189/200 | Batch 5/64 | Training Loss: 0.3463\n",
            "Epoch 189/200 | Batch 6/64 | Training Loss: 0.4876\n",
            "Epoch 189/200 | Batch 7/64 | Training Loss: 0.4214\n",
            "Epoch 189/200 | Batch 8/64 | Training Loss: 0.6136\n",
            "Epoch 190/200 | Batch 1/64 | Training Loss: 0.5739\n",
            "Epoch 190/200 | Batch 2/64 | Training Loss: 0.4592\n",
            "Epoch 190/200 | Batch 3/64 | Training Loss: 0.5616\n",
            "Epoch 190/200 | Batch 4/64 | Training Loss: 0.4726\n",
            "Epoch 190/200 | Batch 5/64 | Training Loss: 0.4465\n",
            "Epoch 190/200 | Batch 6/64 | Training Loss: 0.4835\n",
            "Epoch 190/200 | Batch 7/64 | Training Loss: 0.4143\n",
            "Epoch 190/200 | Batch 8/64 | Training Loss: 0.8574\n",
            "Epoch 191/200 | Batch 1/64 | Training Loss: 0.3391\n",
            "Epoch 191/200 | Batch 2/64 | Training Loss: 0.4837\n",
            "Epoch 191/200 | Batch 3/64 | Training Loss: 0.5046\n",
            "Epoch 191/200 | Batch 4/64 | Training Loss: 0.5019\n",
            "Epoch 191/200 | Batch 5/64 | Training Loss: 0.4529\n",
            "Epoch 191/200 | Batch 6/64 | Training Loss: 0.4470\n",
            "Epoch 191/200 | Batch 7/64 | Training Loss: 0.5404\n",
            "Epoch 191/200 | Batch 8/64 | Training Loss: 0.5663\n",
            "Epoch 192/200 | Batch 1/64 | Training Loss: 0.5336\n",
            "Epoch 192/200 | Batch 2/64 | Training Loss: 0.7165\n",
            "Epoch 192/200 | Batch 3/64 | Training Loss: 0.5070\n",
            "Epoch 192/200 | Batch 4/64 | Training Loss: 0.5439\n",
            "Epoch 192/200 | Batch 5/64 | Training Loss: 0.5299\n",
            "Epoch 192/200 | Batch 6/64 | Training Loss: 0.4303\n",
            "Epoch 192/200 | Batch 7/64 | Training Loss: 0.4759\n",
            "Epoch 192/200 | Batch 8/64 | Training Loss: 0.5366\n",
            "Epoch 193/200 | Batch 1/64 | Training Loss: 0.4087\n",
            "Epoch 193/200 | Batch 2/64 | Training Loss: 0.6822\n",
            "Epoch 193/200 | Batch 3/64 | Training Loss: 0.5159\n",
            "Epoch 193/200 | Batch 4/64 | Training Loss: 0.6005\n",
            "Epoch 193/200 | Batch 5/64 | Training Loss: 0.4690\n",
            "Epoch 193/200 | Batch 6/64 | Training Loss: 0.6392\n",
            "Epoch 193/200 | Batch 7/64 | Training Loss: 0.4518\n",
            "Epoch 193/200 | Batch 8/64 | Training Loss: 0.4923\n",
            "Epoch 194/200 | Batch 1/64 | Training Loss: 0.5931\n",
            "Epoch 194/200 | Batch 2/64 | Training Loss: 0.5847\n",
            "Epoch 194/200 | Batch 3/64 | Training Loss: 0.3438\n",
            "Epoch 194/200 | Batch 4/64 | Training Loss: 0.6853\n",
            "Epoch 194/200 | Batch 5/64 | Training Loss: 0.3720\n",
            "Epoch 194/200 | Batch 6/64 | Training Loss: 0.5164\n",
            "Epoch 194/200 | Batch 7/64 | Training Loss: 0.4195\n",
            "Epoch 194/200 | Batch 8/64 | Training Loss: 0.4541\n",
            "Epoch 195/200 | Batch 1/64 | Training Loss: 0.5549\n",
            "Epoch 195/200 | Batch 2/64 | Training Loss: 0.3385\n",
            "Epoch 195/200 | Batch 3/64 | Training Loss: 0.4867\n",
            "Epoch 195/200 | Batch 4/64 | Training Loss: 0.4691\n",
            "Epoch 195/200 | Batch 5/64 | Training Loss: 0.4122\n",
            "Epoch 195/200 | Batch 6/64 | Training Loss: 0.6131\n",
            "Epoch 195/200 | Batch 7/64 | Training Loss: 0.5328\n",
            "Epoch 195/200 | Batch 8/64 | Training Loss: 0.3591\n",
            "Epoch 196/200 | Batch 1/64 | Training Loss: 0.5319\n",
            "Epoch 196/200 | Batch 2/64 | Training Loss: 0.5101\n",
            "Epoch 196/200 | Batch 3/64 | Training Loss: 0.6192\n",
            "Epoch 196/200 | Batch 4/64 | Training Loss: 0.4736\n",
            "Epoch 196/200 | Batch 5/64 | Training Loss: 0.4864\n",
            "Epoch 196/200 | Batch 6/64 | Training Loss: 0.4321\n",
            "Epoch 196/200 | Batch 7/64 | Training Loss: 0.4816\n",
            "Epoch 196/200 | Batch 8/64 | Training Loss: 0.5376\n",
            "Epoch 197/200 | Batch 1/64 | Training Loss: 0.5195\n",
            "Epoch 197/200 | Batch 2/64 | Training Loss: 0.4099\n",
            "Epoch 197/200 | Batch 3/64 | Training Loss: 0.6340\n",
            "Epoch 197/200 | Batch 4/64 | Training Loss: 0.3947\n",
            "Epoch 197/200 | Batch 5/64 | Training Loss: 0.4639\n",
            "Epoch 197/200 | Batch 6/64 | Training Loss: 0.5670\n",
            "Epoch 197/200 | Batch 7/64 | Training Loss: 0.6006\n",
            "Epoch 197/200 | Batch 8/64 | Training Loss: 0.4767\n",
            "Epoch 198/200 | Batch 1/64 | Training Loss: 0.4094\n",
            "Epoch 198/200 | Batch 2/64 | Training Loss: 0.3951\n",
            "Epoch 198/200 | Batch 3/64 | Training Loss: 0.5884\n",
            "Epoch 198/200 | Batch 4/64 | Training Loss: 0.4982\n",
            "Epoch 198/200 | Batch 5/64 | Training Loss: 0.6301\n",
            "Epoch 198/200 | Batch 6/64 | Training Loss: 0.5977\n",
            "Epoch 198/200 | Batch 7/64 | Training Loss: 0.3863\n",
            "Epoch 198/200 | Batch 8/64 | Training Loss: 0.4731\n",
            "Epoch 199/200 | Batch 1/64 | Training Loss: 0.4638\n",
            "Epoch 199/200 | Batch 2/64 | Training Loss: 0.6568\n",
            "Epoch 199/200 | Batch 3/64 | Training Loss: 0.5632\n",
            "Epoch 199/200 | Batch 4/64 | Training Loss: 0.4900\n",
            "Epoch 199/200 | Batch 5/64 | Training Loss: 0.3699\n",
            "Epoch 199/200 | Batch 6/64 | Training Loss: 0.5941\n",
            "Epoch 199/200 | Batch 7/64 | Training Loss: 0.4545\n",
            "Epoch 199/200 | Batch 8/64 | Training Loss: 0.5750\n",
            "Epoch 200/200 | Batch 1/64 | Training Loss: 0.3667\n",
            "Epoch 200/200 | Batch 2/64 | Training Loss: 0.7080\n",
            "Epoch 200/200 | Batch 3/64 | Training Loss: 0.4548\n",
            "Epoch 200/200 | Batch 4/64 | Training Loss: 0.2963\n",
            "Epoch 200/200 | Batch 5/64 | Training Loss: 0.6111\n",
            "Epoch 200/200 | Batch 6/64 | Training Loss: 0.4519\n",
            "Epoch 200/200 | Batch 7/64 | Training Loss: 0.4880\n",
            "Epoch 200/200 | Batch 8/64 | Training Loss: 0.2801\n"
          ]
        }
      ]
    }
  ]
}